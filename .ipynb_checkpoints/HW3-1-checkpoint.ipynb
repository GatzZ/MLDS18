{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:39:23.825928Z",
     "start_time": "2018-11-22T03:39:20.707846Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:39:24.018508Z",
     "start_time": "2018-11-22T03:39:23.827182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = imread('AnimeDataset/faces/sub/121.jpg')\n",
    "plt.imshow(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:40:55.107897Z",
     "start_time": "2018-11-22T03:40:54.757218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8192)              827392    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 64, 32)        32800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 64, 16)        8208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 64, 3)         771       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,263,539\n",
      "Trainable params: 1,263,059\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 64\n",
    "G_IN_DIM = 100\n",
    "\n",
    "g_in = Input((G_IN_DIM,))\n",
    "g_x = Dense(128 * 8 * 8, activation=\"relu\")(g_in)\n",
    "\n",
    "g_x = Reshape((8, 8,128))(g_x)\n",
    "g_x = UpSampling2D()(g_x)\n",
    "\n",
    "g_x = Conv2D(128, kernel_size=4, padding=\"same\")(g_x)\n",
    "g_x = BatchNormalization(momentum=0.8)(g_x)\n",
    "g_x = Activation(\"relu\")(g_x)\n",
    "\n",
    "g_x = UpSampling2D()(g_x)\n",
    "g_x = Conv2D(64, kernel_size=4, padding=\"same\")(g_x)\n",
    "g_x = BatchNormalization(momentum=0.8)(g_x)\n",
    "g_x = Activation(\"relu\")(g_x)\n",
    "\n",
    "g_x = UpSampling2D()(g_x)\n",
    "g_x = Conv2D(32, kernel_size=4, padding=\"same\")(g_x)\n",
    "g_x = BatchNormalization(momentum=0.8)(g_x)\n",
    "g_x = Activation(\"relu\")(g_x)\n",
    "\n",
    "g_x = Conv2D(3, kernel_size=4, padding=\"same\")(g_x)\n",
    "g_x = Activation(\"tanh\")(g_x)\n",
    "\n",
    "generator = Model(inputs=g_in, outputs=g_x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:40:56.008376Z",
     "start_time": "2018-11-22T03:40:55.609199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 16)        448       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 106,529\n",
      "Trainable params: 106,081\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_in = Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "d_x = Conv2D(16, kernel_size=3, strides=2, padding=\"same\")(d_in)\n",
    "d_x = LeakyReLU(alpha=0.2)(d_x)\n",
    "\n",
    "d_x = Dropout(0.25)(d_x)\n",
    "d_x = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
    "# d_x = ZeroPadding2D(padding=((0,1),(0,1)))(d_x)\n",
    "d_x = BatchNormalization(momentum=0.8)(d_x)\n",
    "d_x = LeakyReLU(alpha=0.2)(d_x)\n",
    "\n",
    "d_x = Dropout(0.25)(d_x)\n",
    "d_x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(d_x)\n",
    "d_x = BatchNormalization(momentum=0.8)(d_x)\n",
    "d_x = LeakyReLU(alpha=0.2)(d_x)\n",
    "d_x = Dropout(0.25)(d_x)\n",
    "\n",
    "d_x = Conv2D(128, kernel_size=3, strides=1, padding=\"same\")(d_x)\n",
    "d_x = BatchNormalization(momentum=0.8)(d_x)\n",
    "d_x = LeakyReLU(alpha=0.2)(d_x)\n",
    "\n",
    "d_x = Dropout(0.25)(d_x)\n",
    "d_x = Flatten()(d_x)\n",
    "d_x = Dense(1)(d_x)\n",
    "\n",
    "critic = Model(d_in, d_x)\n",
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:40:56.517595Z",
     "start_time": "2018-11-22T03:40:56.244744Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_pos = -1, y_neg = 1\n",
    "\n",
    "\n",
    "# For critic, the predicted labels are reversed\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "optimizer = RMSprop(lr=0.00005)\n",
    "critic.compile(optimizer=optimizer, loss=wasserstein_loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "critic.trainable = False\n",
    "g_valid = critic(g_x)\n",
    "dg = Model(g_in, g_valid)\n",
    "dg.compile(optimizer=optimizer, loss=wasserstein_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:40:56.756872Z",
     "start_time": "2018-11-22T03:40:56.752977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8192)              827392    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 64, 32)        32800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 64, 16)        8208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 64, 3)         771       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              (None, 1)                 106529    \n",
      "=================================================================\n",
      "Total params: 1,370,068\n",
      "Trainable params: 1,263,059\n",
      "Non-trainable params: 107,009\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:18:50.864574Z",
     "start_time": "2018-11-22T06:18:50.862463Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100000\n",
    "critic_repeat_times = 5\n",
    "batch_size = 32\n",
    "seed = 1\n",
    "clip_value = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:40:58.189076Z",
     "start_time": "2018-11-22T03:40:57.249265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33431 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=lambda x: (x / 127.5) -1)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'AnimeDataset/faces/',\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:40:58.438406Z",
     "start_time": "2018-11-22T03:40:58.434509Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_samples(save=False):\n",
    "\n",
    "    zz = np.random.normal(0., 1., (100, G_IN_DIM))\n",
    "    generated_images = generator.predict(zz)\n",
    "\n",
    "    rr = []\n",
    "    for c in range(10):\n",
    "        rr.append(\n",
    "            np.concatenate(generated_images[c * 10:(1 + c) * 10]).reshape(\n",
    "                IMG_SIZE * 10, IMG_SIZE, 3))\n",
    "    img = np.hstack(rr)\n",
    "\n",
    "#     if save:\n",
    "#         plt.imsave(OUT_DIR + '/samples_%07d.png' % n, img, cmap=plt.cm.gray)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T03:40:58.678998Z",
     "start_time": "2018-11-22T03:40:58.672273Z"
    }
   },
   "outputs": [],
   "source": [
    "# write tensorboard summaries\n",
    "sw = tf.summary.FileWriter('log/')\n",
    "\n",
    "img_to_show = tf.placeholder(tf.float32, (IMG_SIZE * 10, IMG_SIZE * 10, 3))\n",
    "image_op = tf.summary.image('samples', tf.expand_dims(img_to_show, axis=0))\n",
    "\n",
    "def update_tb_summary(step, d_loss, g_loss, write_sample_images=True):\n",
    "\n",
    "    s = tf.Summary()\n",
    "    v = s.value.add()\n",
    "    v.simple_value = d_loss\n",
    "    v.tag = 'd_loss'\n",
    "    \n",
    "    v = s.value.add()\n",
    "    v.simple_value = g_loss\n",
    "    v.tag = 'g_loss'\n",
    "\n",
    "    # generated image\n",
    "    if write_sample_images:\n",
    "        img = generate_samples(save=False)\n",
    "        s.MergeFromString(session.run(image_op, feed_dict={img_to_show: img}))\n",
    "\n",
    "    sw.add_summary(s, step)\n",
    "    sw.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T12:22:26.549428Z",
     "start_time": "2018-11-22T06:19:05.433467Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 [D loss: 0.999959] [G loss: 1.000056]\n",
      "30010 [D loss: 0.999978] [G loss: 1.000063]\n",
      "30020 [D loss: 0.999968] [G loss: 1.000070]\n",
      "30030 [D loss: 0.999969] [G loss: 1.000065]\n",
      "30040 [D loss: 0.999963] [G loss: 1.000060]\n",
      "30050 [D loss: 0.999976] [G loss: 1.000051]\n",
      "30060 [D loss: 0.999970] [G loss: 1.000027]\n",
      "30070 [D loss: 0.999959] [G loss: 1.000034]\n",
      "30080 [D loss: 0.999971] [G loss: 1.000063]\n",
      "30090 [D loss: 0.999971] [G loss: 1.000063]\n",
      "30100 [D loss: 0.999964] [G loss: 1.000053]\n",
      "30110 [D loss: 0.999956] [G loss: 1.000074]\n",
      "30120 [D loss: 0.999974] [G loss: 1.000066]\n",
      "30130 [D loss: 0.999972] [G loss: 1.000044]\n",
      "30140 [D loss: 0.999965] [G loss: 1.000045]\n",
      "30150 [D loss: 0.999957] [G loss: 1.000062]\n",
      "30160 [D loss: 0.999941] [G loss: 1.000067]\n",
      "30170 [D loss: 0.999979] [G loss: 1.000051]\n",
      "30180 [D loss: 0.999954] [G loss: 1.000070]\n",
      "30190 [D loss: 0.999959] [G loss: 1.000062]\n",
      "30200 [D loss: 0.999993] [G loss: 1.000084]\n",
      "30210 [D loss: 0.999992] [G loss: 1.000048]\n",
      "30220 [D loss: 0.999975] [G loss: 1.000092]\n",
      "30230 [D loss: 0.999960] [G loss: 1.000034]\n",
      "30240 [D loss: 0.999943] [G loss: 1.000042]\n",
      "30250 [D loss: 0.999983] [G loss: 1.000057]\n",
      "30260 [D loss: 0.999960] [G loss: 1.000064]\n",
      "30270 [D loss: 0.999943] [G loss: 1.000068]\n",
      "30280 [D loss: 0.999974] [G loss: 1.000054]\n",
      "30290 [D loss: 0.999964] [G loss: 1.000061]\n",
      "30300 [D loss: 1.000002] [G loss: 1.000026]\n",
      "30310 [D loss: 0.999966] [G loss: 1.000076]\n",
      "30320 [D loss: 0.999963] [G loss: 1.000037]\n",
      "30330 [D loss: 0.999984] [G loss: 1.000081]\n",
      "30340 [D loss: 0.999976] [G loss: 1.000094]\n",
      "30350 [D loss: 0.999957] [G loss: 1.000041]\n",
      "30360 [D loss: 0.999961] [G loss: 1.000061]\n",
      "30370 [D loss: 0.999963] [G loss: 1.000049]\n",
      "30380 [D loss: 0.999961] [G loss: 1.000014]\n",
      "30390 [D loss: 0.999970] [G loss: 1.000081]\n",
      "30400 [D loss: 0.999966] [G loss: 1.000051]\n",
      "30410 [D loss: 0.999972] [G loss: 1.000097]\n",
      "30420 [D loss: 0.999969] [G loss: 1.000083]\n",
      "30430 [D loss: 0.999978] [G loss: 1.000051]\n",
      "30440 [D loss: 0.999963] [G loss: 1.000055]\n",
      "30450 [D loss: 0.999958] [G loss: 1.000073]\n",
      "30460 [D loss: 0.999969] [G loss: 1.000064]\n",
      "30470 [D loss: 0.999961] [G loss: 1.000077]\n",
      "30480 [D loss: 0.999970] [G loss: 1.000053]\n",
      "30490 [D loss: 0.999978] [G loss: 1.000074]\n",
      "30500 [D loss: 0.999953] [G loss: 1.000063]\n",
      "30510 [D loss: 0.999974] [G loss: 1.000055]\n",
      "30520 [D loss: 0.999965] [G loss: 1.000086]\n",
      "30530 [D loss: 0.999975] [G loss: 1.000047]\n",
      "30540 [D loss: 0.999947] [G loss: 1.000035]\n",
      "30550 [D loss: 0.999984] [G loss: 1.000051]\n",
      "30560 [D loss: 0.999957] [G loss: 1.000050]\n",
      "30570 [D loss: 0.999954] [G loss: 1.000063]\n",
      "30580 [D loss: 0.999947] [G loss: 1.000067]\n",
      "30590 [D loss: 0.999969] [G loss: 1.000047]\n",
      "30600 [D loss: 0.999964] [G loss: 1.000045]\n",
      "30610 [D loss: 0.999987] [G loss: 1.000035]\n",
      "30620 [D loss: 0.999959] [G loss: 1.000067]\n",
      "30630 [D loss: 0.999970] [G loss: 1.000079]\n",
      "30640 [D loss: 0.999980] [G loss: 1.000055]\n",
      "30650 [D loss: 0.999982] [G loss: 1.000070]\n",
      "30660 [D loss: 0.999970] [G loss: 1.000071]\n",
      "30670 [D loss: 0.999960] [G loss: 1.000064]\n",
      "30680 [D loss: 0.999968] [G loss: 1.000061]\n",
      "30690 [D loss: 0.999978] [G loss: 1.000054]\n",
      "30700 [D loss: 0.999984] [G loss: 1.000053]\n",
      "30710 [D loss: 0.999976] [G loss: 1.000050]\n",
      "30720 [D loss: 0.999951] [G loss: 1.000061]\n",
      "30730 [D loss: 0.999958] [G loss: 1.000046]\n",
      "30740 [D loss: 0.999967] [G loss: 1.000075]\n",
      "30750 [D loss: 0.999956] [G loss: 1.000075]\n",
      "30760 [D loss: 0.999961] [G loss: 1.000065]\n",
      "30770 [D loss: 0.999982] [G loss: 1.000050]\n",
      "30780 [D loss: 0.999973] [G loss: 1.000060]\n",
      "30790 [D loss: 0.999974] [G loss: 1.000058]\n",
      "30800 [D loss: 0.999979] [G loss: 1.000070]\n",
      "30810 [D loss: 0.999970] [G loss: 1.000105]\n",
      "30820 [D loss: 0.999995] [G loss: 1.000029]\n",
      "30830 [D loss: 0.999993] [G loss: 1.000065]\n",
      "30840 [D loss: 0.999968] [G loss: 1.000063]\n",
      "30850 [D loss: 0.999950] [G loss: 1.000054]\n",
      "30860 [D loss: 0.999964] [G loss: 1.000066]\n",
      "30870 [D loss: 0.999968] [G loss: 1.000079]\n",
      "30880 [D loss: 0.999974] [G loss: 1.000086]\n",
      "30890 [D loss: 0.999967] [G loss: 1.000040]\n",
      "30900 [D loss: 0.999957] [G loss: 1.000076]\n",
      "30910 [D loss: 0.999960] [G loss: 1.000047]\n",
      "30920 [D loss: 0.999970] [G loss: 1.000054]\n",
      "30930 [D loss: 0.999982] [G loss: 1.000033]\n",
      "30940 [D loss: 0.999957] [G loss: 1.000062]\n",
      "30950 [D loss: 0.999951] [G loss: 1.000049]\n",
      "30960 [D loss: 0.999968] [G loss: 1.000078]\n",
      "30970 [D loss: 0.999969] [G loss: 1.000049]\n",
      "30980 [D loss: 0.999978] [G loss: 1.000077]\n",
      "30990 [D loss: 0.999937] [G loss: 1.000045]\n",
      "31000 [D loss: 0.999971] [G loss: 1.000069]\n",
      "31010 [D loss: 0.999945] [G loss: 1.000053]\n",
      "31020 [D loss: 0.999965] [G loss: 1.000073]\n",
      "31030 [D loss: 0.999984] [G loss: 1.000042]\n",
      "31040 [D loss: 0.999957] [G loss: 1.000063]\n",
      "31050 [D loss: 0.999976] [G loss: 1.000062]\n",
      "31060 [D loss: 0.999970] [G loss: 1.000055]\n",
      "31070 [D loss: 0.999957] [G loss: 1.000052]\n",
      "31080 [D loss: 0.999964] [G loss: 1.000049]\n",
      "31090 [D loss: 0.999959] [G loss: 1.000085]\n",
      "31100 [D loss: 0.999958] [G loss: 1.000070]\n",
      "31110 [D loss: 0.999973] [G loss: 1.000051]\n",
      "31120 [D loss: 0.999967] [G loss: 1.000065]\n",
      "31130 [D loss: 0.999965] [G loss: 1.000072]\n",
      "31140 [D loss: 0.999963] [G loss: 1.000057]\n",
      "31150 [D loss: 0.999952] [G loss: 1.000059]\n",
      "31160 [D loss: 0.999962] [G loss: 1.000044]\n",
      "31170 [D loss: 0.999967] [G loss: 1.000079]\n",
      "31180 [D loss: 0.999968] [G loss: 1.000066]\n",
      "31190 [D loss: 0.999958] [G loss: 1.000064]\n",
      "31200 [D loss: 0.999964] [G loss: 1.000070]\n",
      "31210 [D loss: 0.999969] [G loss: 1.000058]\n",
      "31220 [D loss: 0.999957] [G loss: 1.000061]\n",
      "31230 [D loss: 0.999970] [G loss: 1.000058]\n",
      "31240 [D loss: 0.999976] [G loss: 1.000074]\n",
      "31250 [D loss: 0.999959] [G loss: 1.000071]\n",
      "31260 [D loss: 0.999976] [G loss: 1.000056]\n",
      "31270 [D loss: 0.999983] [G loss: 1.000061]\n",
      "31280 [D loss: 0.999970] [G loss: 1.000055]\n",
      "31290 [D loss: 0.999971] [G loss: 1.000068]\n",
      "31300 [D loss: 0.999966] [G loss: 1.000062]\n",
      "31310 [D loss: 0.999965] [G loss: 1.000048]\n",
      "31320 [D loss: 0.999970] [G loss: 1.000072]\n",
      "31330 [D loss: 0.999962] [G loss: 1.000046]\n",
      "31340 [D loss: 0.999974] [G loss: 1.000072]\n",
      "31350 [D loss: 0.999959] [G loss: 1.000060]\n",
      "31360 [D loss: 0.999957] [G loss: 1.000070]\n",
      "31370 [D loss: 0.999966] [G loss: 1.000060]\n",
      "31380 [D loss: 0.999973] [G loss: 1.000052]\n",
      "31390 [D loss: 0.999967] [G loss: 1.000073]\n",
      "31400 [D loss: 0.999960] [G loss: 1.000048]\n",
      "31410 [D loss: 0.999971] [G loss: 1.000046]\n",
      "31420 [D loss: 0.999963] [G loss: 1.000057]\n",
      "31430 [D loss: 0.999955] [G loss: 1.000059]\n",
      "31440 [D loss: 0.999971] [G loss: 1.000058]\n",
      "31450 [D loss: 0.999962] [G loss: 1.000059]\n",
      "31460 [D loss: 0.999958] [G loss: 1.000060]\n",
      "31470 [D loss: 0.999962] [G loss: 1.000054]\n",
      "31480 [D loss: 0.999951] [G loss: 1.000057]\n",
      "31490 [D loss: 0.999972] [G loss: 1.000067]\n",
      "31500 [D loss: 0.999969] [G loss: 1.000055]\n",
      "31510 [D loss: 0.999956] [G loss: 1.000055]\n",
      "31520 [D loss: 0.999959] [G loss: 1.000075]\n",
      "31530 [D loss: 0.999959] [G loss: 1.000052]\n",
      "31540 [D loss: 0.999973] [G loss: 1.000029]\n",
      "31550 [D loss: 0.999958] [G loss: 1.000050]\n",
      "31560 [D loss: 0.999949] [G loss: 1.000058]\n",
      "31570 [D loss: 0.999965] [G loss: 1.000081]\n",
      "31580 [D loss: 0.999951] [G loss: 1.000038]\n",
      "31590 [D loss: 0.999966] [G loss: 1.000056]\n",
      "31600 [D loss: 0.999965] [G loss: 1.000078]\n",
      "31610 [D loss: 0.999963] [G loss: 1.000068]\n",
      "31620 [D loss: 0.999956] [G loss: 1.000084]\n",
      "31630 [D loss: 0.999958] [G loss: 1.000068]\n",
      "31640 [D loss: 0.999968] [G loss: 1.000079]\n",
      "31650 [D loss: 0.999960] [G loss: 1.000071]\n",
      "31660 [D loss: 0.999958] [G loss: 1.000061]\n",
      "31670 [D loss: 0.999969] [G loss: 1.000073]\n",
      "31680 [D loss: 0.999978] [G loss: 1.000088]\n",
      "31690 [D loss: 0.999985] [G loss: 1.000050]\n",
      "31700 [D loss: 0.999971] [G loss: 1.000061]\n",
      "31710 [D loss: 0.999969] [G loss: 1.000063]\n",
      "31720 [D loss: 0.999965] [G loss: 1.000069]\n",
      "31730 [D loss: 0.999964] [G loss: 1.000073]\n",
      "31740 [D loss: 0.999967] [G loss: 1.000068]\n",
      "31750 [D loss: 0.999969] [G loss: 1.000063]\n",
      "31760 [D loss: 0.999956] [G loss: 1.000047]\n",
      "31770 [D loss: 0.999986] [G loss: 1.000072]\n",
      "31780 [D loss: 0.999974] [G loss: 1.000053]\n",
      "31790 [D loss: 0.999963] [G loss: 1.000077]\n",
      "31800 [D loss: 0.999988] [G loss: 1.000059]\n",
      "31810 [D loss: 0.999968] [G loss: 1.000034]\n",
      "31820 [D loss: 0.999973] [G loss: 1.000055]\n",
      "31830 [D loss: 0.999964] [G loss: 1.000039]\n",
      "31840 [D loss: 0.999964] [G loss: 1.000055]\n",
      "31850 [D loss: 0.999976] [G loss: 1.000051]\n",
      "31860 [D loss: 0.999965] [G loss: 1.000055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31870 [D loss: 0.999969] [G loss: 1.000072]\n",
      "31880 [D loss: 0.999991] [G loss: 1.000057]\n",
      "31890 [D loss: 0.999967] [G loss: 1.000067]\n",
      "31900 [D loss: 0.999961] [G loss: 1.000055]\n",
      "31910 [D loss: 0.999963] [G loss: 1.000073]\n",
      "31920 [D loss: 0.999952] [G loss: 1.000046]\n",
      "31930 [D loss: 0.999962] [G loss: 1.000055]\n",
      "31940 [D loss: 0.999963] [G loss: 1.000042]\n",
      "31950 [D loss: 0.999957] [G loss: 1.000062]\n",
      "31960 [D loss: 0.999967] [G loss: 1.000084]\n",
      "31970 [D loss: 0.999972] [G loss: 1.000054]\n",
      "31980 [D loss: 0.999970] [G loss: 1.000066]\n",
      "31990 [D loss: 0.999969] [G loss: 1.000043]\n",
      "32000 [D loss: 0.999981] [G loss: 1.000077]\n",
      "32010 [D loss: 0.999962] [G loss: 1.000059]\n",
      "32020 [D loss: 0.999972] [G loss: 1.000041]\n",
      "32030 [D loss: 0.999970] [G loss: 1.000049]\n",
      "32040 [D loss: 0.999960] [G loss: 1.000059]\n",
      "32050 [D loss: 0.999961] [G loss: 1.000075]\n",
      "32060 [D loss: 0.999967] [G loss: 1.000069]\n",
      "32070 [D loss: 0.999976] [G loss: 1.000076]\n",
      "32080 [D loss: 0.999960] [G loss: 1.000050]\n",
      "32090 [D loss: 0.999955] [G loss: 1.000062]\n",
      "32100 [D loss: 0.999966] [G loss: 1.000040]\n",
      "32110 [D loss: 0.999975] [G loss: 1.000045]\n",
      "32120 [D loss: 0.999958] [G loss: 1.000068]\n",
      "32130 [D loss: 0.999950] [G loss: 1.000063]\n",
      "32140 [D loss: 0.999956] [G loss: 1.000054]\n",
      "32150 [D loss: 0.999941] [G loss: 1.000080]\n",
      "32160 [D loss: 0.999961] [G loss: 1.000068]\n",
      "32170 [D loss: 0.999971] [G loss: 1.000074]\n",
      "32180 [D loss: 0.999979] [G loss: 1.000056]\n",
      "32190 [D loss: 0.999952] [G loss: 1.000079]\n",
      "32200 [D loss: 0.999965] [G loss: 1.000049]\n",
      "32210 [D loss: 0.999964] [G loss: 1.000063]\n",
      "32220 [D loss: 0.999968] [G loss: 1.000069]\n",
      "32230 [D loss: 0.999964] [G loss: 1.000067]\n",
      "32240 [D loss: 0.999954] [G loss: 1.000081]\n",
      "32250 [D loss: 0.999961] [G loss: 1.000067]\n",
      "32260 [D loss: 0.999965] [G loss: 1.000053]\n",
      "32270 [D loss: 0.999972] [G loss: 1.000062]\n",
      "32280 [D loss: 0.999966] [G loss: 1.000057]\n",
      "32290 [D loss: 0.999933] [G loss: 1.000059]\n",
      "32300 [D loss: 0.999956] [G loss: 1.000093]\n",
      "32310 [D loss: 0.999944] [G loss: 1.000052]\n",
      "32320 [D loss: 0.999972] [G loss: 1.000073]\n",
      "32330 [D loss: 0.999959] [G loss: 1.000059]\n",
      "32340 [D loss: 0.999967] [G loss: 1.000041]\n",
      "32350 [D loss: 0.999958] [G loss: 1.000045]\n",
      "32360 [D loss: 0.999961] [G loss: 1.000067]\n",
      "32370 [D loss: 0.999964] [G loss: 1.000067]\n",
      "32380 [D loss: 0.999968] [G loss: 1.000063]\n",
      "32390 [D loss: 0.999959] [G loss: 1.000052]\n",
      "32400 [D loss: 0.999960] [G loss: 1.000061]\n",
      "32410 [D loss: 0.999973] [G loss: 1.000089]\n",
      "32420 [D loss: 0.999966] [G loss: 1.000080]\n",
      "32430 [D loss: 0.999964] [G loss: 1.000046]\n",
      "32440 [D loss: 0.999960] [G loss: 1.000072]\n",
      "32450 [D loss: 0.999971] [G loss: 1.000075]\n",
      "32460 [D loss: 0.999946] [G loss: 1.000064]\n",
      "32470 [D loss: 0.999958] [G loss: 1.000061]\n",
      "32480 [D loss: 0.999973] [G loss: 1.000046]\n",
      "32490 [D loss: 0.999956] [G loss: 1.000053]\n",
      "32500 [D loss: 0.999974] [G loss: 1.000046]\n",
      "32510 [D loss: 0.999971] [G loss: 1.000050]\n",
      "32520 [D loss: 0.999976] [G loss: 1.000046]\n",
      "32530 [D loss: 0.999974] [G loss: 1.000026]\n",
      "32540 [D loss: 0.999969] [G loss: 1.000060]\n",
      "32550 [D loss: 0.999968] [G loss: 1.000050]\n",
      "32560 [D loss: 0.999956] [G loss: 1.000046]\n",
      "32570 [D loss: 0.999960] [G loss: 1.000057]\n",
      "32580 [D loss: 0.999971] [G loss: 1.000056]\n",
      "32590 [D loss: 0.999953] [G loss: 1.000055]\n",
      "32600 [D loss: 0.999979] [G loss: 1.000013]\n",
      "32610 [D loss: 0.999957] [G loss: 1.000069]\n",
      "32620 [D loss: 0.999946] [G loss: 1.000064]\n",
      "32630 [D loss: 0.999956] [G loss: 1.000060]\n",
      "32640 [D loss: 0.999963] [G loss: 1.000054]\n",
      "32650 [D loss: 0.999957] [G loss: 1.000073]\n",
      "32660 [D loss: 0.999969] [G loss: 1.000040]\n",
      "32670 [D loss: 0.999974] [G loss: 1.000045]\n",
      "32680 [D loss: 0.999955] [G loss: 1.000047]\n",
      "32690 [D loss: 0.999955] [G loss: 1.000065]\n",
      "32700 [D loss: 0.999979] [G loss: 1.000064]\n",
      "32710 [D loss: 0.999968] [G loss: 1.000077]\n",
      "32720 [D loss: 0.999960] [G loss: 1.000043]\n",
      "32730 [D loss: 0.999970] [G loss: 1.000080]\n",
      "32740 [D loss: 0.999975] [G loss: 1.000064]\n",
      "32750 [D loss: 0.999960] [G loss: 1.000053]\n",
      "32760 [D loss: 0.999958] [G loss: 1.000076]\n",
      "32770 [D loss: 0.999970] [G loss: 1.000050]\n",
      "32780 [D loss: 0.999971] [G loss: 1.000054]\n",
      "32790 [D loss: 0.999966] [G loss: 1.000053]\n",
      "32800 [D loss: 0.999974] [G loss: 1.000056]\n",
      "32810 [D loss: 0.999976] [G loss: 1.000042]\n",
      "32820 [D loss: 0.999962] [G loss: 1.000039]\n",
      "32830 [D loss: 0.999967] [G loss: 1.000070]\n",
      "32840 [D loss: 0.999968] [G loss: 1.000070]\n",
      "32850 [D loss: 0.999952] [G loss: 1.000061]\n",
      "32860 [D loss: 0.999965] [G loss: 1.000053]\n",
      "32870 [D loss: 0.999960] [G loss: 1.000049]\n",
      "32880 [D loss: 0.999972] [G loss: 1.000075]\n",
      "32890 [D loss: 0.999967] [G loss: 1.000039]\n",
      "32900 [D loss: 0.999975] [G loss: 1.000020]\n",
      "32910 [D loss: 0.999968] [G loss: 1.000099]\n",
      "32920 [D loss: 0.999959] [G loss: 1.000062]\n",
      "32930 [D loss: 0.999966] [G loss: 1.000063]\n",
      "32940 [D loss: 0.999977] [G loss: 1.000077]\n",
      "32950 [D loss: 0.999970] [G loss: 1.000057]\n",
      "32960 [D loss: 0.999981] [G loss: 1.000061]\n",
      "32970 [D loss: 0.999961] [G loss: 1.000063]\n",
      "32980 [D loss: 0.999959] [G loss: 1.000058]\n",
      "32990 [D loss: 0.999960] [G loss: 1.000076]\n",
      "33000 [D loss: 0.999960] [G loss: 1.000065]\n",
      "33010 [D loss: 0.999955] [G loss: 1.000076]\n",
      "33020 [D loss: 0.999965] [G loss: 1.000069]\n",
      "33030 [D loss: 0.999976] [G loss: 1.000047]\n",
      "33040 [D loss: 0.999976] [G loss: 1.000067]\n",
      "33050 [D loss: 0.999970] [G loss: 1.000054]\n",
      "33060 [D loss: 0.999971] [G loss: 1.000079]\n",
      "33070 [D loss: 0.999978] [G loss: 1.000090]\n",
      "33080 [D loss: 0.999960] [G loss: 1.000050]\n",
      "33090 [D loss: 0.999974] [G loss: 1.000058]\n",
      "33100 [D loss: 0.999962] [G loss: 1.000040]\n",
      "33110 [D loss: 0.999946] [G loss: 1.000026]\n",
      "33120 [D loss: 0.999964] [G loss: 1.000069]\n",
      "33130 [D loss: 0.999984] [G loss: 1.000043]\n",
      "33140 [D loss: 0.999962] [G loss: 1.000030]\n",
      "33150 [D loss: 0.999937] [G loss: 1.000063]\n",
      "33160 [D loss: 0.999976] [G loss: 1.000043]\n",
      "33170 [D loss: 0.999971] [G loss: 1.000046]\n",
      "33180 [D loss: 0.999954] [G loss: 1.000080]\n",
      "33190 [D loss: 0.999976] [G loss: 1.000064]\n",
      "33200 [D loss: 0.999976] [G loss: 1.000048]\n",
      "33210 [D loss: 0.999961] [G loss: 1.000043]\n",
      "33220 [D loss: 0.999963] [G loss: 1.000079]\n",
      "33230 [D loss: 0.999977] [G loss: 1.000046]\n",
      "33240 [D loss: 0.999952] [G loss: 1.000064]\n",
      "33250 [D loss: 0.999959] [G loss: 1.000041]\n",
      "33260 [D loss: 0.999952] [G loss: 1.000063]\n",
      "33270 [D loss: 0.999937] [G loss: 1.000044]\n",
      "33280 [D loss: 0.999969] [G loss: 1.000082]\n",
      "33290 [D loss: 0.999965] [G loss: 1.000072]\n",
      "33300 [D loss: 0.999967] [G loss: 1.000061]\n",
      "33310 [D loss: 0.999965] [G loss: 1.000038]\n",
      "33320 [D loss: 0.999955] [G loss: 1.000055]\n",
      "33330 [D loss: 0.999968] [G loss: 1.000069]\n",
      "33340 [D loss: 0.999971] [G loss: 1.000071]\n",
      "33350 [D loss: 0.999977] [G loss: 1.000066]\n",
      "33360 [D loss: 0.999983] [G loss: 1.000067]\n",
      "33370 [D loss: 0.999962] [G loss: 1.000054]\n",
      "33380 [D loss: 0.999975] [G loss: 1.000059]\n",
      "33390 [D loss: 0.999956] [G loss: 1.000037]\n",
      "33400 [D loss: 0.999961] [G loss: 1.000056]\n",
      "33410 [D loss: 0.999970] [G loss: 1.000053]\n",
      "33420 [D loss: 0.999964] [G loss: 1.000060]\n",
      "33430 [D loss: 0.999962] [G loss: 1.000071]\n",
      "33440 [D loss: 0.999967] [G loss: 1.000082]\n",
      "33450 [D loss: 0.999972] [G loss: 1.000070]\n",
      "33460 [D loss: 0.999954] [G loss: 1.000061]\n",
      "33470 [D loss: 0.999973] [G loss: 1.000069]\n",
      "33480 [D loss: 0.999970] [G loss: 1.000068]\n",
      "33490 [D loss: 0.999965] [G loss: 1.000060]\n",
      "33500 [D loss: 0.999979] [G loss: 1.000055]\n",
      "33510 [D loss: 0.999965] [G loss: 1.000049]\n",
      "33520 [D loss: 0.999959] [G loss: 1.000071]\n",
      "33530 [D loss: 0.999963] [G loss: 1.000075]\n",
      "33540 [D loss: 0.999971] [G loss: 1.000041]\n",
      "33550 [D loss: 0.999959] [G loss: 1.000059]\n",
      "33560 [D loss: 0.999958] [G loss: 1.000043]\n",
      "33570 [D loss: 0.999960] [G loss: 1.000028]\n",
      "33580 [D loss: 0.999961] [G loss: 1.000021]\n",
      "33590 [D loss: 0.999954] [G loss: 1.000076]\n",
      "33600 [D loss: 0.999973] [G loss: 1.000056]\n",
      "33610 [D loss: 0.999983] [G loss: 1.000078]\n",
      "33620 [D loss: 0.999953] [G loss: 1.000076]\n",
      "33630 [D loss: 0.999979] [G loss: 1.000044]\n",
      "33640 [D loss: 0.999971] [G loss: 1.000058]\n",
      "33650 [D loss: 0.999963] [G loss: 1.000068]\n",
      "33660 [D loss: 0.999961] [G loss: 1.000065]\n",
      "33670 [D loss: 0.999956] [G loss: 1.000057]\n",
      "33680 [D loss: 0.999976] [G loss: 1.000080]\n",
      "33690 [D loss: 0.999992] [G loss: 1.000046]\n",
      "33700 [D loss: 0.999955] [G loss: 1.000052]\n",
      "33710 [D loss: 0.999991] [G loss: 1.000056]\n",
      "33720 [D loss: 0.999967] [G loss: 1.000077]\n",
      "33730 [D loss: 0.999966] [G loss: 1.000062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33740 [D loss: 0.999975] [G loss: 1.000073]\n",
      "33750 [D loss: 0.999962] [G loss: 1.000050]\n",
      "33760 [D loss: 0.999960] [G loss: 1.000050]\n",
      "33770 [D loss: 0.999965] [G loss: 1.000061]\n",
      "33780 [D loss: 0.999964] [G loss: 1.000073]\n",
      "33790 [D loss: 0.999957] [G loss: 1.000064]\n",
      "33800 [D loss: 0.999965] [G loss: 1.000059]\n",
      "33810 [D loss: 0.999970] [G loss: 1.000045]\n",
      "33820 [D loss: 0.999988] [G loss: 1.000055]\n",
      "33830 [D loss: 0.999964] [G loss: 1.000085]\n",
      "33840 [D loss: 0.999963] [G loss: 1.000040]\n",
      "33850 [D loss: 0.999963] [G loss: 1.000055]\n",
      "33860 [D loss: 0.999968] [G loss: 1.000066]\n",
      "33870 [D loss: 0.999975] [G loss: 1.000046]\n",
      "33880 [D loss: 0.999951] [G loss: 1.000046]\n",
      "33890 [D loss: 0.999983] [G loss: 1.000054]\n",
      "33900 [D loss: 0.999947] [G loss: 1.000052]\n",
      "33910 [D loss: 0.999963] [G loss: 1.000066]\n",
      "33920 [D loss: 0.999958] [G loss: 1.000062]\n",
      "33930 [D loss: 0.999956] [G loss: 1.000063]\n",
      "33940 [D loss: 0.999938] [G loss: 1.000039]\n",
      "33950 [D loss: 0.999970] [G loss: 1.000063]\n",
      "33960 [D loss: 0.999987] [G loss: 1.000060]\n",
      "33970 [D loss: 0.999981] [G loss: 1.000034]\n",
      "33980 [D loss: 0.999959] [G loss: 1.000050]\n",
      "33990 [D loss: 0.999970] [G loss: 1.000063]\n",
      "34000 [D loss: 0.999979] [G loss: 1.000099]\n",
      "34010 [D loss: 0.999960] [G loss: 1.000068]\n",
      "34020 [D loss: 0.999950] [G loss: 1.000048]\n",
      "34030 [D loss: 0.999975] [G loss: 1.000050]\n",
      "34040 [D loss: 0.999961] [G loss: 1.000063]\n",
      "34050 [D loss: 0.999974] [G loss: 1.000019]\n",
      "34060 [D loss: 0.999944] [G loss: 1.000031]\n",
      "34070 [D loss: 0.999958] [G loss: 1.000063]\n",
      "34080 [D loss: 0.999978] [G loss: 1.000073]\n",
      "34090 [D loss: 0.999957] [G loss: 1.000068]\n",
      "34100 [D loss: 0.999978] [G loss: 1.000059]\n",
      "34110 [D loss: 0.999957] [G loss: 1.000055]\n",
      "34120 [D loss: 0.999970] [G loss: 1.000097]\n",
      "34130 [D loss: 0.999986] [G loss: 1.000089]\n",
      "34140 [D loss: 0.999979] [G loss: 1.000045]\n",
      "34150 [D loss: 0.999949] [G loss: 1.000052]\n",
      "34160 [D loss: 0.999964] [G loss: 1.000042]\n",
      "34170 [D loss: 0.999968] [G loss: 1.000036]\n",
      "34180 [D loss: 0.999961] [G loss: 1.000054]\n",
      "34190 [D loss: 0.999983] [G loss: 1.000082]\n",
      "34200 [D loss: 0.999967] [G loss: 1.000060]\n",
      "34210 [D loss: 0.999951] [G loss: 1.000048]\n",
      "34220 [D loss: 0.999978] [G loss: 1.000070]\n",
      "34230 [D loss: 0.999931] [G loss: 1.000018]\n",
      "34240 [D loss: 0.999984] [G loss: 1.000055]\n",
      "34250 [D loss: 0.999956] [G loss: 1.000063]\n",
      "34260 [D loss: 0.999948] [G loss: 1.000089]\n",
      "34270 [D loss: 0.999947] [G loss: 1.000058]\n",
      "34280 [D loss: 0.999968] [G loss: 1.000070]\n",
      "34290 [D loss: 0.999971] [G loss: 1.000076]\n",
      "34300 [D loss: 0.999986] [G loss: 1.000012]\n",
      "34310 [D loss: 0.999973] [G loss: 1.000073]\n",
      "34320 [D loss: 0.999981] [G loss: 1.000051]\n",
      "34330 [D loss: 0.999941] [G loss: 1.000070]\n",
      "34340 [D loss: 0.999963] [G loss: 1.000074]\n",
      "34350 [D loss: 0.999965] [G loss: 1.000087]\n",
      "34360 [D loss: 0.999960] [G loss: 1.000096]\n",
      "34370 [D loss: 0.999960] [G loss: 1.000061]\n",
      "34380 [D loss: 0.999986] [G loss: 1.000050]\n",
      "34390 [D loss: 0.999961] [G loss: 1.000065]\n",
      "34400 [D loss: 0.999976] [G loss: 1.000044]\n",
      "34410 [D loss: 0.999975] [G loss: 1.000041]\n",
      "34420 [D loss: 0.999964] [G loss: 1.000051]\n",
      "34430 [D loss: 0.999968] [G loss: 1.000068]\n",
      "34440 [D loss: 0.999980] [G loss: 1.000040]\n",
      "34450 [D loss: 0.999962] [G loss: 1.000063]\n",
      "34460 [D loss: 0.999971] [G loss: 1.000063]\n",
      "34470 [D loss: 0.999960] [G loss: 1.000053]\n",
      "34480 [D loss: 0.999973] [G loss: 1.000047]\n",
      "34490 [D loss: 0.999958] [G loss: 1.000062]\n",
      "34500 [D loss: 0.999968] [G loss: 1.000080]\n",
      "34510 [D loss: 0.999999] [G loss: 1.000038]\n",
      "34520 [D loss: 0.999962] [G loss: 1.000079]\n",
      "34530 [D loss: 0.999959] [G loss: 1.000086]\n",
      "34540 [D loss: 0.999954] [G loss: 1.000041]\n",
      "34550 [D loss: 0.999970] [G loss: 1.000055]\n",
      "34560 [D loss: 0.999974] [G loss: 1.000018]\n",
      "34570 [D loss: 0.999973] [G loss: 1.000041]\n",
      "34580 [D loss: 0.999970] [G loss: 1.000066]\n",
      "34590 [D loss: 0.999964] [G loss: 1.000054]\n",
      "34600 [D loss: 0.999969] [G loss: 1.000053]\n",
      "34610 [D loss: 0.999969] [G loss: 1.000057]\n",
      "34620 [D loss: 0.999977] [G loss: 1.000051]\n",
      "34630 [D loss: 0.999957] [G loss: 1.000080]\n",
      "34640 [D loss: 0.999951] [G loss: 1.000054]\n",
      "34650 [D loss: 0.999951] [G loss: 1.000074]\n",
      "34660 [D loss: 0.999964] [G loss: 1.000062]\n",
      "34670 [D loss: 0.999956] [G loss: 1.000060]\n",
      "34680 [D loss: 0.999959] [G loss: 1.000074]\n",
      "34690 [D loss: 0.999956] [G loss: 1.000075]\n",
      "34700 [D loss: 0.999960] [G loss: 1.000077]\n",
      "34710 [D loss: 0.999970] [G loss: 1.000034]\n",
      "34720 [D loss: 0.999960] [G loss: 1.000045]\n",
      "34730 [D loss: 0.999959] [G loss: 1.000056]\n",
      "34740 [D loss: 0.999959] [G loss: 1.000069]\n",
      "34750 [D loss: 0.999975] [G loss: 1.000066]\n",
      "34760 [D loss: 0.999952] [G loss: 1.000058]\n",
      "34770 [D loss: 0.999966] [G loss: 1.000036]\n",
      "34780 [D loss: 0.999956] [G loss: 1.000062]\n",
      "34790 [D loss: 0.999971] [G loss: 1.000067]\n",
      "34800 [D loss: 0.999962] [G loss: 1.000061]\n",
      "34810 [D loss: 0.999953] [G loss: 1.000058]\n",
      "34820 [D loss: 0.999950] [G loss: 1.000065]\n",
      "34830 [D loss: 0.999967] [G loss: 1.000063]\n",
      "34840 [D loss: 0.999962] [G loss: 1.000060]\n",
      "34850 [D loss: 0.999952] [G loss: 1.000075]\n",
      "34860 [D loss: 0.999964] [G loss: 1.000052]\n",
      "34870 [D loss: 0.999964] [G loss: 1.000051]\n",
      "34880 [D loss: 0.999968] [G loss: 1.000078]\n",
      "34890 [D loss: 0.999957] [G loss: 1.000075]\n",
      "34900 [D loss: 0.999966] [G loss: 1.000062]\n",
      "34910 [D loss: 0.999970] [G loss: 1.000053]\n",
      "34920 [D loss: 0.999953] [G loss: 1.000058]\n",
      "34930 [D loss: 0.999963] [G loss: 1.000072]\n",
      "34940 [D loss: 0.999953] [G loss: 1.000043]\n",
      "34950 [D loss: 0.999970] [G loss: 1.000040]\n",
      "34960 [D loss: 0.999965] [G loss: 1.000058]\n",
      "34970 [D loss: 0.999962] [G loss: 1.000052]\n",
      "34980 [D loss: 0.999975] [G loss: 1.000057]\n",
      "34990 [D loss: 0.999964] [G loss: 1.000056]\n",
      "35000 [D loss: 0.999960] [G loss: 1.000045]\n",
      "35010 [D loss: 0.999969] [G loss: 1.000066]\n",
      "35020 [D loss: 0.999963] [G loss: 1.000050]\n",
      "35030 [D loss: 0.999979] [G loss: 1.000050]\n",
      "35040 [D loss: 0.999956] [G loss: 1.000065]\n",
      "35050 [D loss: 0.999969] [G loss: 1.000061]\n",
      "35060 [D loss: 0.999971] [G loss: 1.000056]\n",
      "35070 [D loss: 0.999957] [G loss: 1.000063]\n",
      "35080 [D loss: 0.999962] [G loss: 1.000060]\n",
      "35090 [D loss: 0.999970] [G loss: 1.000066]\n",
      "35100 [D loss: 0.999964] [G loss: 1.000053]\n",
      "35110 [D loss: 0.999974] [G loss: 1.000072]\n",
      "35120 [D loss: 0.999963] [G loss: 1.000058]\n",
      "35130 [D loss: 0.999967] [G loss: 1.000067]\n",
      "35140 [D loss: 0.999957] [G loss: 1.000043]\n",
      "35150 [D loss: 0.999959] [G loss: 1.000048]\n",
      "35160 [D loss: 0.999951] [G loss: 1.000063]\n",
      "35170 [D loss: 0.999963] [G loss: 1.000077]\n",
      "35180 [D loss: 0.999975] [G loss: 1.000080]\n",
      "35190 [D loss: 0.999954] [G loss: 1.000060]\n",
      "35200 [D loss: 0.999970] [G loss: 1.000054]\n",
      "35210 [D loss: 0.999961] [G loss: 1.000073]\n",
      "35220 [D loss: 0.999973] [G loss: 1.000041]\n",
      "35230 [D loss: 0.999970] [G loss: 1.000060]\n",
      "35240 [D loss: 0.999973] [G loss: 1.000080]\n",
      "35250 [D loss: 0.999952] [G loss: 1.000054]\n",
      "35260 [D loss: 0.999968] [G loss: 1.000062]\n",
      "35270 [D loss: 0.999966] [G loss: 1.000032]\n",
      "35280 [D loss: 0.999967] [G loss: 1.000066]\n",
      "35290 [D loss: 0.999965] [G loss: 1.000072]\n",
      "35300 [D loss: 0.999962] [G loss: 1.000029]\n",
      "35310 [D loss: 0.999968] [G loss: 1.000065]\n",
      "35320 [D loss: 0.999973] [G loss: 1.000053]\n",
      "35330 [D loss: 0.999971] [G loss: 1.000070]\n",
      "35340 [D loss: 0.999968] [G loss: 1.000096]\n",
      "35350 [D loss: 0.999965] [G loss: 1.000054]\n",
      "35360 [D loss: 0.999962] [G loss: 1.000068]\n",
      "35370 [D loss: 0.999962] [G loss: 1.000058]\n",
      "35380 [D loss: 0.999965] [G loss: 1.000062]\n",
      "35390 [D loss: 0.999980] [G loss: 1.000069]\n",
      "35400 [D loss: 0.999966] [G loss: 1.000066]\n",
      "35410 [D loss: 0.999960] [G loss: 1.000063]\n",
      "35420 [D loss: 0.999962] [G loss: 1.000054]\n",
      "35430 [D loss: 0.999969] [G loss: 1.000061]\n",
      "35440 [D loss: 0.999970] [G loss: 1.000060]\n",
      "35450 [D loss: 0.999984] [G loss: 1.000069]\n",
      "35460 [D loss: 0.999965] [G loss: 1.000068]\n",
      "35470 [D loss: 0.999967] [G loss: 1.000060]\n",
      "35480 [D loss: 0.999964] [G loss: 1.000051]\n",
      "35490 [D loss: 0.999961] [G loss: 1.000066]\n",
      "35500 [D loss: 0.999964] [G loss: 1.000058]\n",
      "35510 [D loss: 0.999972] [G loss: 1.000048]\n",
      "35520 [D loss: 0.999970] [G loss: 1.000059]\n",
      "35530 [D loss: 0.999975] [G loss: 1.000062]\n",
      "35540 [D loss: 0.999958] [G loss: 1.000063]\n",
      "35550 [D loss: 0.999961] [G loss: 1.000056]\n",
      "35560 [D loss: 0.999972] [G loss: 1.000049]\n",
      "35570 [D loss: 0.999959] [G loss: 1.000059]\n",
      "35580 [D loss: 0.999965] [G loss: 1.000054]\n",
      "35590 [D loss: 0.999960] [G loss: 1.000057]\n",
      "35600 [D loss: 0.999962] [G loss: 1.000050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35610 [D loss: 0.999960] [G loss: 1.000050]\n",
      "35620 [D loss: 0.999974] [G loss: 1.000048]\n",
      "35630 [D loss: 0.999974] [G loss: 1.000068]\n",
      "35640 [D loss: 0.999970] [G loss: 1.000079]\n",
      "35650 [D loss: 0.999959] [G loss: 1.000060]\n",
      "35660 [D loss: 0.999974] [G loss: 1.000056]\n",
      "35670 [D loss: 0.999957] [G loss: 1.000066]\n",
      "35680 [D loss: 0.999962] [G loss: 1.000059]\n",
      "35690 [D loss: 0.999973] [G loss: 1.000054]\n",
      "35700 [D loss: 0.999971] [G loss: 1.000056]\n",
      "35710 [D loss: 0.999966] [G loss: 1.000065]\n",
      "35720 [D loss: 0.999963] [G loss: 1.000062]\n",
      "35730 [D loss: 0.999964] [G loss: 1.000049]\n",
      "35740 [D loss: 0.999954] [G loss: 1.000062]\n",
      "35750 [D loss: 0.999957] [G loss: 1.000046]\n",
      "35760 [D loss: 0.999970] [G loss: 1.000072]\n",
      "35770 [D loss: 0.999968] [G loss: 1.000065]\n",
      "35780 [D loss: 0.999967] [G loss: 1.000082]\n",
      "35790 [D loss: 0.999962] [G loss: 1.000041]\n",
      "35800 [D loss: 0.999983] [G loss: 1.000064]\n",
      "35810 [D loss: 0.999976] [G loss: 1.000069]\n",
      "35820 [D loss: 0.999963] [G loss: 1.000053]\n",
      "35830 [D loss: 0.999977] [G loss: 1.000075]\n",
      "35840 [D loss: 0.999971] [G loss: 1.000064]\n",
      "35850 [D loss: 0.999981] [G loss: 1.000057]\n",
      "35860 [D loss: 0.999974] [G loss: 1.000061]\n",
      "35870 [D loss: 0.999962] [G loss: 1.000030]\n",
      "35880 [D loss: 0.999960] [G loss: 1.000046]\n",
      "35890 [D loss: 0.999954] [G loss: 1.000077]\n",
      "35900 [D loss: 0.999960] [G loss: 1.000081]\n",
      "35910 [D loss: 0.999976] [G loss: 1.000047]\n",
      "35920 [D loss: 0.999964] [G loss: 1.000065]\n",
      "35930 [D loss: 0.999963] [G loss: 1.000068]\n",
      "35940 [D loss: 0.999963] [G loss: 1.000036]\n",
      "35950 [D loss: 0.999964] [G loss: 1.000059]\n",
      "35960 [D loss: 0.999949] [G loss: 1.000076]\n",
      "35970 [D loss: 0.999982] [G loss: 1.000048]\n",
      "35980 [D loss: 0.999966] [G loss: 1.000071]\n",
      "35990 [D loss: 0.999967] [G loss: 1.000065]\n",
      "36000 [D loss: 0.999973] [G loss: 1.000081]\n",
      "36010 [D loss: 0.999968] [G loss: 1.000070]\n",
      "36020 [D loss: 0.999972] [G loss: 1.000056]\n",
      "36030 [D loss: 0.999968] [G loss: 1.000058]\n",
      "36040 [D loss: 0.999962] [G loss: 1.000067]\n",
      "36050 [D loss: 0.999968] [G loss: 1.000066]\n",
      "36060 [D loss: 0.999966] [G loss: 1.000050]\n",
      "36070 [D loss: 0.999951] [G loss: 1.000070]\n",
      "36080 [D loss: 0.999962] [G loss: 1.000069]\n",
      "36090 [D loss: 0.999973] [G loss: 1.000084]\n",
      "36100 [D loss: 0.999969] [G loss: 1.000071]\n",
      "36110 [D loss: 0.999966] [G loss: 1.000054]\n",
      "36120 [D loss: 0.999958] [G loss: 1.000089]\n",
      "36130 [D loss: 0.999955] [G loss: 1.000051]\n",
      "36140 [D loss: 0.999983] [G loss: 1.000066]\n",
      "36150 [D loss: 0.999972] [G loss: 1.000060]\n",
      "36160 [D loss: 0.999973] [G loss: 1.000084]\n",
      "36170 [D loss: 0.999970] [G loss: 1.000049]\n",
      "36180 [D loss: 0.999954] [G loss: 1.000051]\n",
      "36190 [D loss: 0.999968] [G loss: 1.000069]\n",
      "36200 [D loss: 0.999969] [G loss: 1.000070]\n",
      "36210 [D loss: 0.999964] [G loss: 1.000084]\n",
      "36220 [D loss: 0.999981] [G loss: 1.000061]\n",
      "36230 [D loss: 0.999969] [G loss: 1.000068]\n",
      "36240 [D loss: 0.999967] [G loss: 1.000082]\n",
      "36250 [D loss: 0.999967] [G loss: 1.000062]\n",
      "36260 [D loss: 0.999969] [G loss: 1.000052]\n",
      "36270 [D loss: 0.999967] [G loss: 1.000042]\n",
      "36280 [D loss: 0.999953] [G loss: 1.000073]\n",
      "36290 [D loss: 0.999958] [G loss: 1.000054]\n",
      "36300 [D loss: 0.999974] [G loss: 1.000069]\n",
      "36310 [D loss: 0.999964] [G loss: 1.000095]\n",
      "36320 [D loss: 0.999954] [G loss: 1.000058]\n",
      "36330 [D loss: 0.999955] [G loss: 1.000063]\n",
      "36340 [D loss: 0.999962] [G loss: 1.000061]\n",
      "36350 [D loss: 0.999980] [G loss: 1.000055]\n",
      "36360 [D loss: 0.999960] [G loss: 1.000065]\n",
      "36370 [D loss: 0.999954] [G loss: 1.000060]\n",
      "36380 [D loss: 0.999965] [G loss: 1.000067]\n",
      "36390 [D loss: 0.999957] [G loss: 1.000059]\n",
      "36400 [D loss: 0.999967] [G loss: 1.000071]\n",
      "36410 [D loss: 0.999963] [G loss: 1.000064]\n",
      "36420 [D loss: 0.999965] [G loss: 1.000056]\n",
      "36430 [D loss: 0.999956] [G loss: 1.000063]\n",
      "36440 [D loss: 0.999962] [G loss: 1.000052]\n",
      "36450 [D loss: 0.999967] [G loss: 1.000073]\n",
      "36460 [D loss: 0.999946] [G loss: 1.000057]\n",
      "36470 [D loss: 0.999955] [G loss: 1.000075]\n",
      "36480 [D loss: 0.999971] [G loss: 1.000072]\n",
      "36490 [D loss: 0.999947] [G loss: 1.000077]\n",
      "36500 [D loss: 0.999973] [G loss: 1.000048]\n",
      "36510 [D loss: 0.999965] [G loss: 1.000053]\n",
      "36520 [D loss: 0.999967] [G loss: 1.000070]\n",
      "36530 [D loss: 0.999976] [G loss: 1.000061]\n",
      "36540 [D loss: 0.999963] [G loss: 1.000059]\n",
      "36550 [D loss: 0.999956] [G loss: 1.000049]\n",
      "36560 [D loss: 0.999959] [G loss: 1.000062]\n",
      "36570 [D loss: 0.999969] [G loss: 1.000045]\n",
      "36580 [D loss: 0.999981] [G loss: 1.000053]\n",
      "36590 [D loss: 0.999970] [G loss: 1.000042]\n",
      "36600 [D loss: 0.999955] [G loss: 1.000063]\n",
      "36610 [D loss: 0.999962] [G loss: 1.000049]\n",
      "36620 [D loss: 0.999954] [G loss: 1.000052]\n",
      "36630 [D loss: 0.999968] [G loss: 1.000067]\n",
      "36640 [D loss: 0.999962] [G loss: 1.000063]\n",
      "36650 [D loss: 0.999968] [G loss: 1.000064]\n",
      "36660 [D loss: 0.999967] [G loss: 1.000074]\n",
      "36670 [D loss: 0.999973] [G loss: 1.000061]\n",
      "36680 [D loss: 0.999956] [G loss: 1.000084]\n",
      "36690 [D loss: 0.999960] [G loss: 1.000064]\n",
      "36700 [D loss: 0.999951] [G loss: 1.000053]\n",
      "36710 [D loss: 0.999972] [G loss: 1.000047]\n",
      "36720 [D loss: 0.999970] [G loss: 1.000056]\n",
      "36730 [D loss: 0.999958] [G loss: 1.000069]\n",
      "36740 [D loss: 0.999956] [G loss: 1.000054]\n",
      "36750 [D loss: 0.999965] [G loss: 1.000064]\n",
      "36760 [D loss: 0.999970] [G loss: 1.000065]\n",
      "36770 [D loss: 0.999954] [G loss: 1.000069]\n",
      "36780 [D loss: 0.999948] [G loss: 1.000070]\n",
      "36790 [D loss: 0.999969] [G loss: 1.000069]\n",
      "36800 [D loss: 0.999965] [G loss: 1.000066]\n",
      "36810 [D loss: 0.999959] [G loss: 1.000069]\n",
      "36820 [D loss: 0.999953] [G loss: 1.000067]\n",
      "36830 [D loss: 0.999968] [G loss: 1.000062]\n",
      "36840 [D loss: 0.999965] [G loss: 1.000071]\n",
      "36850 [D loss: 0.999964] [G loss: 1.000059]\n",
      "36860 [D loss: 0.999970] [G loss: 1.000055]\n",
      "36870 [D loss: 0.999963] [G loss: 1.000053]\n",
      "36880 [D loss: 0.999972] [G loss: 1.000040]\n",
      "36890 [D loss: 0.999966] [G loss: 1.000065]\n",
      "36900 [D loss: 0.999970] [G loss: 1.000065]\n",
      "36910 [D loss: 0.999965] [G loss: 1.000068]\n",
      "36920 [D loss: 0.999968] [G loss: 1.000076]\n",
      "36930 [D loss: 0.999966] [G loss: 1.000060]\n",
      "36940 [D loss: 0.999968] [G loss: 1.000060]\n",
      "36950 [D loss: 0.999961] [G loss: 1.000063]\n",
      "36960 [D loss: 0.999963] [G loss: 1.000077]\n",
      "36970 [D loss: 0.999973] [G loss: 1.000057]\n",
      "36980 [D loss: 0.999968] [G loss: 1.000062]\n",
      "36990 [D loss: 0.999964] [G loss: 1.000056]\n",
      "37000 [D loss: 0.999967] [G loss: 1.000072]\n",
      "37010 [D loss: 0.999969] [G loss: 1.000062]\n",
      "37020 [D loss: 0.999969] [G loss: 1.000066]\n",
      "37030 [D loss: 0.999962] [G loss: 1.000064]\n",
      "37040 [D loss: 0.999967] [G loss: 1.000060]\n",
      "37050 [D loss: 0.999965] [G loss: 1.000058]\n",
      "37060 [D loss: 0.999973] [G loss: 1.000048]\n",
      "37070 [D loss: 0.999969] [G loss: 1.000065]\n",
      "37080 [D loss: 0.999966] [G loss: 1.000055]\n",
      "37090 [D loss: 0.999964] [G loss: 1.000069]\n",
      "37100 [D loss: 0.999966] [G loss: 1.000060]\n",
      "37110 [D loss: 0.999965] [G loss: 1.000055]\n",
      "37120 [D loss: 0.999962] [G loss: 1.000054]\n",
      "37130 [D loss: 0.999959] [G loss: 1.000061]\n",
      "37140 [D loss: 0.999963] [G loss: 1.000065]\n",
      "37150 [D loss: 0.999971] [G loss: 1.000069]\n",
      "37160 [D loss: 0.999962] [G loss: 1.000066]\n",
      "37170 [D loss: 0.999972] [G loss: 1.000065]\n",
      "37180 [D loss: 0.999965] [G loss: 1.000062]\n",
      "37190 [D loss: 0.999953] [G loss: 1.000065]\n",
      "37200 [D loss: 0.999974] [G loss: 1.000063]\n",
      "37210 [D loss: 0.999965] [G loss: 1.000061]\n",
      "37220 [D loss: 0.999966] [G loss: 1.000073]\n",
      "37230 [D loss: 0.999969] [G loss: 1.000068]\n",
      "37240 [D loss: 0.999960] [G loss: 1.000066]\n",
      "37250 [D loss: 0.999968] [G loss: 1.000068]\n",
      "37260 [D loss: 0.999967] [G loss: 1.000059]\n",
      "37270 [D loss: 0.999965] [G loss: 1.000060]\n",
      "37280 [D loss: 0.999959] [G loss: 1.000056]\n",
      "37290 [D loss: 0.999971] [G loss: 1.000069]\n",
      "37300 [D loss: 0.999955] [G loss: 1.000064]\n",
      "37310 [D loss: 0.999970] [G loss: 1.000059]\n",
      "37320 [D loss: 0.999966] [G loss: 1.000068]\n",
      "37330 [D loss: 0.999958] [G loss: 1.000051]\n",
      "37340 [D loss: 0.999969] [G loss: 1.000064]\n",
      "37350 [D loss: 0.999970] [G loss: 1.000064]\n",
      "37360 [D loss: 0.999969] [G loss: 1.000063]\n",
      "37370 [D loss: 0.999966] [G loss: 1.000056]\n",
      "37380 [D loss: 0.999966] [G loss: 1.000068]\n",
      "37390 [D loss: 0.999964] [G loss: 1.000061]\n",
      "37400 [D loss: 0.999968] [G loss: 1.000062]\n",
      "37410 [D loss: 0.999966] [G loss: 1.000060]\n",
      "37420 [D loss: 0.999977] [G loss: 1.000053]\n",
      "37430 [D loss: 0.999954] [G loss: 1.000060]\n",
      "37440 [D loss: 0.999961] [G loss: 1.000053]\n",
      "37450 [D loss: 0.999953] [G loss: 1.000058]\n",
      "37460 [D loss: 0.999962] [G loss: 1.000057]\n",
      "37470 [D loss: 0.999968] [G loss: 1.000057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37480 [D loss: 0.999959] [G loss: 1.000077]\n",
      "37490 [D loss: 0.999973] [G loss: 1.000048]\n",
      "37500 [D loss: 0.999972] [G loss: 1.000070]\n",
      "37510 [D loss: 0.999966] [G loss: 1.000050]\n",
      "37520 [D loss: 0.999964] [G loss: 1.000064]\n",
      "37530 [D loss: 0.999957] [G loss: 1.000048]\n",
      "37540 [D loss: 0.999958] [G loss: 1.000061]\n",
      "37550 [D loss: 0.999970] [G loss: 1.000060]\n",
      "37560 [D loss: 0.999967] [G loss: 1.000056]\n",
      "37570 [D loss: 0.999974] [G loss: 1.000063]\n",
      "37580 [D loss: 0.999973] [G loss: 1.000046]\n",
      "37590 [D loss: 0.999961] [G loss: 1.000060]\n",
      "37600 [D loss: 0.999961] [G loss: 1.000065]\n",
      "37610 [D loss: 0.999964] [G loss: 1.000060]\n",
      "37620 [D loss: 0.999965] [G loss: 1.000060]\n",
      "37630 [D loss: 0.999956] [G loss: 1.000049]\n",
      "37640 [D loss: 0.999972] [G loss: 1.000063]\n",
      "37650 [D loss: 0.999968] [G loss: 1.000055]\n",
      "37660 [D loss: 0.999964] [G loss: 1.000050]\n",
      "37670 [D loss: 0.999973] [G loss: 1.000055]\n",
      "37680 [D loss: 0.999962] [G loss: 1.000065]\n",
      "37690 [D loss: 0.999968] [G loss: 1.000051]\n",
      "37700 [D loss: 0.999957] [G loss: 1.000064]\n",
      "37710 [D loss: 0.999959] [G loss: 1.000073]\n",
      "37720 [D loss: 0.999960] [G loss: 1.000068]\n",
      "37730 [D loss: 0.999957] [G loss: 1.000060]\n",
      "37740 [D loss: 0.999962] [G loss: 1.000059]\n",
      "37750 [D loss: 0.999964] [G loss: 1.000064]\n",
      "37760 [D loss: 0.999961] [G loss: 1.000065]\n",
      "37770 [D loss: 0.999970] [G loss: 1.000067]\n",
      "37780 [D loss: 0.999962] [G loss: 1.000063]\n",
      "37790 [D loss: 0.999965] [G loss: 1.000075]\n",
      "37800 [D loss: 0.999973] [G loss: 1.000053]\n",
      "37810 [D loss: 0.999966] [G loss: 1.000063]\n",
      "37820 [D loss: 0.999961] [G loss: 1.000058]\n",
      "37830 [D loss: 0.999964] [G loss: 1.000069]\n",
      "37840 [D loss: 0.999966] [G loss: 1.000060]\n",
      "37850 [D loss: 0.999965] [G loss: 1.000060]\n",
      "37860 [D loss: 0.999963] [G loss: 1.000067]\n",
      "37870 [D loss: 0.999973] [G loss: 1.000058]\n",
      "37880 [D loss: 0.999971] [G loss: 1.000059]\n",
      "37890 [D loss: 0.999968] [G loss: 1.000066]\n",
      "37900 [D loss: 0.999964] [G loss: 1.000065]\n",
      "37910 [D loss: 0.999967] [G loss: 1.000054]\n",
      "37920 [D loss: 0.999962] [G loss: 1.000072]\n",
      "37930 [D loss: 0.999960] [G loss: 1.000067]\n",
      "37940 [D loss: 0.999964] [G loss: 1.000063]\n",
      "37950 [D loss: 0.999967] [G loss: 1.000069]\n",
      "37960 [D loss: 0.999963] [G loss: 1.000060]\n",
      "37970 [D loss: 0.999971] [G loss: 1.000062]\n",
      "37980 [D loss: 0.999965] [G loss: 1.000069]\n",
      "37990 [D loss: 0.999965] [G loss: 1.000070]\n",
      "38000 [D loss: 0.999963] [G loss: 1.000062]\n",
      "38010 [D loss: 0.999968] [G loss: 1.000073]\n",
      "38020 [D loss: 0.999964] [G loss: 1.000051]\n",
      "38030 [D loss: 0.999970] [G loss: 1.000060]\n",
      "38040 [D loss: 0.999963] [G loss: 1.000062]\n",
      "38050 [D loss: 0.999966] [G loss: 1.000065]\n",
      "38060 [D loss: 0.999961] [G loss: 1.000070]\n",
      "38070 [D loss: 0.999967] [G loss: 1.000064]\n",
      "38080 [D loss: 0.999976] [G loss: 1.000047]\n",
      "38090 [D loss: 0.999965] [G loss: 1.000059]\n",
      "38100 [D loss: 0.999958] [G loss: 1.000068]\n",
      "38110 [D loss: 0.999973] [G loss: 1.000062]\n",
      "38120 [D loss: 0.999962] [G loss: 1.000075]\n",
      "38130 [D loss: 0.999973] [G loss: 1.000068]\n",
      "38140 [D loss: 0.999966] [G loss: 1.000060]\n",
      "38150 [D loss: 0.999969] [G loss: 1.000063]\n",
      "38160 [D loss: 0.999971] [G loss: 1.000064]\n",
      "38170 [D loss: 0.999969] [G loss: 1.000053]\n",
      "38180 [D loss: 0.999968] [G loss: 1.000057]\n",
      "38190 [D loss: 0.999970] [G loss: 1.000052]\n",
      "38200 [D loss: 0.999968] [G loss: 1.000052]\n",
      "38210 [D loss: 0.999965] [G loss: 1.000067]\n",
      "38220 [D loss: 0.999961] [G loss: 1.000064]\n",
      "38230 [D loss: 0.999967] [G loss: 1.000059]\n",
      "38240 [D loss: 0.999966] [G loss: 1.000054]\n",
      "38250 [D loss: 0.999962] [G loss: 1.000063]\n",
      "38260 [D loss: 0.999968] [G loss: 1.000066]\n",
      "38270 [D loss: 0.999965] [G loss: 1.000067]\n",
      "38280 [D loss: 0.999963] [G loss: 1.000067]\n",
      "38290 [D loss: 0.999969] [G loss: 1.000058]\n",
      "38300 [D loss: 0.999965] [G loss: 1.000069]\n",
      "38310 [D loss: 0.999962] [G loss: 1.000059]\n",
      "38320 [D loss: 0.999970] [G loss: 1.000068]\n",
      "38330 [D loss: 0.999962] [G loss: 1.000067]\n",
      "38340 [D loss: 0.999981] [G loss: 1.000054]\n",
      "38350 [D loss: 0.999970] [G loss: 1.000055]\n",
      "38360 [D loss: 0.999957] [G loss: 1.000066]\n",
      "38370 [D loss: 0.999969] [G loss: 1.000051]\n",
      "38380 [D loss: 0.999983] [G loss: 1.000069]\n",
      "38390 [D loss: 0.999958] [G loss: 1.000063]\n",
      "38400 [D loss: 0.999960] [G loss: 1.000048]\n",
      "38410 [D loss: 0.999966] [G loss: 1.000051]\n",
      "38420 [D loss: 0.999966] [G loss: 1.000047]\n",
      "38430 [D loss: 0.999980] [G loss: 1.000061]\n",
      "38440 [D loss: 0.999961] [G loss: 1.000090]\n",
      "38450 [D loss: 0.999972] [G loss: 1.000059]\n",
      "38460 [D loss: 0.999961] [G loss: 1.000068]\n",
      "38470 [D loss: 0.999970] [G loss: 1.000056]\n",
      "38480 [D loss: 0.999972] [G loss: 1.000044]\n",
      "38490 [D loss: 0.999972] [G loss: 1.000055]\n",
      "38500 [D loss: 0.999968] [G loss: 1.000044]\n",
      "38510 [D loss: 0.999967] [G loss: 1.000065]\n",
      "38520 [D loss: 0.999981] [G loss: 1.000077]\n",
      "38530 [D loss: 0.999973] [G loss: 1.000074]\n",
      "38540 [D loss: 0.999973] [G loss: 1.000069]\n",
      "38550 [D loss: 0.999973] [G loss: 1.000060]\n",
      "38560 [D loss: 0.999959] [G loss: 1.000067]\n",
      "38570 [D loss: 0.999959] [G loss: 1.000063]\n",
      "38580 [D loss: 0.999966] [G loss: 1.000070]\n",
      "38590 [D loss: 0.999955] [G loss: 1.000057]\n",
      "38600 [D loss: 0.999971] [G loss: 1.000051]\n",
      "38610 [D loss: 0.999966] [G loss: 1.000065]\n",
      "38620 [D loss: 0.999965] [G loss: 1.000070]\n",
      "38630 [D loss: 0.999956] [G loss: 1.000050]\n",
      "38640 [D loss: 0.999973] [G loss: 1.000073]\n",
      "38650 [D loss: 0.999980] [G loss: 1.000059]\n",
      "38660 [D loss: 0.999961] [G loss: 1.000056]\n",
      "38670 [D loss: 0.999964] [G loss: 1.000076]\n",
      "38680 [D loss: 0.999977] [G loss: 1.000052]\n",
      "38690 [D loss: 0.999960] [G loss: 1.000067]\n",
      "38700 [D loss: 0.999967] [G loss: 1.000060]\n",
      "38710 [D loss: 0.999960] [G loss: 1.000050]\n",
      "38720 [D loss: 0.999965] [G loss: 1.000057]\n",
      "38730 [D loss: 0.999956] [G loss: 1.000042]\n",
      "38740 [D loss: 0.999963] [G loss: 1.000054]\n",
      "38750 [D loss: 0.999976] [G loss: 1.000051]\n",
      "38760 [D loss: 0.999949] [G loss: 1.000067]\n",
      "38770 [D loss: 0.999958] [G loss: 1.000044]\n",
      "38780 [D loss: 0.999960] [G loss: 1.000058]\n",
      "38790 [D loss: 0.999958] [G loss: 1.000082]\n",
      "38800 [D loss: 0.999961] [G loss: 1.000059]\n",
      "38810 [D loss: 0.999967] [G loss: 1.000053]\n",
      "38820 [D loss: 0.999966] [G loss: 1.000057]\n",
      "38830 [D loss: 0.999971] [G loss: 1.000072]\n",
      "38840 [D loss: 0.999973] [G loss: 1.000054]\n",
      "38850 [D loss: 0.999961] [G loss: 1.000057]\n",
      "38860 [D loss: 0.999974] [G loss: 1.000069]\n",
      "38870 [D loss: 0.999977] [G loss: 1.000066]\n",
      "38880 [D loss: 0.999972] [G loss: 1.000040]\n",
      "38890 [D loss: 0.999952] [G loss: 1.000048]\n",
      "38900 [D loss: 0.999949] [G loss: 1.000080]\n",
      "38910 [D loss: 0.999946] [G loss: 1.000050]\n",
      "38920 [D loss: 0.999966] [G loss: 1.000076]\n",
      "38930 [D loss: 0.999967] [G loss: 1.000066]\n",
      "38940 [D loss: 0.999953] [G loss: 1.000079]\n",
      "38950 [D loss: 0.999980] [G loss: 1.000071]\n",
      "38960 [D loss: 0.999960] [G loss: 1.000052]\n",
      "38970 [D loss: 0.999977] [G loss: 1.000079]\n",
      "38980 [D loss: 0.999959] [G loss: 1.000068]\n",
      "38990 [D loss: 0.999974] [G loss: 1.000053]\n",
      "39000 [D loss: 0.999980] [G loss: 1.000067]\n",
      "39010 [D loss: 0.999975] [G loss: 1.000046]\n",
      "39020 [D loss: 1.000000] [G loss: 1.000072]\n",
      "39030 [D loss: 0.999965] [G loss: 1.000063]\n",
      "39040 [D loss: 0.999955] [G loss: 1.000038]\n",
      "39050 [D loss: 0.999969] [G loss: 1.000070]\n",
      "39060 [D loss: 0.999971] [G loss: 1.000075]\n",
      "39070 [D loss: 0.999967] [G loss: 1.000068]\n",
      "39080 [D loss: 0.999970] [G loss: 1.000076]\n",
      "39090 [D loss: 0.999946] [G loss: 1.000071]\n",
      "39100 [D loss: 0.999976] [G loss: 1.000044]\n",
      "39110 [D loss: 0.999964] [G loss: 1.000059]\n",
      "39120 [D loss: 0.999972] [G loss: 1.000051]\n",
      "39130 [D loss: 0.999976] [G loss: 1.000074]\n",
      "39140 [D loss: 0.999974] [G loss: 1.000056]\n",
      "39150 [D loss: 0.999966] [G loss: 1.000039]\n",
      "39160 [D loss: 0.999965] [G loss: 1.000022]\n",
      "39170 [D loss: 0.999974] [G loss: 1.000047]\n",
      "39180 [D loss: 0.999974] [G loss: 1.000061]\n",
      "39190 [D loss: 0.999964] [G loss: 1.000018]\n",
      "39200 [D loss: 0.999962] [G loss: 1.000076]\n",
      "39210 [D loss: 0.999962] [G loss: 1.000069]\n",
      "39220 [D loss: 0.999956] [G loss: 1.000052]\n",
      "39230 [D loss: 0.999974] [G loss: 1.000064]\n",
      "39240 [D loss: 0.999971] [G loss: 1.000072]\n",
      "39250 [D loss: 0.999985] [G loss: 1.000049]\n",
      "39260 [D loss: 0.999962] [G loss: 1.000062]\n",
      "39270 [D loss: 0.999961] [G loss: 1.000076]\n",
      "39280 [D loss: 0.999949] [G loss: 1.000058]\n",
      "39290 [D loss: 0.999981] [G loss: 1.000054]\n",
      "39300 [D loss: 0.999968] [G loss: 1.000073]\n",
      "39310 [D loss: 0.999955] [G loss: 1.000067]\n",
      "39320 [D loss: 0.999962] [G loss: 1.000075]\n",
      "39330 [D loss: 0.999968] [G loss: 1.000060]\n",
      "39340 [D loss: 0.999955] [G loss: 1.000052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39350 [D loss: 0.999966] [G loss: 1.000050]\n",
      "39360 [D loss: 0.999960] [G loss: 1.000064]\n",
      "39370 [D loss: 0.999962] [G loss: 1.000072]\n",
      "39380 [D loss: 0.999967] [G loss: 1.000073]\n",
      "39390 [D loss: 0.999976] [G loss: 1.000063]\n",
      "39400 [D loss: 0.999973] [G loss: 1.000067]\n",
      "39410 [D loss: 0.999963] [G loss: 1.000068]\n",
      "39420 [D loss: 0.999952] [G loss: 1.000062]\n",
      "39430 [D loss: 0.999978] [G loss: 1.000064]\n",
      "39440 [D loss: 0.999963] [G loss: 1.000071]\n",
      "39450 [D loss: 0.999973] [G loss: 1.000066]\n",
      "39460 [D loss: 0.999953] [G loss: 1.000067]\n",
      "39470 [D loss: 0.999978] [G loss: 1.000076]\n",
      "39480 [D loss: 0.999960] [G loss: 1.000090]\n",
      "39490 [D loss: 0.999968] [G loss: 1.000064]\n",
      "39500 [D loss: 0.999966] [G loss: 1.000061]\n",
      "39510 [D loss: 0.999961] [G loss: 1.000040]\n",
      "39520 [D loss: 0.999973] [G loss: 1.000050]\n",
      "39530 [D loss: 0.999964] [G loss: 1.000076]\n",
      "39540 [D loss: 0.999950] [G loss: 1.000078]\n",
      "39550 [D loss: 0.999963] [G loss: 1.000064]\n",
      "39560 [D loss: 0.999958] [G loss: 1.000061]\n",
      "39570 [D loss: 0.999953] [G loss: 1.000058]\n",
      "39580 [D loss: 0.999964] [G loss: 1.000067]\n",
      "39590 [D loss: 0.999960] [G loss: 1.000063]\n",
      "39600 [D loss: 0.999974] [G loss: 1.000037]\n",
      "39610 [D loss: 0.999958] [G loss: 1.000052]\n",
      "39620 [D loss: 0.999963] [G loss: 1.000068]\n",
      "39630 [D loss: 0.999960] [G loss: 1.000072]\n",
      "39640 [D loss: 0.999964] [G loss: 1.000074]\n",
      "39650 [D loss: 0.999959] [G loss: 1.000062]\n",
      "39660 [D loss: 0.999973] [G loss: 1.000080]\n",
      "39670 [D loss: 0.999980] [G loss: 1.000071]\n",
      "39680 [D loss: 0.999973] [G loss: 1.000044]\n",
      "39690 [D loss: 0.999959] [G loss: 1.000065]\n",
      "39700 [D loss: 0.999967] [G loss: 1.000054]\n",
      "39710 [D loss: 0.999960] [G loss: 1.000064]\n",
      "39720 [D loss: 0.999959] [G loss: 1.000062]\n",
      "39730 [D loss: 0.999964] [G loss: 1.000057]\n",
      "39740 [D loss: 0.999974] [G loss: 1.000056]\n",
      "39750 [D loss: 0.999978] [G loss: 1.000067]\n",
      "39760 [D loss: 0.999949] [G loss: 1.000066]\n",
      "39770 [D loss: 0.999973] [G loss: 1.000072]\n",
      "39780 [D loss: 0.999964] [G loss: 1.000063]\n",
      "39790 [D loss: 0.999963] [G loss: 1.000073]\n",
      "39800 [D loss: 0.999972] [G loss: 1.000052]\n",
      "39810 [D loss: 0.999982] [G loss: 1.000064]\n",
      "39820 [D loss: 0.999964] [G loss: 1.000051]\n",
      "39830 [D loss: 0.999962] [G loss: 1.000059]\n",
      "39840 [D loss: 0.999976] [G loss: 1.000068]\n",
      "39850 [D loss: 0.999958] [G loss: 1.000041]\n",
      "39860 [D loss: 0.999968] [G loss: 1.000066]\n",
      "39870 [D loss: 0.999954] [G loss: 1.000052]\n",
      "39880 [D loss: 0.999979] [G loss: 1.000043]\n",
      "39890 [D loss: 0.999966] [G loss: 1.000073]\n",
      "39900 [D loss: 0.999956] [G loss: 1.000053]\n",
      "39910 [D loss: 0.999964] [G loss: 1.000070]\n",
      "39920 [D loss: 0.999972] [G loss: 1.000085]\n",
      "39930 [D loss: 0.999959] [G loss: 1.000045]\n",
      "39940 [D loss: 0.999955] [G loss: 1.000065]\n",
      "39950 [D loss: 0.999961] [G loss: 1.000072]\n",
      "39960 [D loss: 0.999969] [G loss: 1.000065]\n",
      "39970 [D loss: 0.999974] [G loss: 1.000050]\n",
      "39980 [D loss: 0.999954] [G loss: 1.000059]\n",
      "39990 [D loss: 0.999969] [G loss: 1.000072]\n",
      "40000 [D loss: 0.999961] [G loss: 1.000064]\n",
      "40010 [D loss: 0.999953] [G loss: 1.000067]\n",
      "40020 [D loss: 0.999966] [G loss: 1.000070]\n",
      "40030 [D loss: 0.999982] [G loss: 1.000053]\n",
      "40040 [D loss: 0.999973] [G loss: 1.000059]\n",
      "40050 [D loss: 0.999961] [G loss: 1.000062]\n",
      "40060 [D loss: 0.999977] [G loss: 1.000051]\n",
      "40070 [D loss: 0.999968] [G loss: 1.000061]\n",
      "40080 [D loss: 0.999968] [G loss: 1.000050]\n",
      "40090 [D loss: 0.999967] [G loss: 1.000065]\n",
      "40100 [D loss: 0.999962] [G loss: 1.000060]\n",
      "40110 [D loss: 0.999973] [G loss: 1.000062]\n",
      "40120 [D loss: 0.999969] [G loss: 1.000076]\n",
      "40130 [D loss: 0.999966] [G loss: 1.000062]\n",
      "40140 [D loss: 0.999971] [G loss: 1.000064]\n",
      "40150 [D loss: 0.999966] [G loss: 1.000053]\n",
      "40160 [D loss: 0.999985] [G loss: 1.000056]\n",
      "40170 [D loss: 0.999963] [G loss: 1.000063]\n",
      "40180 [D loss: 0.999969] [G loss: 1.000069]\n",
      "40190 [D loss: 0.999967] [G loss: 1.000059]\n",
      "40200 [D loss: 0.999976] [G loss: 1.000076]\n",
      "40210 [D loss: 0.999965] [G loss: 1.000033]\n",
      "40220 [D loss: 0.999960] [G loss: 1.000060]\n",
      "40230 [D loss: 0.999977] [G loss: 1.000066]\n",
      "40240 [D loss: 0.999964] [G loss: 1.000067]\n",
      "40250 [D loss: 0.999979] [G loss: 1.000070]\n",
      "40260 [D loss: 0.999966] [G loss: 1.000087]\n",
      "40270 [D loss: 0.999949] [G loss: 1.000067]\n",
      "40280 [D loss: 0.999964] [G loss: 1.000057]\n",
      "40290 [D loss: 0.999980] [G loss: 1.000069]\n",
      "40300 [D loss: 0.999964] [G loss: 1.000047]\n",
      "40310 [D loss: 0.999982] [G loss: 1.000070]\n",
      "40320 [D loss: 0.999966] [G loss: 1.000080]\n",
      "40330 [D loss: 0.999963] [G loss: 1.000068]\n",
      "40340 [D loss: 0.999950] [G loss: 1.000087]\n",
      "40350 [D loss: 0.999974] [G loss: 1.000076]\n",
      "40360 [D loss: 0.999956] [G loss: 1.000062]\n",
      "40370 [D loss: 0.999968] [G loss: 1.000045]\n",
      "40380 [D loss: 0.999959] [G loss: 1.000065]\n",
      "40390 [D loss: 0.999970] [G loss: 1.000074]\n",
      "40400 [D loss: 0.999955] [G loss: 1.000072]\n",
      "40410 [D loss: 0.999960] [G loss: 1.000030]\n",
      "40420 [D loss: 0.999960] [G loss: 1.000057]\n",
      "40430 [D loss: 0.999953] [G loss: 1.000083]\n",
      "40440 [D loss: 0.999956] [G loss: 1.000058]\n",
      "40450 [D loss: 0.999943] [G loss: 1.000055]\n",
      "40460 [D loss: 0.999965] [G loss: 1.000082]\n",
      "40470 [D loss: 0.999981] [G loss: 1.000051]\n",
      "40480 [D loss: 0.999971] [G loss: 1.000072]\n",
      "40490 [D loss: 0.999984] [G loss: 1.000072]\n",
      "40500 [D loss: 0.999983] [G loss: 1.000052]\n",
      "40510 [D loss: 0.999971] [G loss: 1.000083]\n",
      "40520 [D loss: 0.999960] [G loss: 1.000039]\n",
      "40530 [D loss: 0.999959] [G loss: 1.000078]\n",
      "40540 [D loss: 0.999963] [G loss: 1.000079]\n",
      "40550 [D loss: 0.999965] [G loss: 1.000072]\n",
      "40560 [D loss: 0.999980] [G loss: 1.000100]\n",
      "40570 [D loss: 0.999980] [G loss: 1.000064]\n",
      "40580 [D loss: 0.999971] [G loss: 1.000065]\n",
      "40590 [D loss: 0.999968] [G loss: 1.000071]\n",
      "40600 [D loss: 0.999959] [G loss: 1.000051]\n",
      "40610 [D loss: 0.999960] [G loss: 1.000071]\n",
      "40620 [D loss: 0.999966] [G loss: 1.000043]\n",
      "40630 [D loss: 0.999961] [G loss: 1.000046]\n",
      "40640 [D loss: 0.999959] [G loss: 1.000056]\n",
      "40650 [D loss: 0.999958] [G loss: 1.000043]\n",
      "40660 [D loss: 0.999959] [G loss: 1.000050]\n",
      "40670 [D loss: 0.999954] [G loss: 1.000057]\n",
      "40680 [D loss: 0.999960] [G loss: 1.000048]\n",
      "40690 [D loss: 0.999966] [G loss: 1.000070]\n",
      "40700 [D loss: 0.999962] [G loss: 1.000056]\n",
      "40710 [D loss: 0.999964] [G loss: 1.000061]\n",
      "40720 [D loss: 0.999971] [G loss: 1.000051]\n",
      "40730 [D loss: 0.999957] [G loss: 1.000053]\n",
      "40740 [D loss: 0.999963] [G loss: 1.000062]\n",
      "40750 [D loss: 0.999977] [G loss: 1.000047]\n",
      "40760 [D loss: 0.999964] [G loss: 1.000065]\n",
      "40770 [D loss: 0.999965] [G loss: 1.000073]\n",
      "40780 [D loss: 0.999973] [G loss: 1.000055]\n",
      "40790 [D loss: 0.999970] [G loss: 1.000059]\n",
      "40800 [D loss: 0.999973] [G loss: 1.000057]\n",
      "40810 [D loss: 0.999966] [G loss: 1.000058]\n",
      "40820 [D loss: 0.999965] [G loss: 1.000030]\n",
      "40830 [D loss: 0.999962] [G loss: 1.000058]\n",
      "40840 [D loss: 0.999963] [G loss: 1.000073]\n",
      "40850 [D loss: 0.999977] [G loss: 1.000061]\n",
      "40860 [D loss: 0.999965] [G loss: 1.000056]\n",
      "40870 [D loss: 0.999963] [G loss: 1.000061]\n",
      "40880 [D loss: 0.999968] [G loss: 1.000056]\n",
      "40890 [D loss: 0.999975] [G loss: 1.000062]\n",
      "40900 [D loss: 0.999965] [G loss: 1.000054]\n",
      "40910 [D loss: 0.999964] [G loss: 1.000057]\n",
      "40920 [D loss: 0.999962] [G loss: 1.000082]\n",
      "40930 [D loss: 0.999961] [G loss: 1.000052]\n",
      "40940 [D loss: 0.999966] [G loss: 1.000057]\n",
      "40950 [D loss: 0.999968] [G loss: 1.000066]\n",
      "40960 [D loss: 0.999969] [G loss: 1.000068]\n",
      "40970 [D loss: 0.999966] [G loss: 1.000073]\n",
      "40980 [D loss: 0.999963] [G loss: 1.000067]\n",
      "40990 [D loss: 0.999976] [G loss: 1.000048]\n",
      "41000 [D loss: 0.999964] [G loss: 1.000051]\n",
      "41010 [D loss: 0.999965] [G loss: 1.000055]\n",
      "41020 [D loss: 0.999964] [G loss: 1.000071]\n",
      "41030 [D loss: 0.999959] [G loss: 1.000067]\n",
      "41040 [D loss: 0.999974] [G loss: 1.000061]\n",
      "41050 [D loss: 0.999965] [G loss: 1.000046]\n",
      "41060 [D loss: 0.999975] [G loss: 1.000057]\n",
      "41070 [D loss: 0.999963] [G loss: 1.000070]\n",
      "41080 [D loss: 0.999970] [G loss: 1.000059]\n",
      "41090 [D loss: 0.999967] [G loss: 1.000078]\n",
      "41100 [D loss: 0.999968] [G loss: 1.000056]\n",
      "41110 [D loss: 0.999976] [G loss: 1.000050]\n",
      "41120 [D loss: 0.999964] [G loss: 1.000067]\n",
      "41130 [D loss: 0.999964] [G loss: 1.000057]\n",
      "41140 [D loss: 0.999965] [G loss: 1.000065]\n",
      "41150 [D loss: 0.999965] [G loss: 1.000054]\n",
      "41160 [D loss: 0.999957] [G loss: 1.000062]\n",
      "41170 [D loss: 0.999949] [G loss: 1.000053]\n",
      "41180 [D loss: 0.999961] [G loss: 1.000060]\n",
      "41190 [D loss: 0.999965] [G loss: 1.000060]\n",
      "41200 [D loss: 0.999954] [G loss: 1.000060]\n",
      "41210 [D loss: 0.999966] [G loss: 1.000069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41220 [D loss: 0.999965] [G loss: 1.000070]\n",
      "41230 [D loss: 0.999967] [G loss: 1.000068]\n",
      "41240 [D loss: 0.999969] [G loss: 1.000072]\n",
      "41250 [D loss: 0.999961] [G loss: 1.000069]\n",
      "41260 [D loss: 0.999961] [G loss: 1.000079]\n",
      "41270 [D loss: 0.999961] [G loss: 1.000061]\n",
      "41280 [D loss: 0.999966] [G loss: 1.000055]\n",
      "41290 [D loss: 0.999963] [G loss: 1.000075]\n",
      "41300 [D loss: 0.999963] [G loss: 1.000070]\n",
      "41310 [D loss: 0.999960] [G loss: 1.000069]\n",
      "41320 [D loss: 0.999974] [G loss: 1.000056]\n",
      "41330 [D loss: 0.999957] [G loss: 1.000052]\n",
      "41340 [D loss: 0.999961] [G loss: 1.000058]\n",
      "41350 [D loss: 0.999961] [G loss: 1.000053]\n",
      "41360 [D loss: 0.999965] [G loss: 1.000062]\n",
      "41370 [D loss: 0.999960] [G loss: 1.000059]\n",
      "41380 [D loss: 0.999968] [G loss: 1.000065]\n",
      "41390 [D loss: 0.999972] [G loss: 1.000057]\n",
      "41400 [D loss: 0.999966] [G loss: 1.000061]\n",
      "41410 [D loss: 0.999963] [G loss: 1.000060]\n",
      "41420 [D loss: 0.999969] [G loss: 1.000054]\n",
      "41430 [D loss: 0.999966] [G loss: 1.000057]\n",
      "41440 [D loss: 0.999963] [G loss: 1.000068]\n",
      "41450 [D loss: 0.999959] [G loss: 1.000061]\n",
      "41460 [D loss: 0.999965] [G loss: 1.000063]\n",
      "41470 [D loss: 0.999972] [G loss: 1.000057]\n",
      "41480 [D loss: 0.999964] [G loss: 1.000063]\n",
      "41490 [D loss: 0.999958] [G loss: 1.000056]\n",
      "41500 [D loss: 0.999964] [G loss: 1.000066]\n",
      "41510 [D loss: 0.999963] [G loss: 1.000063]\n",
      "41520 [D loss: 0.999973] [G loss: 1.000055]\n",
      "41530 [D loss: 0.999965] [G loss: 1.000056]\n",
      "41540 [D loss: 0.999949] [G loss: 1.000054]\n",
      "41550 [D loss: 0.999968] [G loss: 1.000055]\n",
      "41560 [D loss: 0.999968] [G loss: 1.000066]\n",
      "41570 [D loss: 0.999964] [G loss: 1.000064]\n",
      "41580 [D loss: 0.999965] [G loss: 1.000075]\n",
      "41590 [D loss: 0.999969] [G loss: 1.000054]\n",
      "41600 [D loss: 0.999967] [G loss: 1.000069]\n",
      "41610 [D loss: 0.999971] [G loss: 1.000060]\n",
      "41620 [D loss: 0.999963] [G loss: 1.000062]\n",
      "41630 [D loss: 0.999960] [G loss: 1.000062]\n",
      "41640 [D loss: 0.999971] [G loss: 1.000056]\n",
      "41650 [D loss: 0.999969] [G loss: 1.000074]\n",
      "41660 [D loss: 0.999966] [G loss: 1.000065]\n",
      "41670 [D loss: 0.999963] [G loss: 1.000069]\n",
      "41680 [D loss: 0.999964] [G loss: 1.000061]\n",
      "41690 [D loss: 0.999969] [G loss: 1.000060]\n",
      "41700 [D loss: 0.999963] [G loss: 1.000058]\n",
      "41710 [D loss: 0.999957] [G loss: 1.000069]\n",
      "41720 [D loss: 0.999970] [G loss: 1.000056]\n",
      "41730 [D loss: 0.999965] [G loss: 1.000068]\n",
      "41740 [D loss: 0.999965] [G loss: 1.000063]\n",
      "41750 [D loss: 0.999964] [G loss: 1.000067]\n",
      "41760 [D loss: 0.999971] [G loss: 1.000064]\n",
      "41770 [D loss: 0.999969] [G loss: 1.000074]\n",
      "41780 [D loss: 0.999958] [G loss: 1.000062]\n",
      "41790 [D loss: 0.999964] [G loss: 1.000061]\n",
      "41800 [D loss: 0.999972] [G loss: 1.000041]\n",
      "41810 [D loss: 0.999965] [G loss: 1.000051]\n",
      "41820 [D loss: 0.999966] [G loss: 1.000062]\n",
      "41830 [D loss: 0.999966] [G loss: 1.000069]\n",
      "41840 [D loss: 0.999966] [G loss: 1.000063]\n",
      "41850 [D loss: 0.999966] [G loss: 1.000062]\n",
      "41860 [D loss: 0.999967] [G loss: 1.000061]\n",
      "41870 [D loss: 0.999967] [G loss: 1.000061]\n",
      "41880 [D loss: 0.999966] [G loss: 1.000067]\n",
      "41890 [D loss: 0.999963] [G loss: 1.000076]\n",
      "41900 [D loss: 0.999965] [G loss: 1.000046]\n",
      "41910 [D loss: 0.999966] [G loss: 1.000059]\n",
      "41920 [D loss: 0.999960] [G loss: 1.000056]\n",
      "41930 [D loss: 0.999969] [G loss: 1.000067]\n",
      "41940 [D loss: 0.999966] [G loss: 1.000056]\n",
      "41950 [D loss: 0.999956] [G loss: 1.000061]\n",
      "41960 [D loss: 0.999964] [G loss: 1.000060]\n",
      "41970 [D loss: 0.999964] [G loss: 1.000046]\n",
      "41980 [D loss: 0.999964] [G loss: 1.000060]\n",
      "41990 [D loss: 0.999961] [G loss: 1.000051]\n",
      "42000 [D loss: 0.999966] [G loss: 1.000069]\n",
      "42010 [D loss: 0.999969] [G loss: 1.000054]\n",
      "42020 [D loss: 0.999964] [G loss: 1.000055]\n",
      "42030 [D loss: 0.999968] [G loss: 1.000061]\n",
      "42040 [D loss: 0.999966] [G loss: 1.000053]\n",
      "42050 [D loss: 0.999968] [G loss: 1.000063]\n",
      "42060 [D loss: 0.999959] [G loss: 1.000064]\n",
      "42070 [D loss: 0.999966] [G loss: 1.000071]\n",
      "42080 [D loss: 0.999967] [G loss: 1.000061]\n",
      "42090 [D loss: 0.999969] [G loss: 1.000057]\n",
      "42100 [D loss: 0.999976] [G loss: 1.000063]\n",
      "42110 [D loss: 0.999974] [G loss: 1.000063]\n",
      "42120 [D loss: 0.999967] [G loss: 1.000057]\n",
      "42130 [D loss: 0.999976] [G loss: 1.000062]\n",
      "42140 [D loss: 0.999966] [G loss: 1.000064]\n",
      "42150 [D loss: 0.999971] [G loss: 1.000074]\n",
      "42160 [D loss: 0.999962] [G loss: 1.000063]\n",
      "42170 [D loss: 0.999964] [G loss: 1.000056]\n",
      "42180 [D loss: 0.999968] [G loss: 1.000069]\n",
      "42190 [D loss: 0.999968] [G loss: 1.000058]\n",
      "42200 [D loss: 0.999967] [G loss: 1.000049]\n",
      "42210 [D loss: 0.999961] [G loss: 1.000047]\n",
      "42220 [D loss: 0.999964] [G loss: 1.000057]\n",
      "42230 [D loss: 0.999972] [G loss: 1.000050]\n",
      "42240 [D loss: 0.999965] [G loss: 1.000053]\n",
      "42250 [D loss: 0.999973] [G loss: 1.000064]\n",
      "42260 [D loss: 0.999959] [G loss: 1.000063]\n",
      "42270 [D loss: 0.999970] [G loss: 1.000053]\n",
      "42280 [D loss: 0.999966] [G loss: 1.000076]\n",
      "42290 [D loss: 0.999961] [G loss: 1.000068]\n",
      "42300 [D loss: 0.999965] [G loss: 1.000064]\n",
      "42310 [D loss: 0.999970] [G loss: 1.000068]\n",
      "42320 [D loss: 0.999961] [G loss: 1.000072]\n",
      "42330 [D loss: 0.999970] [G loss: 1.000062]\n",
      "42340 [D loss: 0.999975] [G loss: 1.000070]\n",
      "42350 [D loss: 0.999975] [G loss: 1.000051]\n",
      "42360 [D loss: 0.999971] [G loss: 1.000059]\n",
      "42370 [D loss: 0.999968] [G loss: 1.000049]\n",
      "42380 [D loss: 0.999953] [G loss: 1.000060]\n",
      "42390 [D loss: 0.999960] [G loss: 1.000063]\n",
      "42400 [D loss: 0.999966] [G loss: 1.000069]\n",
      "42410 [D loss: 0.999967] [G loss: 1.000065]\n",
      "42420 [D loss: 0.999965] [G loss: 1.000064]\n",
      "42430 [D loss: 0.999965] [G loss: 1.000065]\n",
      "42440 [D loss: 0.999972] [G loss: 1.000066]\n",
      "42450 [D loss: 0.999960] [G loss: 1.000069]\n",
      "42460 [D loss: 0.999967] [G loss: 1.000052]\n",
      "42470 [D loss: 0.999960] [G loss: 1.000071]\n",
      "42480 [D loss: 0.999962] [G loss: 1.000055]\n",
      "42490 [D loss: 0.999957] [G loss: 1.000062]\n",
      "42500 [D loss: 0.999965] [G loss: 1.000058]\n",
      "42510 [D loss: 0.999970] [G loss: 1.000064]\n",
      "42520 [D loss: 0.999969] [G loss: 1.000069]\n",
      "42530 [D loss: 0.999970] [G loss: 1.000065]\n",
      "42540 [D loss: 0.999967] [G loss: 1.000064]\n",
      "42550 [D loss: 0.999972] [G loss: 1.000060]\n",
      "42560 [D loss: 0.999968] [G loss: 1.000074]\n",
      "42570 [D loss: 0.999968] [G loss: 1.000058]\n",
      "42580 [D loss: 0.999954] [G loss: 1.000067]\n",
      "42590 [D loss: 0.999970] [G loss: 1.000061]\n",
      "42600 [D loss: 0.999970] [G loss: 1.000062]\n",
      "42610 [D loss: 0.999965] [G loss: 1.000055]\n",
      "42620 [D loss: 0.999962] [G loss: 1.000067]\n",
      "42630 [D loss: 0.999971] [G loss: 1.000054]\n",
      "42640 [D loss: 0.999958] [G loss: 1.000067]\n",
      "42650 [D loss: 0.999967] [G loss: 1.000059]\n",
      "42660 [D loss: 0.999966] [G loss: 1.000050]\n",
      "42670 [D loss: 0.999963] [G loss: 1.000055]\n",
      "42680 [D loss: 0.999969] [G loss: 1.000066]\n",
      "42690 [D loss: 0.999974] [G loss: 1.000059]\n",
      "42700 [D loss: 0.999961] [G loss: 1.000062]\n",
      "42710 [D loss: 0.999968] [G loss: 1.000068]\n",
      "42720 [D loss: 0.999968] [G loss: 1.000045]\n",
      "42730 [D loss: 0.999972] [G loss: 1.000056]\n",
      "42740 [D loss: 0.999973] [G loss: 1.000069]\n",
      "42750 [D loss: 0.999972] [G loss: 1.000053]\n",
      "42760 [D loss: 0.999959] [G loss: 1.000058]\n",
      "42770 [D loss: 0.999969] [G loss: 1.000056]\n",
      "42780 [D loss: 0.999968] [G loss: 1.000058]\n",
      "42790 [D loss: 0.999971] [G loss: 1.000075]\n",
      "42800 [D loss: 0.999960] [G loss: 1.000063]\n",
      "42810 [D loss: 0.999969] [G loss: 1.000055]\n",
      "42820 [D loss: 0.999975] [G loss: 1.000049]\n",
      "42830 [D loss: 0.999966] [G loss: 1.000075]\n",
      "42840 [D loss: 0.999970] [G loss: 1.000051]\n",
      "42850 [D loss: 0.999958] [G loss: 1.000050]\n",
      "42860 [D loss: 0.999967] [G loss: 1.000059]\n",
      "42870 [D loss: 0.999967] [G loss: 1.000070]\n",
      "42880 [D loss: 0.999968] [G loss: 1.000058]\n",
      "42890 [D loss: 0.999966] [G loss: 1.000058]\n",
      "42900 [D loss: 0.999964] [G loss: 1.000056]\n",
      "42910 [D loss: 0.999960] [G loss: 1.000056]\n",
      "42920 [D loss: 0.999962] [G loss: 1.000064]\n",
      "42930 [D loss: 0.999972] [G loss: 1.000060]\n",
      "42940 [D loss: 0.999969] [G loss: 1.000052]\n",
      "42950 [D loss: 0.999960] [G loss: 1.000061]\n",
      "42960 [D loss: 0.999967] [G loss: 1.000063]\n",
      "42970 [D loss: 0.999971] [G loss: 1.000060]\n",
      "42980 [D loss: 0.999963] [G loss: 1.000059]\n",
      "42990 [D loss: 0.999968] [G loss: 1.000057]\n",
      "43000 [D loss: 0.999966] [G loss: 1.000056]\n",
      "43010 [D loss: 0.999964] [G loss: 1.000059]\n",
      "43020 [D loss: 0.999962] [G loss: 1.000068]\n",
      "43030 [D loss: 0.999968] [G loss: 1.000068]\n",
      "43040 [D loss: 0.999959] [G loss: 1.000063]\n",
      "43050 [D loss: 0.999970] [G loss: 1.000065]\n",
      "43060 [D loss: 0.999965] [G loss: 1.000055]\n",
      "43070 [D loss: 0.999970] [G loss: 1.000060]\n",
      "43080 [D loss: 0.999969] [G loss: 1.000042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43090 [D loss: 0.999961] [G loss: 1.000066]\n",
      "43100 [D loss: 0.999967] [G loss: 1.000061]\n",
      "43110 [D loss: 0.999972] [G loss: 1.000068]\n",
      "43120 [D loss: 0.999969] [G loss: 1.000054]\n",
      "43130 [D loss: 0.999971] [G loss: 1.000058]\n",
      "43140 [D loss: 0.999972] [G loss: 1.000067]\n",
      "43150 [D loss: 0.999962] [G loss: 1.000061]\n",
      "43160 [D loss: 0.999965] [G loss: 1.000065]\n",
      "43170 [D loss: 0.999968] [G loss: 1.000068]\n",
      "43180 [D loss: 0.999967] [G loss: 1.000062]\n",
      "43190 [D loss: 0.999964] [G loss: 1.000054]\n",
      "43200 [D loss: 0.999961] [G loss: 1.000062]\n",
      "43210 [D loss: 0.999970] [G loss: 1.000060]\n",
      "43220 [D loss: 0.999970] [G loss: 1.000062]\n",
      "43230 [D loss: 0.999970] [G loss: 1.000056]\n",
      "43240 [D loss: 0.999966] [G loss: 1.000066]\n",
      "43250 [D loss: 0.999966] [G loss: 1.000063]\n",
      "43260 [D loss: 0.999959] [G loss: 1.000060]\n",
      "43270 [D loss: 0.999963] [G loss: 1.000066]\n",
      "43280 [D loss: 0.999964] [G loss: 1.000062]\n",
      "43290 [D loss: 0.999974] [G loss: 1.000053]\n",
      "43300 [D loss: 0.999967] [G loss: 1.000062]\n",
      "43310 [D loss: 0.999971] [G loss: 1.000058]\n",
      "43320 [D loss: 0.999970] [G loss: 1.000064]\n",
      "43330 [D loss: 0.999957] [G loss: 1.000067]\n",
      "43340 [D loss: 0.999962] [G loss: 1.000071]\n",
      "43350 [D loss: 0.999976] [G loss: 1.000048]\n",
      "43360 [D loss: 0.999970] [G loss: 1.000063]\n",
      "43370 [D loss: 0.999973] [G loss: 1.000051]\n",
      "43380 [D loss: 0.999968] [G loss: 1.000059]\n",
      "43390 [D loss: 0.999966] [G loss: 1.000069]\n",
      "43400 [D loss: 0.999973] [G loss: 1.000063]\n",
      "43410 [D loss: 0.999969] [G loss: 1.000067]\n",
      "43420 [D loss: 0.999969] [G loss: 1.000064]\n",
      "43430 [D loss: 0.999967] [G loss: 1.000062]\n",
      "43440 [D loss: 0.999966] [G loss: 1.000064]\n",
      "43450 [D loss: 0.999969] [G loss: 1.000054]\n",
      "43460 [D loss: 0.999972] [G loss: 1.000057]\n",
      "43470 [D loss: 0.999964] [G loss: 1.000062]\n",
      "43480 [D loss: 0.999965] [G loss: 1.000050]\n",
      "43490 [D loss: 0.999967] [G loss: 1.000058]\n",
      "43500 [D loss: 0.999973] [G loss: 1.000054]\n",
      "43510 [D loss: 0.999963] [G loss: 1.000069]\n",
      "43520 [D loss: 0.999960] [G loss: 1.000067]\n",
      "43530 [D loss: 0.999960] [G loss: 1.000056]\n",
      "43540 [D loss: 0.999962] [G loss: 1.000059]\n",
      "43550 [D loss: 0.999966] [G loss: 1.000058]\n",
      "43560 [D loss: 0.999967] [G loss: 1.000060]\n",
      "43570 [D loss: 0.999961] [G loss: 1.000057]\n",
      "43580 [D loss: 0.999968] [G loss: 1.000060]\n",
      "43590 [D loss: 0.999965] [G loss: 1.000058]\n",
      "43600 [D loss: 0.999966] [G loss: 1.000060]\n",
      "43610 [D loss: 0.999967] [G loss: 1.000063]\n",
      "43620 [D loss: 0.999968] [G loss: 1.000061]\n",
      "43630 [D loss: 0.999970] [G loss: 1.000065]\n",
      "43640 [D loss: 0.999968] [G loss: 1.000054]\n",
      "43650 [D loss: 0.999971] [G loss: 1.000062]\n",
      "43660 [D loss: 0.999970] [G loss: 1.000058]\n",
      "43670 [D loss: 0.999965] [G loss: 1.000057]\n",
      "43680 [D loss: 0.999968] [G loss: 1.000061]\n",
      "43690 [D loss: 0.999970] [G loss: 1.000064]\n",
      "43700 [D loss: 0.999961] [G loss: 1.000047]\n",
      "43710 [D loss: 0.999972] [G loss: 1.000057]\n",
      "43720 [D loss: 0.999959] [G loss: 1.000068]\n",
      "43730 [D loss: 0.999967] [G loss: 1.000065]\n",
      "43740 [D loss: 0.999964] [G loss: 1.000057]\n",
      "43750 [D loss: 0.999966] [G loss: 1.000061]\n",
      "43760 [D loss: 0.999963] [G loss: 1.000062]\n",
      "43770 [D loss: 0.999964] [G loss: 1.000058]\n",
      "43780 [D loss: 0.999967] [G loss: 1.000061]\n",
      "43790 [D loss: 0.999964] [G loss: 1.000058]\n",
      "43800 [D loss: 0.999968] [G loss: 1.000063]\n",
      "43810 [D loss: 0.999968] [G loss: 1.000052]\n",
      "43820 [D loss: 0.999965] [G loss: 1.000061]\n",
      "43830 [D loss: 0.999964] [G loss: 1.000061]\n",
      "43840 [D loss: 0.999963] [G loss: 1.000051]\n",
      "43850 [D loss: 0.999968] [G loss: 1.000059]\n",
      "43860 [D loss: 0.999970] [G loss: 1.000071]\n",
      "43870 [D loss: 0.999966] [G loss: 1.000052]\n",
      "43880 [D loss: 0.999970] [G loss: 1.000059]\n",
      "43890 [D loss: 0.999963] [G loss: 1.000067]\n",
      "43900 [D loss: 0.999968] [G loss: 1.000068]\n",
      "43910 [D loss: 0.999966] [G loss: 1.000057]\n",
      "43920 [D loss: 0.999966] [G loss: 1.000063]\n",
      "43930 [D loss: 0.999968] [G loss: 1.000063]\n",
      "43940 [D loss: 0.999967] [G loss: 1.000074]\n",
      "43950 [D loss: 0.999963] [G loss: 1.000067]\n",
      "43960 [D loss: 0.999967] [G loss: 1.000069]\n",
      "43970 [D loss: 0.999958] [G loss: 1.000067]\n",
      "43980 [D loss: 0.999969] [G loss: 1.000072]\n",
      "43990 [D loss: 0.999963] [G loss: 1.000056]\n",
      "44000 [D loss: 0.999975] [G loss: 1.000056]\n",
      "44010 [D loss: 0.999968] [G loss: 1.000065]\n",
      "44020 [D loss: 0.999969] [G loss: 1.000070]\n",
      "44030 [D loss: 0.999977] [G loss: 1.000055]\n",
      "44040 [D loss: 0.999962] [G loss: 1.000075]\n",
      "44050 [D loss: 0.999966] [G loss: 1.000069]\n",
      "44060 [D loss: 0.999966] [G loss: 1.000061]\n",
      "44070 [D loss: 0.999961] [G loss: 1.000066]\n",
      "44080 [D loss: 0.999966] [G loss: 1.000062]\n",
      "44090 [D loss: 0.999967] [G loss: 1.000063]\n",
      "44100 [D loss: 0.999967] [G loss: 1.000073]\n",
      "44110 [D loss: 0.999969] [G loss: 1.000045]\n",
      "44120 [D loss: 0.999963] [G loss: 1.000057]\n",
      "44130 [D loss: 0.999960] [G loss: 1.000061]\n",
      "44140 [D loss: 0.999962] [G loss: 1.000058]\n",
      "44150 [D loss: 0.999964] [G loss: 1.000059]\n",
      "44160 [D loss: 0.999970] [G loss: 1.000054]\n",
      "44170 [D loss: 0.999960] [G loss: 1.000061]\n",
      "44180 [D loss: 0.999968] [G loss: 1.000061]\n",
      "44190 [D loss: 0.999968] [G loss: 1.000056]\n",
      "44200 [D loss: 0.999972] [G loss: 1.000057]\n",
      "44210 [D loss: 0.999961] [G loss: 1.000064]\n",
      "44220 [D loss: 0.999965] [G loss: 1.000064]\n",
      "44230 [D loss: 0.999965] [G loss: 1.000062]\n",
      "44240 [D loss: 0.999964] [G loss: 1.000049]\n",
      "44250 [D loss: 0.999958] [G loss: 1.000061]\n",
      "44260 [D loss: 0.999960] [G loss: 1.000066]\n",
      "44270 [D loss: 0.999964] [G loss: 1.000058]\n",
      "44280 [D loss: 0.999967] [G loss: 1.000064]\n",
      "44290 [D loss: 0.999959] [G loss: 1.000066]\n",
      "44300 [D loss: 0.999963] [G loss: 1.000069]\n",
      "44310 [D loss: 0.999966] [G loss: 1.000066]\n",
      "44320 [D loss: 0.999970] [G loss: 1.000063]\n",
      "44330 [D loss: 0.999969] [G loss: 1.000066]\n",
      "44340 [D loss: 0.999968] [G loss: 1.000065]\n",
      "44350 [D loss: 0.999969] [G loss: 1.000058]\n",
      "44360 [D loss: 0.999973] [G loss: 1.000039]\n",
      "44370 [D loss: 0.999973] [G loss: 1.000054]\n",
      "44380 [D loss: 0.999966] [G loss: 1.000061]\n",
      "44390 [D loss: 0.999972] [G loss: 1.000055]\n",
      "44400 [D loss: 0.999967] [G loss: 1.000068]\n",
      "44410 [D loss: 0.999968] [G loss: 1.000061]\n",
      "44420 [D loss: 0.999967] [G loss: 1.000057]\n",
      "44430 [D loss: 0.999964] [G loss: 1.000049]\n",
      "44440 [D loss: 0.999963] [G loss: 1.000064]\n",
      "44450 [D loss: 0.999961] [G loss: 1.000068]\n",
      "44460 [D loss: 0.999968] [G loss: 1.000060]\n",
      "44470 [D loss: 0.999969] [G loss: 1.000067]\n",
      "44480 [D loss: 0.999965] [G loss: 1.000060]\n",
      "44490 [D loss: 0.999967] [G loss: 1.000055]\n",
      "44500 [D loss: 0.999966] [G loss: 1.000069]\n",
      "44510 [D loss: 0.999962] [G loss: 1.000047]\n",
      "44520 [D loss: 0.999961] [G loss: 1.000069]\n",
      "44530 [D loss: 0.999963] [G loss: 1.000071]\n",
      "44540 [D loss: 0.999971] [G loss: 1.000061]\n",
      "44550 [D loss: 0.999965] [G loss: 1.000066]\n",
      "44560 [D loss: 0.999971] [G loss: 1.000065]\n",
      "44570 [D loss: 0.999963] [G loss: 1.000061]\n",
      "44580 [D loss: 0.999967] [G loss: 1.000056]\n",
      "44590 [D loss: 0.999962] [G loss: 1.000057]\n",
      "44600 [D loss: 0.999970] [G loss: 1.000060]\n",
      "44610 [D loss: 0.999965] [G loss: 1.000058]\n",
      "44620 [D loss: 0.999965] [G loss: 1.000056]\n",
      "44630 [D loss: 0.999971] [G loss: 1.000063]\n",
      "44640 [D loss: 0.999966] [G loss: 1.000066]\n",
      "44650 [D loss: 0.999968] [G loss: 1.000061]\n",
      "44660 [D loss: 0.999959] [G loss: 1.000064]\n",
      "44670 [D loss: 0.999970] [G loss: 1.000063]\n",
      "44680 [D loss: 0.999964] [G loss: 1.000060]\n",
      "44690 [D loss: 0.999964] [G loss: 1.000056]\n",
      "44700 [D loss: 0.999961] [G loss: 1.000063]\n",
      "44710 [D loss: 0.999969] [G loss: 1.000061]\n",
      "44720 [D loss: 0.999970] [G loss: 1.000063]\n",
      "44730 [D loss: 0.999968] [G loss: 1.000056]\n",
      "44740 [D loss: 0.999974] [G loss: 1.000086]\n",
      "44750 [D loss: 0.999957] [G loss: 1.000050]\n",
      "44760 [D loss: 0.999959] [G loss: 1.000069]\n",
      "44770 [D loss: 0.999966] [G loss: 1.000052]\n",
      "44780 [D loss: 0.999969] [G loss: 1.000059]\n",
      "44790 [D loss: 0.999964] [G loss: 1.000051]\n",
      "44800 [D loss: 0.999963] [G loss: 1.000059]\n",
      "44810 [D loss: 0.999968] [G loss: 1.000059]\n",
      "44820 [D loss: 0.999961] [G loss: 1.000060]\n",
      "44830 [D loss: 0.999963] [G loss: 1.000061]\n",
      "44840 [D loss: 0.999963] [G loss: 1.000058]\n",
      "44850 [D loss: 0.999969] [G loss: 1.000056]\n",
      "44860 [D loss: 0.999961] [G loss: 1.000065]\n",
      "44870 [D loss: 0.999961] [G loss: 1.000062]\n",
      "44880 [D loss: 0.999968] [G loss: 1.000054]\n",
      "44890 [D loss: 0.999973] [G loss: 1.000063]\n",
      "44900 [D loss: 0.999954] [G loss: 1.000073]\n",
      "44910 [D loss: 0.999967] [G loss: 1.000064]\n",
      "44920 [D loss: 0.999975] [G loss: 1.000049]\n",
      "44930 [D loss: 0.999963] [G loss: 1.000067]\n",
      "44940 [D loss: 0.999973] [G loss: 1.000069]\n",
      "44950 [D loss: 0.999951] [G loss: 1.000067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44960 [D loss: 0.999952] [G loss: 1.000071]\n",
      "44970 [D loss: 0.999976] [G loss: 1.000070]\n",
      "44980 [D loss: 0.999965] [G loss: 1.000052]\n",
      "44990 [D loss: 0.999973] [G loss: 1.000079]\n",
      "45000 [D loss: 0.999977] [G loss: 1.000071]\n",
      "45010 [D loss: 0.999965] [G loss: 1.000059]\n",
      "45020 [D loss: 0.999958] [G loss: 1.000070]\n",
      "45030 [D loss: 0.999959] [G loss: 1.000059]\n",
      "45040 [D loss: 0.999964] [G loss: 1.000063]\n",
      "45050 [D loss: 0.999968] [G loss: 1.000053]\n",
      "45060 [D loss: 0.999976] [G loss: 1.000061]\n",
      "45070 [D loss: 0.999965] [G loss: 1.000049]\n",
      "45080 [D loss: 0.999967] [G loss: 1.000050]\n",
      "45090 [D loss: 0.999959] [G loss: 1.000065]\n",
      "45100 [D loss: 0.999963] [G loss: 1.000054]\n",
      "45110 [D loss: 0.999959] [G loss: 1.000076]\n",
      "45120 [D loss: 0.999956] [G loss: 1.000061]\n",
      "45130 [D loss: 0.999966] [G loss: 1.000075]\n",
      "45140 [D loss: 0.999963] [G loss: 1.000054]\n",
      "45150 [D loss: 0.999969] [G loss: 1.000052]\n",
      "45160 [D loss: 0.999973] [G loss: 1.000052]\n",
      "45170 [D loss: 0.999965] [G loss: 1.000069]\n",
      "45180 [D loss: 0.999967] [G loss: 1.000058]\n",
      "45190 [D loss: 0.999982] [G loss: 1.000056]\n",
      "45200 [D loss: 0.999966] [G loss: 1.000055]\n",
      "45210 [D loss: 0.999962] [G loss: 1.000087]\n",
      "45220 [D loss: 0.999955] [G loss: 1.000087]\n",
      "45230 [D loss: 0.999968] [G loss: 1.000053]\n",
      "45240 [D loss: 0.999969] [G loss: 1.000060]\n",
      "45250 [D loss: 0.999959] [G loss: 1.000062]\n",
      "45260 [D loss: 0.999956] [G loss: 1.000082]\n",
      "45270 [D loss: 0.999968] [G loss: 1.000052]\n",
      "45280 [D loss: 0.999962] [G loss: 1.000067]\n",
      "45290 [D loss: 0.999974] [G loss: 1.000062]\n",
      "45300 [D loss: 0.999967] [G loss: 1.000043]\n",
      "45310 [D loss: 0.999971] [G loss: 1.000069]\n",
      "45320 [D loss: 0.999967] [G loss: 1.000070]\n",
      "45330 [D loss: 0.999961] [G loss: 1.000059]\n",
      "45340 [D loss: 0.999964] [G loss: 1.000075]\n",
      "45350 [D loss: 0.999958] [G loss: 1.000055]\n",
      "45360 [D loss: 0.999975] [G loss: 1.000077]\n",
      "45370 [D loss: 0.999968] [G loss: 1.000058]\n",
      "45380 [D loss: 0.999979] [G loss: 1.000075]\n",
      "45390 [D loss: 0.999974] [G loss: 1.000053]\n",
      "45400 [D loss: 0.999959] [G loss: 1.000069]\n",
      "45410 [D loss: 0.999960] [G loss: 1.000054]\n",
      "45420 [D loss: 0.999965] [G loss: 1.000067]\n",
      "45430 [D loss: 0.999960] [G loss: 1.000049]\n",
      "45440 [D loss: 0.999951] [G loss: 1.000061]\n",
      "45450 [D loss: 0.999992] [G loss: 1.000068]\n",
      "45460 [D loss: 0.999971] [G loss: 1.000056]\n",
      "45470 [D loss: 0.999955] [G loss: 1.000079]\n",
      "45480 [D loss: 0.999988] [G loss: 1.000064]\n",
      "45490 [D loss: 0.999965] [G loss: 1.000053]\n",
      "45500 [D loss: 0.999968] [G loss: 1.000068]\n",
      "45510 [D loss: 0.999967] [G loss: 1.000103]\n",
      "45520 [D loss: 0.999945] [G loss: 1.000015]\n",
      "45530 [D loss: 0.999952] [G loss: 1.000059]\n",
      "45540 [D loss: 0.999968] [G loss: 1.000084]\n",
      "45550 [D loss: 0.999972] [G loss: 1.000042]\n",
      "45560 [D loss: 0.999967] [G loss: 1.000087]\n",
      "45570 [D loss: 0.999975] [G loss: 1.000050]\n",
      "45580 [D loss: 0.999956] [G loss: 1.000055]\n",
      "45590 [D loss: 0.999962] [G loss: 1.000068]\n",
      "45600 [D loss: 0.999966] [G loss: 1.000040]\n",
      "45610 [D loss: 0.999968] [G loss: 1.000058]\n",
      "45620 [D loss: 0.999961] [G loss: 1.000080]\n",
      "45630 [D loss: 0.999992] [G loss: 1.000067]\n",
      "45640 [D loss: 0.999969] [G loss: 1.000040]\n",
      "45650 [D loss: 0.999972] [G loss: 1.000066]\n",
      "45660 [D loss: 0.999976] [G loss: 1.000060]\n",
      "45670 [D loss: 0.999948] [G loss: 1.000077]\n",
      "45680 [D loss: 0.999948] [G loss: 1.000061]\n",
      "45690 [D loss: 0.999959] [G loss: 1.000050]\n",
      "45700 [D loss: 0.999973] [G loss: 1.000084]\n",
      "45710 [D loss: 0.999959] [G loss: 1.000052]\n",
      "45720 [D loss: 0.999972] [G loss: 1.000043]\n",
      "45730 [D loss: 0.999965] [G loss: 1.000079]\n",
      "45740 [D loss: 0.999961] [G loss: 1.000080]\n",
      "45750 [D loss: 0.999968] [G loss: 1.000052]\n",
      "45760 [D loss: 0.999967] [G loss: 1.000047]\n",
      "45770 [D loss: 0.999973] [G loss: 1.000079]\n",
      "45780 [D loss: 0.999961] [G loss: 1.000055]\n",
      "45790 [D loss: 0.999976] [G loss: 1.000055]\n",
      "45800 [D loss: 0.999977] [G loss: 1.000036]\n",
      "45810 [D loss: 0.999971] [G loss: 1.000036]\n",
      "45820 [D loss: 0.999975] [G loss: 1.000064]\n",
      "45830 [D loss: 0.999964] [G loss: 1.000054]\n",
      "45840 [D loss: 0.999965] [G loss: 1.000071]\n",
      "45850 [D loss: 0.999972] [G loss: 1.000060]\n",
      "45860 [D loss: 0.999971] [G loss: 1.000075]\n",
      "45870 [D loss: 0.999970] [G loss: 1.000074]\n",
      "45880 [D loss: 0.999973] [G loss: 1.000058]\n",
      "45890 [D loss: 0.999963] [G loss: 1.000060]\n",
      "45900 [D loss: 0.999950] [G loss: 1.000051]\n",
      "45910 [D loss: 0.999964] [G loss: 1.000061]\n",
      "45920 [D loss: 0.999960] [G loss: 1.000061]\n",
      "45930 [D loss: 0.999965] [G loss: 1.000057]\n",
      "45940 [D loss: 0.999965] [G loss: 1.000072]\n",
      "45950 [D loss: 0.999961] [G loss: 1.000072]\n",
      "45960 [D loss: 0.999962] [G loss: 1.000057]\n",
      "45970 [D loss: 0.999972] [G loss: 1.000070]\n",
      "45980 [D loss: 0.999972] [G loss: 1.000074]\n",
      "45990 [D loss: 0.999977] [G loss: 1.000055]\n",
      "46000 [D loss: 0.999964] [G loss: 1.000062]\n",
      "46010 [D loss: 0.999960] [G loss: 1.000075]\n",
      "46020 [D loss: 0.999958] [G loss: 1.000063]\n",
      "46030 [D loss: 0.999963] [G loss: 1.000062]\n",
      "46040 [D loss: 0.999962] [G loss: 1.000064]\n",
      "46050 [D loss: 0.999970] [G loss: 1.000053]\n",
      "46060 [D loss: 0.999961] [G loss: 1.000065]\n",
      "46070 [D loss: 0.999958] [G loss: 1.000072]\n",
      "46080 [D loss: 0.999979] [G loss: 1.000057]\n",
      "46090 [D loss: 0.999973] [G loss: 1.000054]\n",
      "46100 [D loss: 0.999949] [G loss: 1.000062]\n",
      "46110 [D loss: 0.999956] [G loss: 1.000060]\n",
      "46120 [D loss: 0.999967] [G loss: 1.000069]\n",
      "46130 [D loss: 0.999959] [G loss: 1.000071]\n",
      "46140 [D loss: 0.999969] [G loss: 1.000071]\n",
      "46150 [D loss: 0.999956] [G loss: 1.000050]\n",
      "46160 [D loss: 0.999958] [G loss: 1.000050]\n",
      "46170 [D loss: 0.999967] [G loss: 1.000074]\n",
      "46180 [D loss: 0.999976] [G loss: 1.000054]\n",
      "46190 [D loss: 0.999961] [G loss: 1.000082]\n",
      "46200 [D loss: 0.999982] [G loss: 1.000080]\n",
      "46210 [D loss: 0.999959] [G loss: 1.000058]\n",
      "46220 [D loss: 0.999971] [G loss: 1.000073]\n",
      "46230 [D loss: 0.999984] [G loss: 1.000086]\n",
      "46240 [D loss: 0.999968] [G loss: 1.000061]\n",
      "46250 [D loss: 0.999975] [G loss: 1.000071]\n",
      "46260 [D loss: 0.999964] [G loss: 1.000089]\n",
      "46270 [D loss: 0.999975] [G loss: 1.000045]\n",
      "46280 [D loss: 0.999962] [G loss: 1.000072]\n",
      "46290 [D loss: 0.999970] [G loss: 1.000076]\n",
      "46300 [D loss: 0.999968] [G loss: 1.000048]\n",
      "46310 [D loss: 0.999966] [G loss: 1.000073]\n",
      "46320 [D loss: 0.999959] [G loss: 1.000071]\n",
      "46330 [D loss: 0.999976] [G loss: 1.000019]\n",
      "46340 [D loss: 0.999963] [G loss: 1.000067]\n",
      "46350 [D loss: 0.999952] [G loss: 1.000077]\n",
      "46360 [D loss: 0.999969] [G loss: 1.000042]\n",
      "46370 [D loss: 0.999971] [G loss: 1.000070]\n",
      "46380 [D loss: 0.999960] [G loss: 1.000065]\n",
      "46390 [D loss: 0.999959] [G loss: 1.000070]\n",
      "46400 [D loss: 0.999968] [G loss: 1.000058]\n",
      "46410 [D loss: 0.999966] [G loss: 1.000065]\n",
      "46420 [D loss: 0.999960] [G loss: 1.000077]\n",
      "46430 [D loss: 0.999977] [G loss: 1.000023]\n",
      "46440 [D loss: 0.999970] [G loss: 1.000035]\n",
      "46450 [D loss: 0.999974] [G loss: 1.000077]\n",
      "46460 [D loss: 0.999969] [G loss: 1.000050]\n",
      "46470 [D loss: 0.999968] [G loss: 1.000036]\n",
      "46480 [D loss: 0.999961] [G loss: 1.000066]\n",
      "46490 [D loss: 0.999953] [G loss: 1.000074]\n",
      "46500 [D loss: 0.999963] [G loss: 1.000089]\n",
      "46510 [D loss: 0.999971] [G loss: 1.000061]\n",
      "46520 [D loss: 0.999964] [G loss: 1.000045]\n",
      "46530 [D loss: 0.999959] [G loss: 1.000065]\n",
      "46540 [D loss: 0.999958] [G loss: 1.000059]\n",
      "46550 [D loss: 0.999982] [G loss: 1.000073]\n",
      "46560 [D loss: 0.999972] [G loss: 1.000042]\n",
      "46570 [D loss: 0.999964] [G loss: 1.000053]\n",
      "46580 [D loss: 0.999955] [G loss: 1.000073]\n",
      "46590 [D loss: 0.999955] [G loss: 1.000100]\n",
      "46600 [D loss: 0.999964] [G loss: 1.000053]\n",
      "46610 [D loss: 0.999961] [G loss: 1.000055]\n",
      "46620 [D loss: 0.999972] [G loss: 1.000063]\n",
      "46630 [D loss: 0.999974] [G loss: 1.000064]\n",
      "46640 [D loss: 0.999975] [G loss: 1.000070]\n",
      "46650 [D loss: 0.999955] [G loss: 1.000077]\n",
      "46660 [D loss: 0.999958] [G loss: 1.000087]\n",
      "46670 [D loss: 0.999963] [G loss: 1.000069]\n",
      "46680 [D loss: 0.999959] [G loss: 1.000071]\n",
      "46690 [D loss: 0.999959] [G loss: 1.000069]\n",
      "46700 [D loss: 0.999981] [G loss: 1.000058]\n",
      "46710 [D loss: 0.999980] [G loss: 1.000042]\n",
      "46720 [D loss: 0.999957] [G loss: 1.000061]\n",
      "46730 [D loss: 0.999958] [G loss: 1.000077]\n",
      "46740 [D loss: 0.999960] [G loss: 1.000070]\n",
      "46750 [D loss: 0.999959] [G loss: 1.000082]\n",
      "46760 [D loss: 0.999972] [G loss: 1.000046]\n",
      "46770 [D loss: 0.999962] [G loss: 1.000050]\n",
      "46780 [D loss: 0.999970] [G loss: 1.000064]\n",
      "46790 [D loss: 0.999965] [G loss: 1.000068]\n",
      "46800 [D loss: 0.999972] [G loss: 1.000025]\n",
      "46810 [D loss: 0.999965] [G loss: 1.000077]\n",
      "46820 [D loss: 0.999962] [G loss: 1.000074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46830 [D loss: 0.999981] [G loss: 1.000019]\n",
      "46840 [D loss: 0.999975] [G loss: 1.000065]\n",
      "46850 [D loss: 0.999971] [G loss: 1.000074]\n",
      "46860 [D loss: 0.999967] [G loss: 1.000044]\n",
      "46870 [D loss: 0.999968] [G loss: 1.000066]\n",
      "46880 [D loss: 0.999973] [G loss: 1.000050]\n",
      "46890 [D loss: 0.999960] [G loss: 1.000058]\n",
      "46900 [D loss: 0.999956] [G loss: 1.000057]\n",
      "46910 [D loss: 0.999961] [G loss: 1.000072]\n",
      "46920 [D loss: 0.999967] [G loss: 1.000089]\n",
      "46930 [D loss: 0.999981] [G loss: 1.000076]\n",
      "46940 [D loss: 0.999972] [G loss: 1.000044]\n",
      "46950 [D loss: 0.999982] [G loss: 1.000038]\n",
      "46960 [D loss: 0.999972] [G loss: 1.000037]\n",
      "46970 [D loss: 0.999975] [G loss: 1.000055]\n",
      "46980 [D loss: 0.999954] [G loss: 1.000083]\n",
      "46990 [D loss: 0.999977] [G loss: 1.000046]\n",
      "47000 [D loss: 0.999974] [G loss: 1.000056]\n",
      "47010 [D loss: 0.999978] [G loss: 1.000015]\n",
      "47020 [D loss: 0.999964] [G loss: 1.000057]\n",
      "47030 [D loss: 0.999957] [G loss: 1.000091]\n",
      "47040 [D loss: 0.999969] [G loss: 1.000106]\n",
      "47050 [D loss: 0.999975] [G loss: 1.000042]\n",
      "47060 [D loss: 0.999956] [G loss: 1.000074]\n",
      "47070 [D loss: 0.999961] [G loss: 1.000099]\n",
      "47080 [D loss: 0.999957] [G loss: 1.000071]\n",
      "47090 [D loss: 0.999971] [G loss: 1.000078]\n",
      "47100 [D loss: 0.999970] [G loss: 1.000062]\n",
      "47110 [D loss: 0.999974] [G loss: 1.000042]\n",
      "47120 [D loss: 0.999959] [G loss: 1.000049]\n",
      "47130 [D loss: 0.999954] [G loss: 1.000045]\n",
      "47140 [D loss: 0.999969] [G loss: 1.000056]\n",
      "47150 [D loss: 0.999967] [G loss: 1.000072]\n",
      "47160 [D loss: 0.999978] [G loss: 1.000050]\n",
      "47170 [D loss: 0.999997] [G loss: 1.000046]\n",
      "47180 [D loss: 0.999994] [G loss: 1.000044]\n",
      "47190 [D loss: 0.999949] [G loss: 1.000053]\n",
      "47200 [D loss: 0.999968] [G loss: 1.000088]\n",
      "47210 [D loss: 0.999964] [G loss: 1.000068]\n",
      "47220 [D loss: 0.999948] [G loss: 1.000101]\n",
      "47230 [D loss: 1.000029] [G loss: 0.999965]\n",
      "47240 [D loss: 0.999983] [G loss: 1.000048]\n",
      "47250 [D loss: 0.999962] [G loss: 1.000071]\n",
      "47260 [D loss: 0.999960] [G loss: 1.000106]\n",
      "47270 [D loss: 0.999956] [G loss: 1.000065]\n",
      "47280 [D loss: 0.999967] [G loss: 1.000073]\n",
      "47290 [D loss: 0.999960] [G loss: 1.000072]\n",
      "47300 [D loss: 0.999949] [G loss: 1.000083]\n",
      "47310 [D loss: 0.999982] [G loss: 1.000038]\n",
      "47320 [D loss: 0.999973] [G loss: 1.000039]\n",
      "47330 [D loss: 0.999985] [G loss: 1.000053]\n",
      "47340 [D loss: 0.999979] [G loss: 1.000054]\n",
      "47350 [D loss: 0.999962] [G loss: 1.000061]\n",
      "47360 [D loss: 0.999959] [G loss: 1.000084]\n",
      "47370 [D loss: 0.999959] [G loss: 1.000075]\n",
      "47380 [D loss: 0.999973] [G loss: 1.000100]\n",
      "47390 [D loss: 0.999967] [G loss: 1.000080]\n",
      "47400 [D loss: 0.999971] [G loss: 1.000052]\n",
      "47410 [D loss: 0.999971] [G loss: 1.000080]\n",
      "47420 [D loss: 0.999965] [G loss: 1.000101]\n",
      "47430 [D loss: 0.999971] [G loss: 1.000020]\n",
      "47440 [D loss: 0.999994] [G loss: 1.000062]\n",
      "47450 [D loss: 0.999955] [G loss: 1.000076]\n",
      "47460 [D loss: 0.999987] [G loss: 1.000084]\n",
      "47470 [D loss: 0.999988] [G loss: 1.000103]\n",
      "47480 [D loss: 1.000005] [G loss: 1.000016]\n",
      "47490 [D loss: 0.999974] [G loss: 1.000071]\n",
      "47500 [D loss: 0.999967] [G loss: 1.000072]\n",
      "47510 [D loss: 0.999951] [G loss: 1.000070]\n",
      "47520 [D loss: 0.999948] [G loss: 1.000077]\n",
      "47530 [D loss: 0.999966] [G loss: 1.000053]\n",
      "47540 [D loss: 0.999970] [G loss: 1.000045]\n",
      "47550 [D loss: 0.999968] [G loss: 1.000046]\n",
      "47560 [D loss: 0.999976] [G loss: 1.000031]\n",
      "47570 [D loss: 0.999966] [G loss: 1.000071]\n",
      "47580 [D loss: 0.999971] [G loss: 1.000093]\n",
      "47590 [D loss: 0.999973] [G loss: 1.000029]\n",
      "47600 [D loss: 0.999976] [G loss: 1.000048]\n",
      "47610 [D loss: 0.999970] [G loss: 1.000069]\n",
      "47620 [D loss: 0.999964] [G loss: 1.000073]\n",
      "47630 [D loss: 0.999969] [G loss: 1.000096]\n",
      "47640 [D loss: 0.999976] [G loss: 1.000066]\n",
      "47650 [D loss: 0.999955] [G loss: 1.000065]\n",
      "47660 [D loss: 0.999977] [G loss: 1.000033]\n",
      "47670 [D loss: 0.999957] [G loss: 1.000067]\n",
      "47680 [D loss: 0.999966] [G loss: 1.000064]\n",
      "47690 [D loss: 0.999967] [G loss: 1.000073]\n",
      "47700 [D loss: 0.999983] [G loss: 1.000048]\n",
      "47710 [D loss: 0.999949] [G loss: 1.000087]\n",
      "47720 [D loss: 0.999967] [G loss: 1.000080]\n",
      "47730 [D loss: 0.999989] [G loss: 1.000089]\n",
      "47740 [D loss: 1.000009] [G loss: 0.999989]\n",
      "47750 [D loss: 0.999957] [G loss: 1.000072]\n",
      "47760 [D loss: 0.999967] [G loss: 1.000045]\n",
      "47770 [D loss: 0.999953] [G loss: 1.000087]\n",
      "47780 [D loss: 0.999950] [G loss: 1.000093]\n",
      "47790 [D loss: 0.999974] [G loss: 1.000086]\n",
      "47800 [D loss: 0.999958] [G loss: 1.000013]\n",
      "47810 [D loss: 0.999965] [G loss: 1.000066]\n",
      "47820 [D loss: 0.999955] [G loss: 1.000072]\n",
      "47830 [D loss: 0.999977] [G loss: 1.000077]\n",
      "47840 [D loss: 0.999967] [G loss: 1.000005]\n",
      "47850 [D loss: 0.999968] [G loss: 1.000024]\n",
      "47860 [D loss: 0.999960] [G loss: 1.000072]\n",
      "47870 [D loss: 0.999958] [G loss: 1.000098]\n",
      "47880 [D loss: 0.999983] [G loss: 1.000075]\n",
      "47890 [D loss: 0.999970] [G loss: 1.000045]\n",
      "47900 [D loss: 0.999966] [G loss: 1.000078]\n",
      "47910 [D loss: 0.999966] [G loss: 1.000063]\n",
      "47920 [D loss: 0.999975] [G loss: 1.000021]\n",
      "47930 [D loss: 0.999972] [G loss: 1.000060]\n",
      "47940 [D loss: 0.999962] [G loss: 1.000083]\n",
      "47950 [D loss: 0.999957] [G loss: 1.000061]\n",
      "47960 [D loss: 0.999953] [G loss: 1.000079]\n",
      "47970 [D loss: 0.999958] [G loss: 1.000083]\n",
      "47980 [D loss: 0.999974] [G loss: 1.000014]\n",
      "47990 [D loss: 0.999960] [G loss: 1.000051]\n",
      "48000 [D loss: 0.999967] [G loss: 1.000076]\n",
      "48010 [D loss: 0.999949] [G loss: 1.000104]\n",
      "48020 [D loss: 0.999978] [G loss: 1.000077]\n",
      "48030 [D loss: 1.000001] [G loss: 1.000096]\n",
      "48040 [D loss: 0.999960] [G loss: 1.000052]\n",
      "48050 [D loss: 0.999971] [G loss: 1.000094]\n",
      "48060 [D loss: 0.999988] [G loss: 1.000085]\n",
      "48070 [D loss: 0.999951] [G loss: 1.000008]\n",
      "48080 [D loss: 0.999968] [G loss: 1.000059]\n",
      "48090 [D loss: 0.999973] [G loss: 1.000073]\n",
      "48100 [D loss: 0.999983] [G loss: 1.000023]\n",
      "48110 [D loss: 0.999966] [G loss: 1.000068]\n",
      "48120 [D loss: 0.999966] [G loss: 1.000072]\n",
      "48130 [D loss: 0.999980] [G loss: 1.000087]\n",
      "48140 [D loss: 0.999975] [G loss: 1.000038]\n",
      "48150 [D loss: 0.999962] [G loss: 1.000064]\n",
      "48160 [D loss: 0.999942] [G loss: 1.000090]\n",
      "48170 [D loss: 1.000009] [G loss: 1.000010]\n",
      "48180 [D loss: 0.999970] [G loss: 1.000072]\n",
      "48190 [D loss: 0.999969] [G loss: 1.000067]\n",
      "48200 [D loss: 0.999979] [G loss: 1.000018]\n",
      "48210 [D loss: 0.999983] [G loss: 1.000011]\n",
      "48220 [D loss: 0.999968] [G loss: 1.000054]\n",
      "48230 [D loss: 0.999957] [G loss: 1.000076]\n",
      "48240 [D loss: 0.999960] [G loss: 1.000030]\n",
      "48250 [D loss: 0.999972] [G loss: 1.000045]\n",
      "48260 [D loss: 0.999965] [G loss: 1.000065]\n",
      "48270 [D loss: 0.999966] [G loss: 1.000050]\n",
      "48280 [D loss: 0.999971] [G loss: 1.000049]\n",
      "48290 [D loss: 0.999961] [G loss: 1.000052]\n",
      "48300 [D loss: 0.999962] [G loss: 1.000070]\n",
      "48310 [D loss: 0.999980] [G loss: 1.000055]\n",
      "48320 [D loss: 0.999969] [G loss: 1.000065]\n",
      "48330 [D loss: 0.999970] [G loss: 1.000059]\n",
      "48340 [D loss: 0.999975] [G loss: 1.000052]\n",
      "48350 [D loss: 0.999973] [G loss: 1.000058]\n",
      "48360 [D loss: 0.999963] [G loss: 1.000061]\n",
      "48370 [D loss: 0.999974] [G loss: 1.000076]\n",
      "48380 [D loss: 0.999967] [G loss: 1.000066]\n",
      "48390 [D loss: 0.999972] [G loss: 1.000063]\n",
      "48400 [D loss: 0.999982] [G loss: 1.000049]\n",
      "48410 [D loss: 0.999969] [G loss: 1.000061]\n",
      "48420 [D loss: 0.999963] [G loss: 1.000072]\n",
      "48430 [D loss: 0.999969] [G loss: 1.000064]\n",
      "48440 [D loss: 0.999971] [G loss: 1.000068]\n",
      "48450 [D loss: 0.999972] [G loss: 1.000055]\n",
      "48460 [D loss: 0.999961] [G loss: 1.000073]\n",
      "48470 [D loss: 0.999972] [G loss: 1.000063]\n",
      "48480 [D loss: 0.999959] [G loss: 1.000057]\n",
      "48490 [D loss: 0.999955] [G loss: 1.000073]\n",
      "48500 [D loss: 0.999958] [G loss: 1.000062]\n",
      "48510 [D loss: 0.999946] [G loss: 1.000064]\n",
      "48520 [D loss: 0.999950] [G loss: 1.000082]\n",
      "48530 [D loss: 0.999972] [G loss: 1.000036]\n",
      "48540 [D loss: 0.999978] [G loss: 1.000067]\n",
      "48550 [D loss: 0.999968] [G loss: 1.000078]\n",
      "48560 [D loss: 0.999948] [G loss: 1.000059]\n",
      "48570 [D loss: 0.999965] [G loss: 1.000059]\n",
      "48580 [D loss: 0.999970] [G loss: 1.000059]\n",
      "48590 [D loss: 0.999966] [G loss: 1.000073]\n",
      "48600 [D loss: 0.999967] [G loss: 1.000053]\n",
      "48610 [D loss: 0.999959] [G loss: 1.000061]\n",
      "48620 [D loss: 0.999956] [G loss: 1.000089]\n",
      "48630 [D loss: 0.999963] [G loss: 1.000062]\n",
      "48640 [D loss: 0.999966] [G loss: 1.000064]\n",
      "48650 [D loss: 0.999965] [G loss: 1.000060]\n",
      "48660 [D loss: 0.999970] [G loss: 1.000066]\n",
      "48670 [D loss: 0.999970] [G loss: 1.000063]\n",
      "48680 [D loss: 0.999966] [G loss: 1.000077]\n",
      "48690 [D loss: 0.999963] [G loss: 1.000069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48700 [D loss: 0.999972] [G loss: 1.000062]\n",
      "48710 [D loss: 0.999966] [G loss: 1.000045]\n",
      "48720 [D loss: 0.999965] [G loss: 1.000064]\n",
      "48730 [D loss: 0.999968] [G loss: 1.000057]\n",
      "48740 [D loss: 0.999969] [G loss: 1.000069]\n",
      "48750 [D loss: 0.999976] [G loss: 1.000055]\n",
      "48760 [D loss: 0.999960] [G loss: 1.000063]\n",
      "48770 [D loss: 0.999973] [G loss: 1.000048]\n",
      "48780 [D loss: 0.999961] [G loss: 1.000071]\n",
      "48790 [D loss: 0.999966] [G loss: 1.000079]\n",
      "48800 [D loss: 0.999959] [G loss: 1.000050]\n",
      "48810 [D loss: 0.999957] [G loss: 1.000069]\n",
      "48820 [D loss: 0.999958] [G loss: 1.000074]\n",
      "48830 [D loss: 0.999969] [G loss: 1.000050]\n",
      "48840 [D loss: 0.999969] [G loss: 1.000076]\n",
      "48850 [D loss: 0.999973] [G loss: 1.000067]\n",
      "48860 [D loss: 0.999971] [G loss: 1.000057]\n",
      "48870 [D loss: 0.999962] [G loss: 1.000069]\n",
      "48880 [D loss: 0.999977] [G loss: 1.000052]\n",
      "48890 [D loss: 0.999973] [G loss: 1.000057]\n",
      "48900 [D loss: 0.999967] [G loss: 1.000082]\n",
      "48910 [D loss: 0.999978] [G loss: 1.000067]\n",
      "48920 [D loss: 0.999967] [G loss: 1.000070]\n",
      "48930 [D loss: 0.999960] [G loss: 1.000044]\n",
      "48940 [D loss: 0.999961] [G loss: 1.000037]\n",
      "48950 [D loss: 0.999960] [G loss: 1.000067]\n",
      "48960 [D loss: 0.999966] [G loss: 1.000048]\n",
      "48970 [D loss: 0.999979] [G loss: 1.000048]\n",
      "48980 [D loss: 0.999987] [G loss: 1.000034]\n",
      "48990 [D loss: 0.999983] [G loss: 1.000049]\n",
      "49000 [D loss: 0.999966] [G loss: 1.000068]\n",
      "49010 [D loss: 0.999958] [G loss: 1.000064]\n",
      "49020 [D loss: 0.999955] [G loss: 1.000064]\n",
      "49030 [D loss: 0.999966] [G loss: 1.000072]\n",
      "49040 [D loss: 0.999974] [G loss: 1.000100]\n",
      "49050 [D loss: 0.999955] [G loss: 1.000053]\n",
      "49060 [D loss: 0.999969] [G loss: 1.000071]\n",
      "49070 [D loss: 0.999961] [G loss: 1.000084]\n",
      "49080 [D loss: 0.999968] [G loss: 1.000071]\n",
      "49090 [D loss: 0.999960] [G loss: 1.000051]\n",
      "49100 [D loss: 0.999963] [G loss: 1.000072]\n",
      "49110 [D loss: 0.999986] [G loss: 1.000048]\n",
      "49120 [D loss: 0.999975] [G loss: 1.000064]\n",
      "49130 [D loss: 0.999973] [G loss: 1.000071]\n",
      "49140 [D loss: 0.999981] [G loss: 1.000067]\n",
      "49150 [D loss: 0.999958] [G loss: 1.000077]\n",
      "49160 [D loss: 0.999958] [G loss: 1.000077]\n",
      "49170 [D loss: 0.999954] [G loss: 1.000072]\n",
      "49180 [D loss: 0.999982] [G loss: 1.000022]\n",
      "49190 [D loss: 0.999973] [G loss: 1.000072]\n",
      "49200 [D loss: 0.999968] [G loss: 1.000066]\n",
      "49210 [D loss: 0.999968] [G loss: 1.000038]\n",
      "49220 [D loss: 0.999969] [G loss: 1.000085]\n",
      "49230 [D loss: 0.999954] [G loss: 1.000067]\n",
      "49240 [D loss: 0.999979] [G loss: 1.000071]\n",
      "49250 [D loss: 0.999953] [G loss: 1.000051]\n",
      "49260 [D loss: 0.999981] [G loss: 1.000080]\n",
      "49270 [D loss: 0.999957] [G loss: 1.000074]\n",
      "49280 [D loss: 0.999964] [G loss: 1.000026]\n",
      "49290 [D loss: 0.999970] [G loss: 1.000060]\n",
      "49300 [D loss: 0.999964] [G loss: 1.000077]\n",
      "49310 [D loss: 0.999962] [G loss: 1.000059]\n",
      "49320 [D loss: 0.999960] [G loss: 1.000059]\n",
      "49330 [D loss: 0.999963] [G loss: 1.000100]\n",
      "49340 [D loss: 0.999967] [G loss: 1.000068]\n",
      "49350 [D loss: 0.999966] [G loss: 1.000066]\n",
      "49360 [D loss: 0.999970] [G loss: 1.000057]\n",
      "49370 [D loss: 0.999952] [G loss: 1.000043]\n",
      "49380 [D loss: 0.999963] [G loss: 1.000086]\n",
      "49390 [D loss: 0.999975] [G loss: 1.000045]\n",
      "49400 [D loss: 0.999958] [G loss: 1.000053]\n",
      "49410 [D loss: 0.999971] [G loss: 1.000086]\n",
      "49420 [D loss: 0.999971] [G loss: 1.000067]\n",
      "49430 [D loss: 0.999956] [G loss: 1.000069]\n",
      "49440 [D loss: 0.999992] [G loss: 1.000046]\n",
      "49450 [D loss: 0.999971] [G loss: 1.000041]\n",
      "49460 [D loss: 0.999968] [G loss: 1.000063]\n",
      "49470 [D loss: 0.999976] [G loss: 1.000078]\n",
      "49480 [D loss: 0.999967] [G loss: 1.000050]\n",
      "49490 [D loss: 0.999971] [G loss: 1.000078]\n",
      "49500 [D loss: 0.999973] [G loss: 1.000072]\n",
      "49510 [D loss: 0.999978] [G loss: 1.000034]\n",
      "49520 [D loss: 0.999955] [G loss: 1.000079]\n",
      "49530 [D loss: 0.999961] [G loss: 1.000114]\n",
      "49540 [D loss: 0.999969] [G loss: 1.000073]\n",
      "49550 [D loss: 0.999964] [G loss: 1.000064]\n",
      "49560 [D loss: 0.999985] [G loss: 1.000046]\n",
      "49570 [D loss: 0.999958] [G loss: 1.000069]\n",
      "49580 [D loss: 0.999952] [G loss: 1.000054]\n",
      "49590 [D loss: 0.999973] [G loss: 1.000069]\n",
      "49600 [D loss: 0.999962] [G loss: 1.000069]\n",
      "49610 [D loss: 0.999964] [G loss: 1.000060]\n",
      "49620 [D loss: 0.999966] [G loss: 1.000032]\n",
      "49630 [D loss: 0.999978] [G loss: 1.000062]\n",
      "49640 [D loss: 0.999972] [G loss: 1.000067]\n",
      "49650 [D loss: 0.999966] [G loss: 1.000058]\n",
      "49660 [D loss: 0.999964] [G loss: 1.000033]\n",
      "49670 [D loss: 0.999975] [G loss: 1.000069]\n",
      "49680 [D loss: 0.999952] [G loss: 1.000078]\n",
      "49690 [D loss: 0.999971] [G loss: 1.000053]\n",
      "49700 [D loss: 0.999962] [G loss: 1.000038]\n",
      "49710 [D loss: 0.999965] [G loss: 1.000068]\n",
      "49720 [D loss: 0.999963] [G loss: 1.000082]\n",
      "49730 [D loss: 0.999953] [G loss: 1.000042]\n",
      "49740 [D loss: 0.999977] [G loss: 1.000056]\n",
      "49750 [D loss: 0.999959] [G loss: 1.000059]\n",
      "49760 [D loss: 0.999960] [G loss: 1.000057]\n",
      "49770 [D loss: 0.999967] [G loss: 1.000056]\n",
      "49780 [D loss: 0.999971] [G loss: 1.000070]\n",
      "49790 [D loss: 0.999962] [G loss: 1.000072]\n",
      "49800 [D loss: 0.999973] [G loss: 1.000055]\n",
      "49810 [D loss: 0.999972] [G loss: 1.000054]\n",
      "49820 [D loss: 0.999973] [G loss: 1.000058]\n",
      "49830 [D loss: 0.999976] [G loss: 1.000055]\n",
      "49840 [D loss: 0.999973] [G loss: 1.000069]\n",
      "49850 [D loss: 0.999959] [G loss: 1.000060]\n",
      "49860 [D loss: 0.999975] [G loss: 1.000059]\n",
      "49870 [D loss: 0.999978] [G loss: 1.000066]\n",
      "49880 [D loss: 0.999973] [G loss: 1.000053]\n",
      "49890 [D loss: 0.999962] [G loss: 1.000070]\n",
      "49900 [D loss: 0.999966] [G loss: 1.000057]\n",
      "49910 [D loss: 0.999964] [G loss: 1.000066]\n",
      "49920 [D loss: 0.999972] [G loss: 1.000071]\n",
      "49930 [D loss: 0.999963] [G loss: 1.000084]\n",
      "49940 [D loss: 0.999959] [G loss: 1.000086]\n",
      "49950 [D loss: 0.999954] [G loss: 1.000051]\n",
      "49960 [D loss: 0.999957] [G loss: 1.000064]\n",
      "49970 [D loss: 0.999966] [G loss: 1.000055]\n",
      "49980 [D loss: 0.999962] [G loss: 1.000073]\n",
      "49990 [D loss: 0.999973] [G loss: 1.000039]\n",
      "50000 [D loss: 0.999965] [G loss: 1.000033]\n",
      "50010 [D loss: 0.999948] [G loss: 1.000053]\n",
      "50020 [D loss: 1.000005] [G loss: 1.000032]\n",
      "50030 [D loss: 0.999975] [G loss: 1.000046]\n",
      "50040 [D loss: 0.999945] [G loss: 1.000084]\n",
      "50050 [D loss: 0.999961] [G loss: 1.000050]\n",
      "50060 [D loss: 0.999970] [G loss: 1.000018]\n",
      "50070 [D loss: 0.999980] [G loss: 1.000077]\n",
      "50080 [D loss: 0.999974] [G loss: 1.000078]\n",
      "50090 [D loss: 0.999962] [G loss: 1.000047]\n",
      "50100 [D loss: 0.999970] [G loss: 1.000043]\n",
      "50110 [D loss: 0.999971] [G loss: 1.000053]\n",
      "50120 [D loss: 0.999957] [G loss: 1.000064]\n",
      "50130 [D loss: 0.999967] [G loss: 1.000050]\n",
      "50140 [D loss: 0.999971] [G loss: 1.000050]\n",
      "50150 [D loss: 0.999953] [G loss: 1.000048]\n",
      "50160 [D loss: 0.999937] [G loss: 1.000078]\n",
      "50170 [D loss: 0.999982] [G loss: 1.000062]\n",
      "50180 [D loss: 0.999955] [G loss: 1.000056]\n",
      "50190 [D loss: 0.999980] [G loss: 1.000086]\n",
      "50200 [D loss: 0.999961] [G loss: 1.000082]\n",
      "50210 [D loss: 0.999962] [G loss: 1.000070]\n",
      "50220 [D loss: 0.999973] [G loss: 1.000058]\n",
      "50230 [D loss: 0.999969] [G loss: 1.000059]\n",
      "50240 [D loss: 0.999963] [G loss: 1.000061]\n",
      "50250 [D loss: 0.999972] [G loss: 1.000072]\n",
      "50260 [D loss: 0.999962] [G loss: 1.000054]\n",
      "50270 [D loss: 0.999957] [G loss: 1.000068]\n",
      "50280 [D loss: 0.999972] [G loss: 1.000092]\n",
      "50290 [D loss: 0.999975] [G loss: 1.000059]\n",
      "50300 [D loss: 0.999958] [G loss: 1.000060]\n",
      "50310 [D loss: 0.999967] [G loss: 1.000083]\n",
      "50320 [D loss: 0.999954] [G loss: 1.000098]\n",
      "50330 [D loss: 0.999967] [G loss: 1.000078]\n",
      "50340 [D loss: 0.999965] [G loss: 1.000054]\n",
      "50350 [D loss: 0.999965] [G loss: 1.000059]\n",
      "50360 [D loss: 0.999968] [G loss: 1.000062]\n",
      "50370 [D loss: 0.999964] [G loss: 1.000075]\n",
      "50380 [D loss: 0.999946] [G loss: 1.000084]\n",
      "50390 [D loss: 0.999985] [G loss: 1.000044]\n",
      "50400 [D loss: 0.999962] [G loss: 1.000062]\n",
      "50410 [D loss: 0.999971] [G loss: 1.000083]\n",
      "50420 [D loss: 0.999964] [G loss: 1.000076]\n",
      "50430 [D loss: 0.999969] [G loss: 1.000073]\n",
      "50440 [D loss: 0.999976] [G loss: 1.000070]\n",
      "50450 [D loss: 0.999979] [G loss: 1.000078]\n",
      "50460 [D loss: 0.999961] [G loss: 1.000091]\n",
      "50470 [D loss: 0.999969] [G loss: 1.000064]\n",
      "50480 [D loss: 0.999950] [G loss: 1.000073]\n",
      "50490 [D loss: 0.999959] [G loss: 1.000049]\n",
      "50500 [D loss: 0.999956] [G loss: 1.000054]\n",
      "50510 [D loss: 0.999967] [G loss: 1.000077]\n",
      "50520 [D loss: 0.999965] [G loss: 1.000082]\n",
      "50530 [D loss: 0.999965] [G loss: 1.000049]\n",
      "50540 [D loss: 0.999965] [G loss: 1.000047]\n",
      "50550 [D loss: 0.999971] [G loss: 1.000066]\n",
      "50560 [D loss: 0.999982] [G loss: 1.000054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50570 [D loss: 0.999988] [G loss: 1.000045]\n",
      "50580 [D loss: 0.999965] [G loss: 1.000046]\n",
      "50590 [D loss: 0.999967] [G loss: 1.000087]\n",
      "50600 [D loss: 0.999972] [G loss: 1.000052]\n",
      "50610 [D loss: 0.999961] [G loss: 1.000061]\n",
      "50620 [D loss: 0.999963] [G loss: 1.000063]\n",
      "50630 [D loss: 0.999991] [G loss: 1.000069]\n",
      "50640 [D loss: 0.999971] [G loss: 1.000019]\n",
      "50650 [D loss: 0.999959] [G loss: 1.000063]\n",
      "50660 [D loss: 0.999971] [G loss: 1.000056]\n",
      "50670 [D loss: 0.999971] [G loss: 1.000036]\n",
      "50680 [D loss: 0.999967] [G loss: 1.000075]\n",
      "50690 [D loss: 0.999957] [G loss: 1.000055]\n",
      "50700 [D loss: 0.999977] [G loss: 1.000080]\n",
      "50710 [D loss: 0.999965] [G loss: 1.000045]\n",
      "50720 [D loss: 0.999956] [G loss: 1.000055]\n",
      "50730 [D loss: 0.999957] [G loss: 1.000054]\n",
      "50740 [D loss: 0.999996] [G loss: 1.000023]\n",
      "50750 [D loss: 0.999973] [G loss: 1.000060]\n",
      "50760 [D loss: 0.999957] [G loss: 1.000076]\n",
      "50770 [D loss: 0.999971] [G loss: 1.000087]\n",
      "50780 [D loss: 0.999999] [G loss: 1.000040]\n",
      "50790 [D loss: 0.999971] [G loss: 1.000074]\n",
      "50800 [D loss: 0.999973] [G loss: 1.000092]\n",
      "50810 [D loss: 0.999988] [G loss: 1.000038]\n",
      "50820 [D loss: 0.999967] [G loss: 1.000064]\n",
      "50830 [D loss: 0.999959] [G loss: 1.000083]\n",
      "50840 [D loss: 0.999991] [G loss: 1.000090]\n",
      "50850 [D loss: 0.999961] [G loss: 1.000063]\n",
      "50860 [D loss: 0.999958] [G loss: 1.000072]\n",
      "50870 [D loss: 0.999975] [G loss: 1.000070]\n",
      "50880 [D loss: 0.999967] [G loss: 1.000080]\n",
      "50890 [D loss: 0.999958] [G loss: 1.000043]\n",
      "50900 [D loss: 0.999964] [G loss: 1.000054]\n",
      "50910 [D loss: 0.999979] [G loss: 1.000072]\n",
      "50920 [D loss: 0.999952] [G loss: 1.000079]\n",
      "50930 [D loss: 0.999978] [G loss: 1.000063]\n",
      "50940 [D loss: 0.999959] [G loss: 1.000048]\n",
      "50950 [D loss: 0.999949] [G loss: 1.000064]\n",
      "50960 [D loss: 0.999970] [G loss: 1.000035]\n",
      "50970 [D loss: 0.999963] [G loss: 1.000051]\n",
      "50980 [D loss: 0.999970] [G loss: 1.000050]\n",
      "50990 [D loss: 0.999975] [G loss: 1.000058]\n",
      "51000 [D loss: 0.999966] [G loss: 1.000078]\n",
      "51010 [D loss: 0.999959] [G loss: 1.000078]\n",
      "51020 [D loss: 0.999971] [G loss: 1.000083]\n",
      "51030 [D loss: 0.999966] [G loss: 1.000062]\n",
      "51040 [D loss: 0.999974] [G loss: 1.000049]\n",
      "51050 [D loss: 0.999971] [G loss: 1.000080]\n",
      "51060 [D loss: 0.999968] [G loss: 1.000091]\n",
      "51070 [D loss: 0.999954] [G loss: 1.000062]\n",
      "51080 [D loss: 0.999974] [G loss: 1.000068]\n",
      "51090 [D loss: 0.999967] [G loss: 1.000063]\n",
      "51100 [D loss: 0.999957] [G loss: 1.000067]\n",
      "51110 [D loss: 0.999963] [G loss: 1.000047]\n",
      "51120 [D loss: 0.999963] [G loss: 1.000054]\n",
      "51130 [D loss: 0.999977] [G loss: 1.000037]\n",
      "51140 [D loss: 0.999973] [G loss: 1.000087]\n",
      "51150 [D loss: 0.999958] [G loss: 1.000065]\n",
      "51160 [D loss: 0.999977] [G loss: 1.000081]\n",
      "51170 [D loss: 0.999985] [G loss: 1.000055]\n",
      "51180 [D loss: 0.999947] [G loss: 1.000097]\n",
      "51190 [D loss: 0.999966] [G loss: 1.000054]\n",
      "51200 [D loss: 0.999967] [G loss: 1.000052]\n",
      "51210 [D loss: 0.999962] [G loss: 1.000055]\n",
      "51220 [D loss: 0.999962] [G loss: 1.000067]\n",
      "51230 [D loss: 0.999972] [G loss: 1.000052]\n",
      "51240 [D loss: 0.999940] [G loss: 1.000065]\n",
      "51250 [D loss: 0.999971] [G loss: 1.000042]\n",
      "51260 [D loss: 0.999958] [G loss: 1.000062]\n",
      "51270 [D loss: 0.999972] [G loss: 1.000067]\n",
      "51280 [D loss: 0.999972] [G loss: 1.000071]\n",
      "51290 [D loss: 0.999973] [G loss: 1.000064]\n",
      "51300 [D loss: 0.999961] [G loss: 1.000060]\n",
      "51310 [D loss: 0.999970] [G loss: 1.000075]\n",
      "51320 [D loss: 0.999973] [G loss: 1.000071]\n",
      "51330 [D loss: 0.999957] [G loss: 1.000067]\n",
      "51340 [D loss: 0.999962] [G loss: 1.000066]\n",
      "51350 [D loss: 0.999964] [G loss: 1.000068]\n",
      "51360 [D loss: 0.999954] [G loss: 1.000067]\n",
      "51370 [D loss: 0.999953] [G loss: 1.000073]\n",
      "51380 [D loss: 0.999985] [G loss: 1.000053]\n",
      "51390 [D loss: 0.999970] [G loss: 1.000060]\n",
      "51400 [D loss: 0.999964] [G loss: 1.000071]\n",
      "51410 [D loss: 0.999973] [G loss: 1.000028]\n",
      "51420 [D loss: 0.999961] [G loss: 1.000066]\n",
      "51430 [D loss: 0.999961] [G loss: 1.000064]\n",
      "51440 [D loss: 0.999965] [G loss: 1.000044]\n",
      "51450 [D loss: 0.999956] [G loss: 1.000072]\n",
      "51460 [D loss: 0.999961] [G loss: 1.000073]\n",
      "51470 [D loss: 0.999958] [G loss: 1.000055]\n",
      "51480 [D loss: 0.999955] [G loss: 1.000048]\n",
      "51490 [D loss: 0.999967] [G loss: 1.000078]\n",
      "51500 [D loss: 0.999991] [G loss: 1.000039]\n",
      "51510 [D loss: 0.999968] [G loss: 1.000038]\n",
      "51520 [D loss: 0.999961] [G loss: 1.000070]\n",
      "51530 [D loss: 0.999988] [G loss: 1.000088]\n",
      "51540 [D loss: 0.999966] [G loss: 1.000049]\n",
      "51550 [D loss: 0.999973] [G loss: 1.000069]\n",
      "51560 [D loss: 0.999968] [G loss: 1.000090]\n",
      "51570 [D loss: 0.999967] [G loss: 1.000051]\n",
      "51580 [D loss: 0.999969] [G loss: 1.000043]\n",
      "51590 [D loss: 0.999951] [G loss: 1.000083]\n",
      "51600 [D loss: 0.999990] [G loss: 1.000016]\n",
      "51610 [D loss: 0.999972] [G loss: 1.000084]\n",
      "51620 [D loss: 0.999964] [G loss: 1.000088]\n",
      "51630 [D loss: 0.999977] [G loss: 1.000038]\n",
      "51640 [D loss: 0.999962] [G loss: 1.000070]\n",
      "51650 [D loss: 0.999975] [G loss: 1.000085]\n",
      "51660 [D loss: 0.999961] [G loss: 1.000045]\n",
      "51670 [D loss: 0.999966] [G loss: 1.000065]\n",
      "51680 [D loss: 0.999948] [G loss: 1.000076]\n",
      "51690 [D loss: 0.999973] [G loss: 1.000092]\n",
      "51700 [D loss: 0.999971] [G loss: 1.000093]\n",
      "51710 [D loss: 0.999963] [G loss: 1.000057]\n",
      "51720 [D loss: 0.999971] [G loss: 1.000056]\n",
      "51730 [D loss: 0.999978] [G loss: 1.000082]\n",
      "51740 [D loss: 0.999958] [G loss: 1.000055]\n",
      "51750 [D loss: 0.999953] [G loss: 1.000084]\n",
      "51760 [D loss: 0.999959] [G loss: 1.000026]\n",
      "51770 [D loss: 0.999970] [G loss: 1.000073]\n",
      "51780 [D loss: 0.999994] [G loss: 1.000057]\n",
      "51790 [D loss: 0.999996] [G loss: 1.000037]\n",
      "51800 [D loss: 0.999968] [G loss: 1.000075]\n",
      "51810 [D loss: 0.999977] [G loss: 1.000024]\n",
      "51820 [D loss: 0.999958] [G loss: 1.000066]\n",
      "51830 [D loss: 0.999952] [G loss: 1.000043]\n",
      "51840 [D loss: 0.999964] [G loss: 1.000026]\n",
      "51850 [D loss: 0.999967] [G loss: 1.000122]\n",
      "51860 [D loss: 0.999964] [G loss: 1.000020]\n",
      "51870 [D loss: 0.999962] [G loss: 1.000052]\n",
      "51880 [D loss: 0.999963] [G loss: 1.000069]\n",
      "51890 [D loss: 0.999947] [G loss: 1.000056]\n",
      "51900 [D loss: 0.999957] [G loss: 1.000068]\n",
      "51910 [D loss: 0.999951] [G loss: 1.000068]\n",
      "51920 [D loss: 0.999969] [G loss: 1.000037]\n",
      "51930 [D loss: 0.999968] [G loss: 1.000064]\n",
      "51940 [D loss: 0.999958] [G loss: 1.000081]\n",
      "51950 [D loss: 0.999966] [G loss: 1.000087]\n",
      "51960 [D loss: 0.999960] [G loss: 1.000072]\n",
      "51970 [D loss: 0.999969] [G loss: 1.000024]\n",
      "51980 [D loss: 0.999957] [G loss: 1.000060]\n",
      "51990 [D loss: 0.999958] [G loss: 1.000070]\n",
      "52000 [D loss: 0.999984] [G loss: 1.000074]\n",
      "52010 [D loss: 0.999961] [G loss: 1.000062]\n",
      "52020 [D loss: 0.999968] [G loss: 1.000058]\n",
      "52030 [D loss: 0.999959] [G loss: 1.000066]\n",
      "52040 [D loss: 0.999983] [G loss: 1.000076]\n",
      "52050 [D loss: 0.999973] [G loss: 1.000042]\n",
      "52060 [D loss: 0.999965] [G loss: 1.000064]\n",
      "52070 [D loss: 0.999954] [G loss: 1.000060]\n",
      "52080 [D loss: 0.999966] [G loss: 1.000043]\n",
      "52090 [D loss: 0.999965] [G loss: 1.000083]\n",
      "52100 [D loss: 1.000006] [G loss: 1.000025]\n",
      "52110 [D loss: 0.999968] [G loss: 1.000081]\n",
      "52120 [D loss: 0.999949] [G loss: 1.000102]\n",
      "52130 [D loss: 0.999974] [G loss: 1.000052]\n",
      "52140 [D loss: 0.999961] [G loss: 1.000067]\n",
      "52150 [D loss: 0.999972] [G loss: 1.000054]\n",
      "52160 [D loss: 0.999959] [G loss: 1.000059]\n",
      "52170 [D loss: 0.999944] [G loss: 1.000062]\n",
      "52180 [D loss: 0.999974] [G loss: 1.000061]\n",
      "52190 [D loss: 0.999964] [G loss: 1.000019]\n",
      "52200 [D loss: 0.999964] [G loss: 1.000042]\n",
      "52210 [D loss: 0.999954] [G loss: 1.000074]\n",
      "52220 [D loss: 0.999960] [G loss: 1.000060]\n",
      "52230 [D loss: 0.999956] [G loss: 1.000067]\n",
      "52240 [D loss: 0.999946] [G loss: 1.000062]\n",
      "52250 [D loss: 0.999972] [G loss: 1.000085]\n",
      "52260 [D loss: 0.999972] [G loss: 1.000038]\n",
      "52270 [D loss: 0.999964] [G loss: 1.000035]\n",
      "52280 [D loss: 0.999961] [G loss: 1.000053]\n",
      "52290 [D loss: 0.999961] [G loss: 1.000089]\n",
      "52300 [D loss: 0.999960] [G loss: 1.000091]\n",
      "52310 [D loss: 0.999958] [G loss: 1.000058]\n",
      "52320 [D loss: 0.999968] [G loss: 1.000091]\n",
      "52330 [D loss: 0.999972] [G loss: 1.000043]\n",
      "52340 [D loss: 0.999954] [G loss: 1.000084]\n",
      "52350 [D loss: 0.999988] [G loss: 1.000134]\n",
      "52360 [D loss: 0.999978] [G loss: 0.999999]\n",
      "52370 [D loss: 0.999966] [G loss: 1.000058]\n",
      "52380 [D loss: 0.999954] [G loss: 1.000082]\n",
      "52390 [D loss: 0.999965] [G loss: 1.000037]\n",
      "52400 [D loss: 0.999969] [G loss: 1.000080]\n",
      "52410 [D loss: 0.999969] [G loss: 1.000085]\n",
      "52420 [D loss: 0.999940] [G loss: 1.000023]\n",
      "52430 [D loss: 0.999966] [G loss: 1.000069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52440 [D loss: 0.999951] [G loss: 1.000058]\n",
      "52450 [D loss: 0.999948] [G loss: 1.000051]\n",
      "52460 [D loss: 0.999985] [G loss: 1.000071]\n",
      "52470 [D loss: 0.999949] [G loss: 1.000041]\n",
      "52480 [D loss: 0.999968] [G loss: 1.000086]\n",
      "52490 [D loss: 0.999961] [G loss: 1.000037]\n",
      "52500 [D loss: 0.999955] [G loss: 1.000027]\n",
      "52510 [D loss: 0.999959] [G loss: 1.000069]\n",
      "52520 [D loss: 0.999950] [G loss: 1.000098]\n",
      "52530 [D loss: 0.999975] [G loss: 1.000048]\n",
      "52540 [D loss: 0.999967] [G loss: 1.000085]\n",
      "52550 [D loss: 0.999919] [G loss: 1.000071]\n",
      "52560 [D loss: 1.000012] [G loss: 1.000096]\n",
      "52570 [D loss: 0.999969] [G loss: 1.000087]\n",
      "52580 [D loss: 0.999967] [G loss: 1.000074]\n",
      "52590 [D loss: 0.999954] [G loss: 1.000069]\n",
      "52600 [D loss: 0.999961] [G loss: 1.000073]\n",
      "52610 [D loss: 0.999952] [G loss: 1.000122]\n",
      "52620 [D loss: 0.999966] [G loss: 1.000073]\n",
      "52630 [D loss: 0.999969] [G loss: 1.000072]\n",
      "52640 [D loss: 0.999969] [G loss: 1.000055]\n",
      "52650 [D loss: 0.999966] [G loss: 1.000059]\n",
      "52660 [D loss: 0.999959] [G loss: 1.000065]\n",
      "52670 [D loss: 0.999954] [G loss: 1.000092]\n",
      "52680 [D loss: 0.999961] [G loss: 1.000055]\n",
      "52690 [D loss: 0.999968] [G loss: 1.000067]\n",
      "52700 [D loss: 0.999982] [G loss: 1.000050]\n",
      "52710 [D loss: 0.999969] [G loss: 1.000048]\n",
      "52720 [D loss: 0.999962] [G loss: 1.000065]\n",
      "52730 [D loss: 0.999961] [G loss: 1.000069]\n",
      "52740 [D loss: 0.999956] [G loss: 1.000054]\n",
      "52750 [D loss: 0.999972] [G loss: 1.000072]\n",
      "52760 [D loss: 0.999970] [G loss: 1.000063]\n",
      "52770 [D loss: 0.999973] [G loss: 1.000050]\n",
      "52780 [D loss: 1.000002] [G loss: 1.000026]\n",
      "52790 [D loss: 0.999975] [G loss: 1.000052]\n",
      "52800 [D loss: 0.999972] [G loss: 1.000031]\n",
      "52810 [D loss: 0.999983] [G loss: 1.000061]\n",
      "52820 [D loss: 0.999953] [G loss: 1.000079]\n",
      "52830 [D loss: 0.999972] [G loss: 1.000035]\n",
      "52840 [D loss: 0.999978] [G loss: 1.000059]\n",
      "52850 [D loss: 0.999956] [G loss: 1.000063]\n",
      "52860 [D loss: 0.999957] [G loss: 1.000055]\n",
      "52870 [D loss: 0.999983] [G loss: 1.000036]\n",
      "52880 [D loss: 0.999960] [G loss: 1.000073]\n",
      "52890 [D loss: 0.999968] [G loss: 1.000068]\n",
      "52900 [D loss: 0.999979] [G loss: 1.000048]\n",
      "52910 [D loss: 0.999970] [G loss: 1.000058]\n",
      "52920 [D loss: 0.999963] [G loss: 1.000082]\n",
      "52930 [D loss: 0.999964] [G loss: 1.000062]\n",
      "52940 [D loss: 0.999973] [G loss: 1.000065]\n",
      "52950 [D loss: 0.999969] [G loss: 1.000081]\n",
      "52960 [D loss: 0.999966] [G loss: 1.000062]\n",
      "52970 [D loss: 0.999962] [G loss: 1.000057]\n",
      "52980 [D loss: 0.999970] [G loss: 1.000088]\n",
      "52990 [D loss: 0.999968] [G loss: 1.000077]\n",
      "53000 [D loss: 0.999963] [G loss: 1.000080]\n",
      "53010 [D loss: 0.999972] [G loss: 1.000031]\n",
      "53020 [D loss: 0.999943] [G loss: 1.000056]\n",
      "53030 [D loss: 0.999956] [G loss: 1.000087]\n",
      "53040 [D loss: 1.000015] [G loss: 1.000013]\n",
      "53050 [D loss: 0.999982] [G loss: 1.000049]\n",
      "53060 [D loss: 0.999960] [G loss: 1.000059]\n",
      "53070 [D loss: 0.999971] [G loss: 1.000085]\n",
      "53080 [D loss: 0.999977] [G loss: 1.000054]\n",
      "53090 [D loss: 0.999960] [G loss: 1.000048]\n",
      "53100 [D loss: 0.999969] [G loss: 1.000086]\n",
      "53110 [D loss: 0.999967] [G loss: 1.000079]\n",
      "53120 [D loss: 0.999940] [G loss: 1.000085]\n",
      "53130 [D loss: 0.999976] [G loss: 1.000077]\n",
      "53140 [D loss: 0.999954] [G loss: 1.000075]\n",
      "53150 [D loss: 0.999975] [G loss: 1.000059]\n",
      "53160 [D loss: 0.999977] [G loss: 1.000090]\n",
      "53170 [D loss: 0.999953] [G loss: 1.000039]\n",
      "53180 [D loss: 0.999980] [G loss: 1.000044]\n",
      "53190 [D loss: 0.999964] [G loss: 1.000058]\n",
      "53200 [D loss: 0.999964] [G loss: 1.000068]\n",
      "53210 [D loss: 0.999975] [G loss: 1.000065]\n",
      "53220 [D loss: 0.999961] [G loss: 1.000071]\n",
      "53230 [D loss: 0.999974] [G loss: 1.000056]\n",
      "53240 [D loss: 0.999960] [G loss: 1.000065]\n",
      "53250 [D loss: 0.999964] [G loss: 1.000055]\n",
      "53260 [D loss: 0.999952] [G loss: 1.000074]\n",
      "53270 [D loss: 0.999965] [G loss: 1.000047]\n",
      "53280 [D loss: 0.999981] [G loss: 1.000026]\n",
      "53290 [D loss: 0.999963] [G loss: 1.000070]\n",
      "53300 [D loss: 0.999961] [G loss: 1.000073]\n",
      "53310 [D loss: 0.999960] [G loss: 1.000068]\n",
      "53320 [D loss: 0.999953] [G loss: 1.000053]\n",
      "53330 [D loss: 0.999965] [G loss: 1.000075]\n",
      "53340 [D loss: 0.999956] [G loss: 1.000061]\n",
      "53350 [D loss: 0.999983] [G loss: 1.000055]\n",
      "53360 [D loss: 0.999966] [G loss: 1.000056]\n",
      "53370 [D loss: 0.999967] [G loss: 1.000060]\n",
      "53380 [D loss: 0.999964] [G loss: 1.000073]\n",
      "53390 [D loss: 0.999964] [G loss: 1.000079]\n",
      "53400 [D loss: 0.999960] [G loss: 1.000068]\n",
      "53410 [D loss: 0.999966] [G loss: 1.000065]\n",
      "53420 [D loss: 0.999966] [G loss: 1.000048]\n",
      "53430 [D loss: 0.999978] [G loss: 1.000044]\n",
      "53440 [D loss: 0.999966] [G loss: 1.000073]\n",
      "53450 [D loss: 0.999972] [G loss: 1.000062]\n",
      "53460 [D loss: 0.999947] [G loss: 1.000073]\n",
      "53470 [D loss: 0.999980] [G loss: 1.000068]\n",
      "53480 [D loss: 0.999980] [G loss: 1.000061]\n",
      "53490 [D loss: 0.999954] [G loss: 1.000052]\n",
      "53500 [D loss: 0.999965] [G loss: 1.000041]\n",
      "53510 [D loss: 0.999966] [G loss: 1.000059]\n",
      "53520 [D loss: 0.999983] [G loss: 1.000064]\n",
      "53530 [D loss: 0.999975] [G loss: 1.000070]\n",
      "53540 [D loss: 0.999968] [G loss: 1.000104]\n",
      "53550 [D loss: 0.999979] [G loss: 0.999999]\n",
      "53560 [D loss: 0.999971] [G loss: 1.000062]\n",
      "53570 [D loss: 0.999983] [G loss: 1.000060]\n",
      "53580 [D loss: 0.999965] [G loss: 1.000039]\n",
      "53590 [D loss: 0.999971] [G loss: 1.000064]\n",
      "53600 [D loss: 0.999970] [G loss: 1.000067]\n",
      "53610 [D loss: 0.999969] [G loss: 1.000069]\n",
      "53620 [D loss: 0.999955] [G loss: 1.000086]\n",
      "53630 [D loss: 1.000000] [G loss: 1.000056]\n",
      "53640 [D loss: 0.999986] [G loss: 1.000058]\n",
      "53650 [D loss: 0.999953] [G loss: 1.000095]\n",
      "53660 [D loss: 0.999977] [G loss: 1.000051]\n",
      "53670 [D loss: 0.999968] [G loss: 1.000059]\n",
      "53680 [D loss: 0.999954] [G loss: 1.000093]\n",
      "53690 [D loss: 0.999971] [G loss: 1.000071]\n",
      "53700 [D loss: 0.999963] [G loss: 1.000080]\n",
      "53710 [D loss: 0.999963] [G loss: 1.000088]\n",
      "53720 [D loss: 0.999984] [G loss: 1.000084]\n",
      "53730 [D loss: 0.999981] [G loss: 1.000050]\n",
      "53740 [D loss: 0.999974] [G loss: 1.000072]\n",
      "53750 [D loss: 0.999992] [G loss: 1.000049]\n",
      "53760 [D loss: 0.999964] [G loss: 1.000086]\n",
      "53770 [D loss: 0.999966] [G loss: 1.000078]\n",
      "53780 [D loss: 0.999968] [G loss: 1.000055]\n",
      "53790 [D loss: 0.999980] [G loss: 1.000073]\n",
      "53800 [D loss: 0.999968] [G loss: 1.000068]\n",
      "53810 [D loss: 0.999948] [G loss: 1.000079]\n",
      "53820 [D loss: 0.999975] [G loss: 1.000027]\n",
      "53830 [D loss: 0.999958] [G loss: 1.000061]\n",
      "53840 [D loss: 0.999967] [G loss: 1.000089]\n",
      "53850 [D loss: 0.999952] [G loss: 1.000035]\n",
      "53860 [D loss: 0.999962] [G loss: 1.000054]\n",
      "53870 [D loss: 0.999967] [G loss: 1.000089]\n",
      "53880 [D loss: 0.999977] [G loss: 1.000016]\n",
      "53890 [D loss: 0.999958] [G loss: 1.000080]\n",
      "53900 [D loss: 0.999953] [G loss: 1.000094]\n",
      "53910 [D loss: 0.999979] [G loss: 1.000044]\n",
      "53920 [D loss: 0.999968] [G loss: 1.000067]\n",
      "53930 [D loss: 0.999957] [G loss: 1.000093]\n",
      "53940 [D loss: 0.999990] [G loss: 1.000052]\n",
      "53950 [D loss: 0.999980] [G loss: 1.000058]\n",
      "53960 [D loss: 0.999947] [G loss: 1.000079]\n",
      "53970 [D loss: 0.999961] [G loss: 1.000101]\n",
      "53980 [D loss: 0.999984] [G loss: 1.000044]\n",
      "53990 [D loss: 0.999987] [G loss: 1.000045]\n",
      "54000 [D loss: 0.999955] [G loss: 1.000087]\n",
      "54010 [D loss: 0.999960] [G loss: 1.000096]\n",
      "54020 [D loss: 0.999982] [G loss: 1.000018]\n",
      "54030 [D loss: 0.999953] [G loss: 1.000074]\n",
      "54040 [D loss: 0.999967] [G loss: 1.000088]\n",
      "54050 [D loss: 0.999994] [G loss: 1.000003]\n",
      "54060 [D loss: 0.999953] [G loss: 1.000064]\n",
      "54070 [D loss: 0.999971] [G loss: 1.000074]\n",
      "54080 [D loss: 0.999968] [G loss: 1.000049]\n",
      "54090 [D loss: 0.999961] [G loss: 1.000084]\n",
      "54100 [D loss: 0.999977] [G loss: 1.000060]\n",
      "54110 [D loss: 0.999953] [G loss: 1.000066]\n",
      "54120 [D loss: 0.999962] [G loss: 1.000075]\n",
      "54130 [D loss: 0.999965] [G loss: 1.000090]\n",
      "54140 [D loss: 0.999963] [G loss: 1.000060]\n",
      "54150 [D loss: 0.999962] [G loss: 1.000064]\n",
      "54160 [D loss: 0.999975] [G loss: 1.000077]\n",
      "54170 [D loss: 0.999961] [G loss: 1.000070]\n",
      "54180 [D loss: 0.999965] [G loss: 1.000103]\n",
      "54190 [D loss: 0.999972] [G loss: 1.000043]\n",
      "54200 [D loss: 0.999971] [G loss: 1.000073]\n",
      "54210 [D loss: 0.999969] [G loss: 1.000046]\n",
      "54220 [D loss: 0.999971] [G loss: 1.000036]\n",
      "54230 [D loss: 0.999960] [G loss: 1.000090]\n",
      "54240 [D loss: 0.999962] [G loss: 1.000064]\n",
      "54250 [D loss: 0.999962] [G loss: 1.000024]\n",
      "54260 [D loss: 0.999961] [G loss: 1.000057]\n",
      "54270 [D loss: 0.999964] [G loss: 1.000075]\n",
      "54280 [D loss: 0.999961] [G loss: 1.000055]\n",
      "54290 [D loss: 0.999966] [G loss: 1.000021]\n",
      "54300 [D loss: 0.999957] [G loss: 1.000056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54310 [D loss: 0.999974] [G loss: 1.000072]\n",
      "54320 [D loss: 0.999975] [G loss: 1.000055]\n",
      "54330 [D loss: 0.999975] [G loss: 1.000066]\n",
      "54340 [D loss: 0.999966] [G loss: 1.000066]\n",
      "54350 [D loss: 0.999972] [G loss: 1.000052]\n",
      "54360 [D loss: 0.999961] [G loss: 1.000058]\n",
      "54370 [D loss: 0.999968] [G loss: 1.000083]\n",
      "54380 [D loss: 0.999959] [G loss: 1.000069]\n",
      "54390 [D loss: 0.999975] [G loss: 1.000082]\n",
      "54400 [D loss: 0.999949] [G loss: 1.000064]\n",
      "54410 [D loss: 0.999967] [G loss: 1.000071]\n",
      "54420 [D loss: 0.999982] [G loss: 1.000048]\n",
      "54430 [D loss: 0.999956] [G loss: 1.000065]\n",
      "54440 [D loss: 0.999978] [G loss: 1.000070]\n",
      "54450 [D loss: 0.999983] [G loss: 1.000084]\n",
      "54460 [D loss: 0.999970] [G loss: 1.000019]\n",
      "54470 [D loss: 0.999956] [G loss: 1.000074]\n",
      "54480 [D loss: 0.999983] [G loss: 1.000067]\n",
      "54490 [D loss: 0.999996] [G loss: 1.000047]\n",
      "54500 [D loss: 0.999973] [G loss: 1.000039]\n",
      "54510 [D loss: 0.999977] [G loss: 1.000077]\n",
      "54520 [D loss: 0.999982] [G loss: 1.000032]\n",
      "54530 [D loss: 0.999968] [G loss: 1.000050]\n",
      "54540 [D loss: 0.999968] [G loss: 1.000089]\n",
      "54550 [D loss: 0.999976] [G loss: 1.000002]\n",
      "54560 [D loss: 0.999970] [G loss: 1.000069]\n",
      "54570 [D loss: 0.999968] [G loss: 1.000083]\n",
      "54580 [D loss: 0.999981] [G loss: 1.000059]\n",
      "54590 [D loss: 0.999974] [G loss: 1.000067]\n",
      "54600 [D loss: 0.999948] [G loss: 1.000109]\n",
      "54610 [D loss: 0.999967] [G loss: 1.000046]\n",
      "54620 [D loss: 0.999962] [G loss: 1.000081]\n",
      "54630 [D loss: 0.999980] [G loss: 1.000029]\n",
      "54640 [D loss: 0.999961] [G loss: 1.000022]\n",
      "54650 [D loss: 0.999948] [G loss: 1.000079]\n",
      "54660 [D loss: 0.999960] [G loss: 1.000092]\n",
      "54670 [D loss: 0.999968] [G loss: 1.000055]\n",
      "54680 [D loss: 0.999968] [G loss: 1.000060]\n",
      "54690 [D loss: 0.999969] [G loss: 1.000058]\n",
      "54700 [D loss: 0.999969] [G loss: 1.000096]\n",
      "54710 [D loss: 0.999974] [G loss: 1.000054]\n",
      "54720 [D loss: 0.999952] [G loss: 1.000070]\n",
      "54730 [D loss: 0.999969] [G loss: 1.000078]\n",
      "54740 [D loss: 0.999973] [G loss: 1.000026]\n",
      "54750 [D loss: 0.999953] [G loss: 1.000057]\n",
      "54760 [D loss: 0.999963] [G loss: 1.000064]\n",
      "54770 [D loss: 0.999973] [G loss: 1.000061]\n",
      "54780 [D loss: 0.999961] [G loss: 1.000073]\n",
      "54790 [D loss: 0.999969] [G loss: 1.000077]\n",
      "54800 [D loss: 0.999962] [G loss: 1.000035]\n",
      "54810 [D loss: 0.999973] [G loss: 1.000051]\n",
      "54820 [D loss: 0.999963] [G loss: 1.000045]\n",
      "54830 [D loss: 0.999976] [G loss: 1.000069]\n",
      "54840 [D loss: 0.999971] [G loss: 1.000099]\n",
      "54850 [D loss: 0.999958] [G loss: 1.000085]\n",
      "54860 [D loss: 0.999943] [G loss: 1.000060]\n",
      "54870 [D loss: 0.999964] [G loss: 1.000045]\n",
      "54880 [D loss: 0.999963] [G loss: 1.000081]\n",
      "54890 [D loss: 0.999966] [G loss: 1.000084]\n",
      "54900 [D loss: 0.999982] [G loss: 1.000006]\n",
      "54910 [D loss: 0.999968] [G loss: 1.000075]\n",
      "54920 [D loss: 0.999954] [G loss: 1.000072]\n",
      "54930 [D loss: 0.999979] [G loss: 1.000057]\n",
      "54940 [D loss: 0.999964] [G loss: 1.000075]\n",
      "54950 [D loss: 0.999975] [G loss: 1.000047]\n",
      "54960 [D loss: 0.999969] [G loss: 1.000063]\n",
      "54970 [D loss: 0.999958] [G loss: 1.000070]\n",
      "54980 [D loss: 0.999953] [G loss: 1.000073]\n",
      "54990 [D loss: 0.999958] [G loss: 1.000068]\n",
      "55000 [D loss: 0.999958] [G loss: 1.000075]\n",
      "55010 [D loss: 0.999964] [G loss: 1.000062]\n",
      "55020 [D loss: 0.999976] [G loss: 1.000050]\n",
      "55030 [D loss: 0.999961] [G loss: 1.000070]\n",
      "55040 [D loss: 0.999958] [G loss: 1.000054]\n",
      "55050 [D loss: 0.999972] [G loss: 1.000071]\n",
      "55060 [D loss: 0.999968] [G loss: 1.000050]\n",
      "55070 [D loss: 0.999943] [G loss: 1.000081]\n",
      "55080 [D loss: 0.999966] [G loss: 1.000054]\n",
      "55090 [D loss: 0.999962] [G loss: 1.000059]\n",
      "55100 [D loss: 0.999958] [G loss: 1.000061]\n",
      "55110 [D loss: 0.999973] [G loss: 1.000065]\n",
      "55120 [D loss: 0.999969] [G loss: 1.000032]\n",
      "55130 [D loss: 0.999965] [G loss: 1.000066]\n",
      "55140 [D loss: 0.999970] [G loss: 1.000035]\n",
      "55150 [D loss: 0.999969] [G loss: 1.000047]\n",
      "55160 [D loss: 0.999966] [G loss: 1.000068]\n",
      "55170 [D loss: 0.999966] [G loss: 1.000042]\n",
      "55180 [D loss: 0.999967] [G loss: 1.000064]\n",
      "55190 [D loss: 0.999970] [G loss: 1.000051]\n",
      "55200 [D loss: 0.999979] [G loss: 1.000092]\n",
      "55210 [D loss: 0.999968] [G loss: 1.000066]\n",
      "55220 [D loss: 0.999977] [G loss: 1.000036]\n",
      "55230 [D loss: 0.999969] [G loss: 1.000062]\n",
      "55240 [D loss: 0.999955] [G loss: 1.000055]\n",
      "55250 [D loss: 0.999975] [G loss: 1.000044]\n",
      "55260 [D loss: 0.999962] [G loss: 1.000075]\n",
      "55270 [D loss: 0.999974] [G loss: 1.000071]\n",
      "55280 [D loss: 0.999995] [G loss: 1.000040]\n",
      "55290 [D loss: 0.999960] [G loss: 1.000064]\n",
      "55300 [D loss: 0.999978] [G loss: 1.000043]\n",
      "55310 [D loss: 0.999968] [G loss: 1.000062]\n",
      "55320 [D loss: 0.999955] [G loss: 1.000072]\n",
      "55330 [D loss: 0.999951] [G loss: 1.000072]\n",
      "55340 [D loss: 0.999973] [G loss: 1.000072]\n",
      "55350 [D loss: 0.999955] [G loss: 1.000067]\n",
      "55360 [D loss: 0.999971] [G loss: 1.000064]\n",
      "55370 [D loss: 0.999963] [G loss: 1.000078]\n",
      "55380 [D loss: 0.999965] [G loss: 1.000064]\n",
      "55390 [D loss: 0.999952] [G loss: 1.000079]\n",
      "55400 [D loss: 0.999950] [G loss: 1.000057]\n",
      "55410 [D loss: 0.999958] [G loss: 1.000081]\n",
      "55420 [D loss: 0.999963] [G loss: 1.000053]\n",
      "55430 [D loss: 0.999965] [G loss: 1.000075]\n",
      "55440 [D loss: 0.999963] [G loss: 1.000076]\n",
      "55450 [D loss: 0.999971] [G loss: 1.000053]\n",
      "55460 [D loss: 0.999951] [G loss: 1.000047]\n",
      "55470 [D loss: 0.999962] [G loss: 1.000080]\n",
      "55480 [D loss: 0.999973] [G loss: 1.000072]\n",
      "55490 [D loss: 0.999949] [G loss: 1.000092]\n",
      "55500 [D loss: 0.999978] [G loss: 1.000060]\n",
      "55510 [D loss: 0.999968] [G loss: 1.000058]\n",
      "55520 [D loss: 0.999962] [G loss: 1.000083]\n",
      "55530 [D loss: 0.999958] [G loss: 1.000083]\n",
      "55540 [D loss: 0.999966] [G loss: 1.000036]\n",
      "55550 [D loss: 0.999979] [G loss: 1.000069]\n",
      "55560 [D loss: 0.999965] [G loss: 1.000069]\n",
      "55570 [D loss: 0.999961] [G loss: 1.000075]\n",
      "55580 [D loss: 0.999988] [G loss: 1.000056]\n",
      "55590 [D loss: 0.999969] [G loss: 1.000054]\n",
      "55600 [D loss: 0.999961] [G loss: 1.000071]\n",
      "55610 [D loss: 0.999983] [G loss: 1.000045]\n",
      "55620 [D loss: 0.999959] [G loss: 1.000048]\n",
      "55630 [D loss: 0.999971] [G loss: 1.000080]\n",
      "55640 [D loss: 0.999970] [G loss: 1.000084]\n",
      "55650 [D loss: 0.999991] [G loss: 1.000045]\n",
      "55660 [D loss: 0.999965] [G loss: 1.000053]\n",
      "55670 [D loss: 0.999969] [G loss: 1.000059]\n",
      "55680 [D loss: 0.999965] [G loss: 1.000074]\n",
      "55690 [D loss: 0.999973] [G loss: 1.000042]\n",
      "55700 [D loss: 0.999959] [G loss: 1.000067]\n",
      "55710 [D loss: 0.999962] [G loss: 1.000084]\n",
      "55720 [D loss: 0.999956] [G loss: 1.000039]\n",
      "55730 [D loss: 0.999959] [G loss: 1.000076]\n",
      "55740 [D loss: 0.999963] [G loss: 1.000087]\n",
      "55750 [D loss: 1.000000] [G loss: 0.999990]\n",
      "55760 [D loss: 0.999974] [G loss: 1.000065]\n",
      "55770 [D loss: 0.999964] [G loss: 1.000098]\n",
      "55780 [D loss: 0.999985] [G loss: 1.000041]\n",
      "55790 [D loss: 0.999954] [G loss: 1.000053]\n",
      "55800 [D loss: 0.999963] [G loss: 1.000106]\n",
      "55810 [D loss: 0.999965] [G loss: 1.000026]\n",
      "55820 [D loss: 0.999963] [G loss: 1.000069]\n",
      "55830 [D loss: 0.999972] [G loss: 1.000084]\n",
      "55840 [D loss: 0.999958] [G loss: 1.000033]\n",
      "55850 [D loss: 0.999955] [G loss: 1.000093]\n",
      "55860 [D loss: 0.999945] [G loss: 1.000083]\n",
      "55870 [D loss: 1.000004] [G loss: 1.000031]\n",
      "55880 [D loss: 0.999967] [G loss: 1.000070]\n",
      "55890 [D loss: 0.999963] [G loss: 1.000084]\n",
      "55900 [D loss: 0.999978] [G loss: 1.000055]\n",
      "55910 [D loss: 0.999967] [G loss: 1.000082]\n",
      "55920 [D loss: 0.999989] [G loss: 1.000008]\n",
      "55930 [D loss: 0.999963] [G loss: 1.000064]\n",
      "55940 [D loss: 0.999970] [G loss: 1.000092]\n",
      "55950 [D loss: 0.999976] [G loss: 1.000034]\n",
      "55960 [D loss: 0.999974] [G loss: 1.000085]\n",
      "55970 [D loss: 0.999964] [G loss: 1.000110]\n",
      "55980 [D loss: 0.999988] [G loss: 1.000009]\n",
      "55990 [D loss: 0.999957] [G loss: 1.000069]\n",
      "56000 [D loss: 1.000007] [G loss: 1.000137]\n",
      "56010 [D loss: 1.000008] [G loss: 1.000034]\n",
      "56020 [D loss: 0.999957] [G loss: 1.000077]\n",
      "56030 [D loss: 0.999979] [G loss: 1.000060]\n",
      "56040 [D loss: 0.999962] [G loss: 1.000070]\n",
      "56050 [D loss: 0.999953] [G loss: 1.000091]\n",
      "56060 [D loss: 0.999977] [G loss: 1.000007]\n",
      "56070 [D loss: 0.999956] [G loss: 1.000073]\n",
      "56080 [D loss: 0.999927] [G loss: 1.000151]\n",
      "56090 [D loss: 0.999972] [G loss: 1.000058]\n",
      "56100 [D loss: 0.999962] [G loss: 1.000095]\n",
      "56110 [D loss: 0.999994] [G loss: 0.999993]\n",
      "56120 [D loss: 0.999973] [G loss: 1.000085]\n",
      "56130 [D loss: 0.999940] [G loss: 1.000066]\n",
      "56140 [D loss: 0.999973] [G loss: 1.000028]\n",
      "56150 [D loss: 0.999954] [G loss: 1.000067]\n",
      "56160 [D loss: 0.999984] [G loss: 1.000031]\n",
      "56170 [D loss: 0.999957] [G loss: 1.000082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56180 [D loss: 0.999948] [G loss: 1.000090]\n",
      "56190 [D loss: 0.999940] [G loss: 1.000041]\n",
      "56200 [D loss: 0.999985] [G loss: 1.000015]\n",
      "56210 [D loss: 0.999965] [G loss: 1.000102]\n",
      "56220 [D loss: 0.999951] [G loss: 1.000113]\n",
      "56230 [D loss: 1.000002] [G loss: 1.000006]\n",
      "56240 [D loss: 0.999957] [G loss: 1.000040]\n",
      "56250 [D loss: 0.999967] [G loss: 1.000069]\n",
      "56260 [D loss: 0.999946] [G loss: 1.000051]\n",
      "56270 [D loss: 0.999971] [G loss: 1.000057]\n",
      "56280 [D loss: 0.999958] [G loss: 1.000074]\n",
      "56290 [D loss: 0.999961] [G loss: 1.000094]\n",
      "56300 [D loss: 0.999943] [G loss: 1.000121]\n",
      "56310 [D loss: 0.999998] [G loss: 1.000025]\n",
      "56320 [D loss: 0.999963] [G loss: 1.000052]\n",
      "56330 [D loss: 0.999959] [G loss: 1.000122]\n",
      "56340 [D loss: 0.999977] [G loss: 0.999980]\n",
      "56350 [D loss: 0.999973] [G loss: 1.000049]\n",
      "56360 [D loss: 0.999948] [G loss: 1.000091]\n",
      "56370 [D loss: 0.999941] [G loss: 1.000059]\n",
      "56380 [D loss: 0.999958] [G loss: 1.000033]\n",
      "56390 [D loss: 0.999960] [G loss: 1.000088]\n",
      "56400 [D loss: 1.000035] [G loss: 0.999989]\n",
      "56410 [D loss: 0.999963] [G loss: 1.000059]\n",
      "56420 [D loss: 0.999968] [G loss: 1.000087]\n",
      "56430 [D loss: 0.999946] [G loss: 1.000018]\n",
      "56440 [D loss: 0.999970] [G loss: 1.000086]\n",
      "56450 [D loss: 0.999949] [G loss: 1.000110]\n",
      "56460 [D loss: 0.999988] [G loss: 1.000066]\n",
      "56470 [D loss: 0.999952] [G loss: 1.000050]\n",
      "56480 [D loss: 0.999948] [G loss: 1.000077]\n",
      "56490 [D loss: 0.999985] [G loss: 1.000016]\n",
      "56500 [D loss: 0.999973] [G loss: 1.000041]\n",
      "56510 [D loss: 0.999961] [G loss: 1.000059]\n",
      "56520 [D loss: 0.999956] [G loss: 1.000162]\n",
      "56530 [D loss: 1.000012] [G loss: 0.999937]\n",
      "56540 [D loss: 0.999969] [G loss: 1.000050]\n",
      "56550 [D loss: 0.999954] [G loss: 1.000101]\n",
      "56560 [D loss: 0.999958] [G loss: 1.000011]\n",
      "56570 [D loss: 0.999972] [G loss: 1.000039]\n",
      "56580 [D loss: 0.999960] [G loss: 1.000102]\n",
      "56590 [D loss: 0.999955] [G loss: 1.000032]\n",
      "56600 [D loss: 0.999957] [G loss: 1.000085]\n",
      "56610 [D loss: 0.999953] [G loss: 1.000065]\n",
      "56620 [D loss: 1.000018] [G loss: 1.000011]\n",
      "56630 [D loss: 0.999987] [G loss: 1.000023]\n",
      "56640 [D loss: 0.999963] [G loss: 1.000071]\n",
      "56650 [D loss: 0.999958] [G loss: 1.000049]\n",
      "56660 [D loss: 0.999988] [G loss: 1.000047]\n",
      "56670 [D loss: 0.999965] [G loss: 1.000085]\n",
      "56680 [D loss: 0.999958] [G loss: 1.000093]\n",
      "56690 [D loss: 0.999970] [G loss: 1.000059]\n",
      "56700 [D loss: 0.999981] [G loss: 1.000057]\n",
      "56710 [D loss: 0.999956] [G loss: 1.000087]\n",
      "56720 [D loss: 0.999968] [G loss: 1.000085]\n",
      "56730 [D loss: 0.999989] [G loss: 1.000043]\n",
      "56740 [D loss: 0.999981] [G loss: 1.000082]\n",
      "56750 [D loss: 0.999961] [G loss: 1.000139]\n",
      "56760 [D loss: 0.999992] [G loss: 1.000011]\n",
      "56770 [D loss: 0.999973] [G loss: 1.000065]\n",
      "56780 [D loss: 1.000049] [G loss: 0.999947]\n",
      "56790 [D loss: 0.999965] [G loss: 1.000081]\n",
      "56800 [D loss: 0.999935] [G loss: 1.000086]\n",
      "56810 [D loss: 0.999964] [G loss: 1.000045]\n",
      "56820 [D loss: 0.999962] [G loss: 1.000076]\n",
      "56830 [D loss: 0.999990] [G loss: 0.999988]\n",
      "56840 [D loss: 0.999974] [G loss: 1.000059]\n",
      "56850 [D loss: 1.000024] [G loss: 1.000127]\n",
      "56860 [D loss: 0.999949] [G loss: 1.000056]\n",
      "56870 [D loss: 0.999938] [G loss: 1.000090]\n",
      "56880 [D loss: 0.999997] [G loss: 1.000004]\n",
      "56890 [D loss: 0.999983] [G loss: 1.000069]\n",
      "56900 [D loss: 0.999943] [G loss: 1.000066]\n",
      "56910 [D loss: 0.999994] [G loss: 1.000072]\n",
      "56920 [D loss: 0.999965] [G loss: 1.000063]\n",
      "56930 [D loss: 0.999984] [G loss: 1.000075]\n",
      "56940 [D loss: 0.999938] [G loss: 1.000045]\n",
      "56950 [D loss: 0.999961] [G loss: 1.000117]\n",
      "56960 [D loss: 0.999984] [G loss: 1.000010]\n",
      "56970 [D loss: 0.999961] [G loss: 1.000060]\n",
      "56980 [D loss: 0.999942] [G loss: 1.000110]\n",
      "56990 [D loss: 1.000006] [G loss: 1.000023]\n",
      "57000 [D loss: 0.999958] [G loss: 1.000056]\n",
      "57010 [D loss: 0.999965] [G loss: 1.000085]\n",
      "57020 [D loss: 0.999985] [G loss: 1.000052]\n",
      "57030 [D loss: 0.999962] [G loss: 1.000064]\n",
      "57040 [D loss: 0.999965] [G loss: 1.000074]\n",
      "57050 [D loss: 0.999999] [G loss: 1.000047]\n",
      "57060 [D loss: 0.999970] [G loss: 1.000040]\n",
      "57070 [D loss: 0.999968] [G loss: 1.000083]\n",
      "57080 [D loss: 0.999985] [G loss: 0.999999]\n",
      "57090 [D loss: 0.999960] [G loss: 1.000074]\n",
      "57100 [D loss: 0.999964] [G loss: 1.000084]\n",
      "57110 [D loss: 0.999944] [G loss: 1.000078]\n",
      "57120 [D loss: 0.999990] [G loss: 1.000086]\n",
      "57130 [D loss: 0.999963] [G loss: 1.000061]\n",
      "57140 [D loss: 0.999973] [G loss: 1.000043]\n",
      "57150 [D loss: 0.999985] [G loss: 1.000110]\n",
      "57160 [D loss: 0.999980] [G loss: 1.000042]\n",
      "57170 [D loss: 0.999983] [G loss: 1.000070]\n",
      "57180 [D loss: 0.999995] [G loss: 1.000108]\n",
      "57190 [D loss: 1.000005] [G loss: 1.000086]\n",
      "57200 [D loss: 0.999946] [G loss: 1.000091]\n",
      "57210 [D loss: 1.000043] [G loss: 0.999905]\n",
      "57220 [D loss: 0.999958] [G loss: 1.000076]\n",
      "57230 [D loss: 1.000008] [G loss: 1.000040]\n",
      "57240 [D loss: 0.999957] [G loss: 1.000067]\n",
      "57250 [D loss: 0.999951] [G loss: 1.000042]\n",
      "57260 [D loss: 0.999974] [G loss: 1.000045]\n",
      "57270 [D loss: 0.999940] [G loss: 1.000100]\n",
      "57280 [D loss: 1.000063] [G loss: 0.999958]\n",
      "57290 [D loss: 0.999971] [G loss: 1.000102]\n",
      "57300 [D loss: 1.000010] [G loss: 1.000032]\n",
      "57310 [D loss: 0.999960] [G loss: 1.000042]\n",
      "57320 [D loss: 0.999947] [G loss: 1.000105]\n",
      "57330 [D loss: 1.000036] [G loss: 1.000009]\n",
      "57340 [D loss: 0.999957] [G loss: 1.000081]\n",
      "57350 [D loss: 0.999974] [G loss: 1.000142]\n",
      "57360 [D loss: 0.999973] [G loss: 1.000059]\n",
      "57370 [D loss: 0.999950] [G loss: 1.000084]\n",
      "57380 [D loss: 0.999971] [G loss: 1.000023]\n",
      "57390 [D loss: 0.999956] [G loss: 1.000080]\n",
      "57400 [D loss: 0.999961] [G loss: 1.000133]\n",
      "57410 [D loss: 1.000021] [G loss: 0.999960]\n",
      "57420 [D loss: 0.999963] [G loss: 1.000070]\n",
      "57430 [D loss: 1.000002] [G loss: 1.000173]\n",
      "57440 [D loss: 0.999985] [G loss: 1.000021]\n",
      "57450 [D loss: 0.999980] [G loss: 1.000085]\n",
      "57460 [D loss: 1.000005] [G loss: 1.000030]\n",
      "57470 [D loss: 0.999972] [G loss: 1.000055]\n",
      "57480 [D loss: 0.999945] [G loss: 1.000098]\n",
      "57490 [D loss: 0.999993] [G loss: 0.999931]\n",
      "57500 [D loss: 0.999964] [G loss: 1.000097]\n",
      "57510 [D loss: 0.999957] [G loss: 1.000125]\n",
      "57520 [D loss: 0.999942] [G loss: 0.999932]\n",
      "57530 [D loss: 0.999960] [G loss: 1.000055]\n",
      "57540 [D loss: 0.999942] [G loss: 1.000098]\n",
      "57550 [D loss: 0.999973] [G loss: 0.999972]\n",
      "57560 [D loss: 0.999982] [G loss: 1.000066]\n",
      "57570 [D loss: 0.999978] [G loss: 1.000124]\n",
      "57580 [D loss: 1.000013] [G loss: 1.000023]\n",
      "57590 [D loss: 0.999975] [G loss: 1.000083]\n",
      "57600 [D loss: 0.999949] [G loss: 1.000121]\n",
      "57610 [D loss: 1.000020] [G loss: 0.999912]\n",
      "57620 [D loss: 0.999949] [G loss: 1.000102]\n",
      "57630 [D loss: 1.000009] [G loss: 1.000004]\n",
      "57640 [D loss: 0.999966] [G loss: 1.000091]\n",
      "57650 [D loss: 0.999962] [G loss: 1.000161]\n",
      "57660 [D loss: 0.999951] [G loss: 1.000029]\n",
      "57670 [D loss: 0.999950] [G loss: 1.000122]\n",
      "57680 [D loss: 1.000044] [G loss: 0.999990]\n",
      "57690 [D loss: 0.999954] [G loss: 1.000089]\n",
      "57700 [D loss: 1.000005] [G loss: 1.000170]\n",
      "57710 [D loss: 1.000005] [G loss: 1.000030]\n",
      "57720 [D loss: 0.999963] [G loss: 1.000113]\n",
      "57730 [D loss: 1.000040] [G loss: 1.000064]\n",
      "57740 [D loss: 0.999968] [G loss: 1.000081]\n",
      "57750 [D loss: 0.999935] [G loss: 1.000241]\n",
      "57760 [D loss: 1.000042] [G loss: 0.999879]\n",
      "57770 [D loss: 0.999951] [G loss: 1.000122]\n",
      "57780 [D loss: 1.000063] [G loss: 0.999886]\n",
      "57790 [D loss: 0.999974] [G loss: 1.000089]\n",
      "57800 [D loss: 0.999948] [G loss: 1.000113]\n",
      "57810 [D loss: 0.999982] [G loss: 1.000012]\n",
      "57820 [D loss: 0.999946] [G loss: 1.000110]\n",
      "57830 [D loss: 1.000079] [G loss: 0.999883]\n",
      "57840 [D loss: 0.999965] [G loss: 1.000074]\n",
      "57850 [D loss: 0.999985] [G loss: 1.000229]\n",
      "57860 [D loss: 0.999990] [G loss: 0.999966]\n",
      "57870 [D loss: 0.999958] [G loss: 1.000148]\n",
      "57880 [D loss: 1.000027] [G loss: 0.999953]\n",
      "57890 [D loss: 0.999948] [G loss: 1.000098]\n",
      "57900 [D loss: 1.000011] [G loss: 1.000126]\n",
      "57910 [D loss: 0.999982] [G loss: 1.000067]\n",
      "57920 [D loss: 1.000024] [G loss: 1.000072]\n",
      "57930 [D loss: 0.999953] [G loss: 1.000056]\n",
      "57940 [D loss: 0.999927] [G loss: 1.000263]\n",
      "57950 [D loss: 1.000041] [G loss: 0.999831]\n",
      "57960 [D loss: 0.999936] [G loss: 1.000105]\n",
      "57970 [D loss: 1.000010] [G loss: 1.000206]\n",
      "57980 [D loss: 0.999971] [G loss: 1.000032]\n",
      "57990 [D loss: 0.999936] [G loss: 1.000120]\n",
      "58000 [D loss: 1.000056] [G loss: 0.999852]\n",
      "58010 [D loss: 0.999964] [G loss: 1.000096]\n",
      "58020 [D loss: 1.000020] [G loss: 0.999902]\n",
      "58030 [D loss: 0.999966] [G loss: 1.000106]\n",
      "58040 [D loss: 0.999979] [G loss: 1.000311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58050 [D loss: 0.999987] [G loss: 1.000011]\n",
      "58060 [D loss: 0.999976] [G loss: 1.000139]\n",
      "58070 [D loss: 1.000026] [G loss: 0.999850]\n",
      "58080 [D loss: 0.999985] [G loss: 1.000065]\n",
      "58090 [D loss: 0.999873] [G loss: 1.000197]\n",
      "58100 [D loss: 0.999968] [G loss: 1.000040]\n",
      "58110 [D loss: 0.999924] [G loss: 1.000180]\n",
      "58120 [D loss: 1.000031] [G loss: 0.999883]\n",
      "58130 [D loss: 0.999933] [G loss: 1.000101]\n",
      "58140 [D loss: 1.000042] [G loss: 1.000077]\n",
      "58150 [D loss: 0.999951] [G loss: 1.000090]\n",
      "58160 [D loss: 1.000076] [G loss: 1.000247]\n",
      "58170 [D loss: 0.999958] [G loss: 1.000043]\n",
      "58180 [D loss: 0.999956] [G loss: 1.000323]\n",
      "58190 [D loss: 0.999977] [G loss: 1.000018]\n",
      "58200 [D loss: 0.999924] [G loss: 1.000288]\n",
      "58210 [D loss: 1.000109] [G loss: 0.999650]\n",
      "58220 [D loss: 0.999957] [G loss: 1.000125]\n",
      "58230 [D loss: 1.000148] [G loss: 0.999721]\n",
      "58240 [D loss: 0.999963] [G loss: 1.000117]\n",
      "58250 [D loss: 1.000154] [G loss: 1.000028]\n",
      "58260 [D loss: 0.999963] [G loss: 1.000079]\n",
      "58270 [D loss: 1.000044] [G loss: 1.000356]\n",
      "58280 [D loss: 1.000005] [G loss: 0.999999]\n",
      "58290 [D loss: 0.999921] [G loss: 1.000159]\n",
      "58300 [D loss: 1.000046] [G loss: 0.999827]\n",
      "58310 [D loss: 0.999949] [G loss: 1.000115]\n",
      "58320 [D loss: 1.000209] [G loss: 1.000062]\n",
      "58330 [D loss: 0.999954] [G loss: 1.000125]\n",
      "58340 [D loss: 1.000128] [G loss: 1.000279]\n",
      "58350 [D loss: 0.999966] [G loss: 1.000083]\n",
      "58360 [D loss: 1.000092] [G loss: 1.000595]\n",
      "58370 [D loss: 0.999959] [G loss: 1.000052]\n",
      "58380 [D loss: 0.999877] [G loss: 1.000564]\n",
      "58390 [D loss: 0.999999] [G loss: 1.000007]\n",
      "58400 [D loss: 0.999932] [G loss: 1.000210]\n",
      "58410 [D loss: 1.000054] [G loss: 0.999739]\n",
      "58420 [D loss: 0.999946] [G loss: 1.000188]\n",
      "58430 [D loss: 1.000075] [G loss: 0.999749]\n",
      "58440 [D loss: 0.999944] [G loss: 1.000140]\n",
      "58450 [D loss: 1.000078] [G loss: 1.000103]\n",
      "58460 [D loss: 0.999954] [G loss: 1.000102]\n",
      "58470 [D loss: 0.999947] [G loss: 1.000570]\n",
      "58480 [D loss: 1.000009] [G loss: 0.999976]\n",
      "58490 [D loss: 0.999937] [G loss: 1.000196]\n",
      "58500 [D loss: 1.000186] [G loss: 0.999755]\n",
      "58510 [D loss: 0.999914] [G loss: 1.000146]\n",
      "58520 [D loss: 1.000142] [G loss: 1.000031]\n",
      "58530 [D loss: 0.999967] [G loss: 1.000125]\n",
      "58540 [D loss: 0.999923] [G loss: 1.000404]\n",
      "58550 [D loss: 1.000012] [G loss: 0.999929]\n",
      "58560 [D loss: 0.999946] [G loss: 1.000194]\n",
      "58570 [D loss: 1.000125] [G loss: 0.999686]\n",
      "58580 [D loss: 0.999924] [G loss: 1.000144]\n",
      "58590 [D loss: 1.000095] [G loss: 1.000008]\n",
      "58600 [D loss: 0.999942] [G loss: 1.000124]\n",
      "58610 [D loss: 1.000324] [G loss: 1.000269]\n",
      "58620 [D loss: 0.999972] [G loss: 1.000083]\n",
      "58630 [D loss: 1.000017] [G loss: 1.000423]\n",
      "58640 [D loss: 0.999976] [G loss: 1.000018]\n",
      "58650 [D loss: 0.999883] [G loss: 1.000447]\n",
      "58660 [D loss: 1.000033] [G loss: 0.999873]\n",
      "58670 [D loss: 0.999883] [G loss: 1.000317]\n",
      "58680 [D loss: 1.000027] [G loss: 0.999933]\n",
      "58690 [D loss: 0.999897] [G loss: 1.000231]\n",
      "58700 [D loss: 1.000212] [G loss: 0.999728]\n",
      "58710 [D loss: 0.999946] [G loss: 1.000146]\n",
      "58720 [D loss: 1.000103] [G loss: 0.999976]\n",
      "58730 [D loss: 0.999958] [G loss: 1.000054]\n",
      "58740 [D loss: 0.999911] [G loss: 1.000319]\n",
      "58750 [D loss: 1.000194] [G loss: 0.999675]\n",
      "58760 [D loss: 0.999931] [G loss: 1.000144]\n",
      "58770 [D loss: 1.000025] [G loss: 1.000141]\n",
      "58780 [D loss: 0.999969] [G loss: 1.000025]\n",
      "58790 [D loss: 0.999878] [G loss: 1.000292]\n",
      "58800 [D loss: 1.000178] [G loss: 0.999648]\n",
      "58810 [D loss: 0.999948] [G loss: 1.000140]\n",
      "58820 [D loss: 1.000026] [G loss: 1.000297]\n",
      "58830 [D loss: 0.999979] [G loss: 0.999979]\n",
      "58840 [D loss: 0.999931] [G loss: 1.000140]\n",
      "58850 [D loss: 1.000071] [G loss: 1.000056]\n",
      "58860 [D loss: 0.999957] [G loss: 1.000096]\n",
      "58870 [D loss: 0.999919] [G loss: 1.000278]\n",
      "58880 [D loss: 1.000116] [G loss: 0.999844]\n",
      "58890 [D loss: 0.999955] [G loss: 1.000101]\n",
      "58900 [D loss: 0.999929] [G loss: 1.000304]\n",
      "58910 [D loss: 0.999997] [G loss: 0.999920]\n",
      "58920 [D loss: 0.999940] [G loss: 1.000170]\n",
      "58930 [D loss: 1.000093] [G loss: 1.000086]\n",
      "58940 [D loss: 0.999950] [G loss: 1.000084]\n",
      "58950 [D loss: 0.999942] [G loss: 1.000276]\n",
      "58960 [D loss: 1.000186] [G loss: 0.999866]\n",
      "58970 [D loss: 0.999942] [G loss: 1.000091]\n",
      "58980 [D loss: 0.999891] [G loss: 1.000237]\n",
      "58990 [D loss: 1.000110] [G loss: 0.999942]\n",
      "59000 [D loss: 0.999949] [G loss: 1.000097]\n",
      "59010 [D loss: 0.999932] [G loss: 1.000092]\n",
      "59020 [D loss: 1.000044] [G loss: 1.000098]\n",
      "59030 [D loss: 1.000003] [G loss: 0.999961]\n",
      "59040 [D loss: 0.999927] [G loss: 1.000143]\n",
      "59050 [D loss: 1.000066] [G loss: 1.000073]\n",
      "59060 [D loss: 0.999958] [G loss: 1.000074]\n",
      "59070 [D loss: 0.999938] [G loss: 1.000081]\n",
      "59080 [D loss: 0.999943] [G loss: 1.000039]\n",
      "59090 [D loss: 1.000016] [G loss: 0.999922]\n",
      "59100 [D loss: 0.999962] [G loss: 1.000091]\n",
      "59110 [D loss: 0.999940] [G loss: 1.000284]\n",
      "59120 [D loss: 1.000021] [G loss: 0.999973]\n",
      "59130 [D loss: 0.999959] [G loss: 1.000063]\n",
      "59140 [D loss: 0.999935] [G loss: 1.000137]\n",
      "59150 [D loss: 1.000054] [G loss: 0.999868]\n",
      "59160 [D loss: 0.999937] [G loss: 1.000090]\n",
      "59170 [D loss: 0.999959] [G loss: 1.000153]\n",
      "59180 [D loss: 1.000209] [G loss: 0.999717]\n",
      "59190 [D loss: 0.999946] [G loss: 1.000074]\n",
      "59200 [D loss: 0.999948] [G loss: 1.000175]\n",
      "59210 [D loss: 1.000038] [G loss: 0.999958]\n",
      "59220 [D loss: 0.999939] [G loss: 1.000063]\n",
      "59230 [D loss: 0.999899] [G loss: 1.000188]\n",
      "59240 [D loss: 1.000138] [G loss: 0.999872]\n",
      "59250 [D loss: 0.999966] [G loss: 1.000073]\n",
      "59260 [D loss: 0.999924] [G loss: 1.000129]\n",
      "59270 [D loss: 1.000076] [G loss: 1.000000]\n",
      "59280 [D loss: 0.999974] [G loss: 0.999986]\n",
      "59290 [D loss: 0.999950] [G loss: 1.000115]\n",
      "59300 [D loss: 0.999929] [G loss: 1.000182]\n",
      "59310 [D loss: 0.999987] [G loss: 0.999999]\n",
      "59320 [D loss: 0.999960] [G loss: 1.000087]\n",
      "59330 [D loss: 0.999954] [G loss: 1.000197]\n",
      "59340 [D loss: 1.000045] [G loss: 0.999927]\n",
      "59350 [D loss: 0.999967] [G loss: 1.000016]\n",
      "59360 [D loss: 0.999940] [G loss: 1.000110]\n",
      "59370 [D loss: 0.999890] [G loss: 1.000109]\n",
      "59380 [D loss: 1.000084] [G loss: 0.999858]\n",
      "59390 [D loss: 0.999962] [G loss: 1.000043]\n",
      "59400 [D loss: 0.999895] [G loss: 1.000102]\n",
      "59410 [D loss: 0.999893] [G loss: 1.000074]\n",
      "59420 [D loss: 0.999994] [G loss: 1.000025]\n",
      "59430 [D loss: 0.999941] [G loss: 1.000099]\n",
      "59440 [D loss: 0.999934] [G loss: 1.000124]\n",
      "59450 [D loss: 1.000061] [G loss: 0.999665]\n",
      "59460 [D loss: 0.999951] [G loss: 1.000046]\n",
      "59470 [D loss: 0.999977] [G loss: 1.000075]\n",
      "59480 [D loss: 0.999941] [G loss: 1.000098]\n",
      "59490 [D loss: 0.999919] [G loss: 1.000013]\n",
      "59500 [D loss: 0.999923] [G loss: 1.000025]\n",
      "59510 [D loss: 0.999948] [G loss: 1.000102]\n",
      "59520 [D loss: 0.999981] [G loss: 1.000120]\n",
      "59530 [D loss: 0.999959] [G loss: 0.999999]\n",
      "59540 [D loss: 0.999953] [G loss: 1.000047]\n",
      "59550 [D loss: 0.999931] [G loss: 1.000156]\n",
      "59560 [D loss: 0.999998] [G loss: 0.999965]\n",
      "59570 [D loss: 0.999940] [G loss: 1.000115]\n",
      "59580 [D loss: 0.999943] [G loss: 1.000152]\n",
      "59590 [D loss: 1.000032] [G loss: 0.999918]\n",
      "59600 [D loss: 0.999969] [G loss: 1.000058]\n",
      "59610 [D loss: 0.999996] [G loss: 1.000171]\n",
      "59620 [D loss: 1.000012] [G loss: 1.000024]\n",
      "59630 [D loss: 0.999977] [G loss: 1.000138]\n",
      "59640 [D loss: 0.999983] [G loss: 0.999977]\n",
      "59650 [D loss: 0.999966] [G loss: 1.000094]\n",
      "59660 [D loss: 0.999914] [G loss: 1.000179]\n",
      "59670 [D loss: 1.000098] [G loss: 0.999813]\n",
      "59680 [D loss: 0.999966] [G loss: 1.000070]\n",
      "59690 [D loss: 0.999963] [G loss: 1.000124]\n",
      "59700 [D loss: 0.999954] [G loss: 0.999988]\n",
      "59710 [D loss: 0.999951] [G loss: 1.000136]\n",
      "59720 [D loss: 1.000018] [G loss: 1.000000]\n",
      "59730 [D loss: 0.999966] [G loss: 1.000023]\n",
      "59740 [D loss: 0.999921] [G loss: 1.000163]\n",
      "59750 [D loss: 1.000057] [G loss: 0.999944]\n",
      "59760 [D loss: 0.999974] [G loss: 1.000071]\n",
      "59770 [D loss: 0.999962] [G loss: 1.000126]\n",
      "59780 [D loss: 1.000079] [G loss: 0.999840]\n",
      "59790 [D loss: 0.999985] [G loss: 1.000040]\n",
      "59800 [D loss: 0.999906] [G loss: 1.000152]\n",
      "59810 [D loss: 1.000128] [G loss: 0.999845]\n",
      "59820 [D loss: 0.999931] [G loss: 1.000034]\n",
      "59830 [D loss: 0.999976] [G loss: 1.000159]\n",
      "59840 [D loss: 0.999971] [G loss: 1.000030]\n",
      "59850 [D loss: 0.999962] [G loss: 1.000112]\n",
      "59860 [D loss: 0.999928] [G loss: 1.000186]\n",
      "59870 [D loss: 1.000032] [G loss: 1.000016]\n",
      "59880 [D loss: 0.999975] [G loss: 1.000123]\n",
      "59890 [D loss: 0.999933] [G loss: 1.000354]\n",
      "59900 [D loss: 0.999955] [G loss: 0.999922]\n",
      "59910 [D loss: 0.999942] [G loss: 1.000158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59920 [D loss: 1.000094] [G loss: 1.000137]\n",
      "59930 [D loss: 0.999951] [G loss: 1.000066]\n",
      "59940 [D loss: 0.999844] [G loss: 1.000225]\n",
      "59950 [D loss: 1.000151] [G loss: 0.999800]\n",
      "59960 [D loss: 0.999940] [G loss: 1.000115]\n",
      "59970 [D loss: 1.000130] [G loss: 1.000028]\n",
      "59980 [D loss: 0.999981] [G loss: 1.000078]\n",
      "59990 [D loss: 0.999912] [G loss: 1.000194]\n",
      "60000 [D loss: 1.000028] [G loss: 0.999891]\n",
      "60010 [D loss: 0.999945] [G loss: 1.000101]\n",
      "60020 [D loss: 0.999861] [G loss: 1.000194]\n",
      "60030 [D loss: 0.999998] [G loss: 1.000054]\n",
      "60040 [D loss: 0.999943] [G loss: 1.000122]\n",
      "60050 [D loss: 0.999961] [G loss: 1.000235]\n",
      "60060 [D loss: 0.999973] [G loss: 0.999977]\n",
      "60070 [D loss: 0.999941] [G loss: 1.000132]\n",
      "60080 [D loss: 1.000090] [G loss: 1.000000]\n",
      "60090 [D loss: 0.999936] [G loss: 1.000044]\n",
      "60100 [D loss: 0.999935] [G loss: 1.000152]\n",
      "60110 [D loss: 1.000138] [G loss: 0.999969]\n",
      "60120 [D loss: 0.999965] [G loss: 1.000123]\n",
      "60130 [D loss: 0.999933] [G loss: 1.000203]\n",
      "60140 [D loss: 0.999973] [G loss: 1.000108]\n",
      "60150 [D loss: 0.999942] [G loss: 1.000143]\n",
      "60160 [D loss: 0.999945] [G loss: 0.999850]\n",
      "60170 [D loss: 0.999950] [G loss: 1.000111]\n",
      "60180 [D loss: 1.000011] [G loss: 1.000196]\n",
      "60190 [D loss: 1.000121] [G loss: 0.999973]\n",
      "60200 [D loss: 0.999979] [G loss: 1.000096]\n",
      "60210 [D loss: 0.999950] [G loss: 1.000152]\n",
      "60220 [D loss: 0.999975] [G loss: 1.000016]\n",
      "60230 [D loss: 0.999901] [G loss: 1.000069]\n",
      "60240 [D loss: 1.000004] [G loss: 1.000494]\n",
      "60250 [D loss: 0.999944] [G loss: 1.000077]\n",
      "60260 [D loss: 0.999949] [G loss: 1.000138]\n",
      "60270 [D loss: 1.000140] [G loss: 0.999700]\n",
      "60280 [D loss: 0.999929] [G loss: 1.000096]\n",
      "60290 [D loss: 1.000010] [G loss: 0.999947]\n",
      "60300 [D loss: 1.000098] [G loss: 0.999749]\n",
      "60310 [D loss: 0.999976] [G loss: 1.000139]\n",
      "60320 [D loss: 1.000062] [G loss: 1.000073]\n",
      "60330 [D loss: 1.000041] [G loss: 0.999936]\n",
      "60340 [D loss: 0.999919] [G loss: 1.000193]\n",
      "60350 [D loss: 1.000054] [G loss: 0.999897]\n",
      "60360 [D loss: 0.999949] [G loss: 1.000085]\n",
      "60370 [D loss: 0.999962] [G loss: 1.000116]\n",
      "60380 [D loss: 1.000004] [G loss: 0.999985]\n",
      "60390 [D loss: 0.999937] [G loss: 1.000059]\n",
      "60400 [D loss: 0.999913] [G loss: 1.000142]\n",
      "60410 [D loss: 1.000007] [G loss: 0.999995]\n",
      "60420 [D loss: 1.000066] [G loss: 0.999823]\n",
      "60430 [D loss: 0.999962] [G loss: 1.000105]\n",
      "60440 [D loss: 0.999871] [G loss: 1.000216]\n",
      "60450 [D loss: 1.000099] [G loss: 0.999841]\n",
      "60460 [D loss: 0.999949] [G loss: 1.000131]\n",
      "60470 [D loss: 0.999950] [G loss: 1.000146]\n",
      "60480 [D loss: 1.000121] [G loss: 0.999893]\n",
      "60490 [D loss: 0.999923] [G loss: 1.000113]\n",
      "60500 [D loss: 0.999933] [G loss: 1.000116]\n",
      "60510 [D loss: 1.000026] [G loss: 1.000259]\n",
      "60520 [D loss: 0.999974] [G loss: 1.000066]\n",
      "60530 [D loss: 0.999995] [G loss: 1.000051]\n",
      "60540 [D loss: 0.999985] [G loss: 1.000227]\n",
      "60550 [D loss: 1.000026] [G loss: 0.999991]\n",
      "60560 [D loss: 0.999933] [G loss: 1.000038]\n",
      "60570 [D loss: 0.999943] [G loss: 1.000137]\n",
      "60580 [D loss: 0.999930] [G loss: 1.000244]\n",
      "60590 [D loss: 0.999977] [G loss: 0.999999]\n",
      "60600 [D loss: 0.999978] [G loss: 1.000120]\n",
      "60610 [D loss: 1.000048] [G loss: 1.000068]\n",
      "60620 [D loss: 0.999961] [G loss: 1.000090]\n",
      "60630 [D loss: 0.999978] [G loss: 1.000197]\n",
      "60640 [D loss: 1.000063] [G loss: 0.999999]\n",
      "60650 [D loss: 1.000007] [G loss: 1.000152]\n",
      "60660 [D loss: 0.999949] [G loss: 1.000238]\n",
      "60670 [D loss: 0.999929] [G loss: 0.999929]\n",
      "60680 [D loss: 0.999984] [G loss: 1.000052]\n",
      "60690 [D loss: 0.999958] [G loss: 1.000151]\n",
      "60700 [D loss: 0.999971] [G loss: 1.000009]\n",
      "60710 [D loss: 1.000004] [G loss: 1.000095]\n",
      "60720 [D loss: 0.999965] [G loss: 1.000003]\n",
      "60730 [D loss: 0.999924] [G loss: 1.000110]\n",
      "60740 [D loss: 0.999975] [G loss: 1.000269]\n",
      "60750 [D loss: 0.999944] [G loss: 1.000081]\n",
      "60760 [D loss: 0.999935] [G loss: 1.000086]\n",
      "60770 [D loss: 1.000019] [G loss: 1.000202]\n",
      "60780 [D loss: 0.999957] [G loss: 1.000021]\n",
      "60790 [D loss: 0.999952] [G loss: 1.000149]\n",
      "60800 [D loss: 1.000128] [G loss: 0.999928]\n",
      "60810 [D loss: 0.999954] [G loss: 1.000085]\n",
      "60820 [D loss: 0.999973] [G loss: 1.000158]\n",
      "60830 [D loss: 0.999924] [G loss: 0.999935]\n",
      "60840 [D loss: 1.000013] [G loss: 1.000011]\n",
      "60850 [D loss: 0.999960] [G loss: 1.000147]\n",
      "60860 [D loss: 1.000006] [G loss: 1.000058]\n",
      "60870 [D loss: 1.000007] [G loss: 1.000004]\n",
      "60880 [D loss: 0.999952] [G loss: 1.000056]\n",
      "60890 [D loss: 0.999988] [G loss: 1.000127]\n",
      "60900 [D loss: 1.000038] [G loss: 1.000062]\n",
      "60910 [D loss: 0.999944] [G loss: 0.999997]\n",
      "60920 [D loss: 0.999937] [G loss: 1.000101]\n",
      "60930 [D loss: 0.999967] [G loss: 1.000030]\n",
      "60940 [D loss: 1.000098] [G loss: 0.999845]\n",
      "60950 [D loss: 0.999972] [G loss: 1.000141]\n",
      "60960 [D loss: 0.999842] [G loss: 1.000234]\n",
      "60970 [D loss: 0.999931] [G loss: 0.999971]\n",
      "60980 [D loss: 0.999922] [G loss: 1.000101]\n",
      "60990 [D loss: 0.999952] [G loss: 1.000062]\n",
      "61000 [D loss: 1.000049] [G loss: 1.000113]\n",
      "61010 [D loss: 0.999965] [G loss: 1.000032]\n",
      "61020 [D loss: 0.999929] [G loss: 1.000068]\n",
      "61030 [D loss: 0.999911] [G loss: 1.000323]\n",
      "61040 [D loss: 1.000012] [G loss: 0.999952]\n",
      "61050 [D loss: 0.999922] [G loss: 1.000155]\n",
      "61060 [D loss: 1.000114] [G loss: 0.999755]\n",
      "61070 [D loss: 0.999982] [G loss: 1.000107]\n",
      "61080 [D loss: 0.999991] [G loss: 1.000154]\n",
      "61090 [D loss: 1.000046] [G loss: 0.999884]\n",
      "61100 [D loss: 0.999965] [G loss: 1.000090]\n",
      "61110 [D loss: 0.999940] [G loss: 1.000126]\n",
      "61120 [D loss: 0.999984] [G loss: 1.000047]\n",
      "61130 [D loss: 0.999950] [G loss: 1.000151]\n",
      "61140 [D loss: 1.000095] [G loss: 1.000252]\n",
      "61150 [D loss: 0.999989] [G loss: 1.000028]\n",
      "61160 [D loss: 0.999880] [G loss: 1.000264]\n",
      "61170 [D loss: 0.999950] [G loss: 0.999993]\n",
      "61180 [D loss: 0.999921] [G loss: 1.000160]\n",
      "61190 [D loss: 0.999915] [G loss: 0.999865]\n",
      "61200 [D loss: 0.999963] [G loss: 1.000085]\n",
      "61210 [D loss: 0.999943] [G loss: 1.000230]\n",
      "61220 [D loss: 1.000084] [G loss: 0.999937]\n",
      "61230 [D loss: 0.999929] [G loss: 1.000071]\n",
      "61240 [D loss: 0.999973] [G loss: 1.000212]\n",
      "61250 [D loss: 1.000185] [G loss: 0.999730]\n",
      "61260 [D loss: 0.999953] [G loss: 1.000079]\n",
      "61270 [D loss: 0.999862] [G loss: 1.000137]\n",
      "61280 [D loss: 1.000141] [G loss: 0.999923]\n",
      "61290 [D loss: 0.999920] [G loss: 1.000070]\n",
      "61300 [D loss: 0.999924] [G loss: 1.000407]\n",
      "61310 [D loss: 0.999970] [G loss: 0.999957]\n",
      "61320 [D loss: 0.999953] [G loss: 1.000142]\n",
      "61330 [D loss: 0.999944] [G loss: 1.000224]\n",
      "61340 [D loss: 1.000031] [G loss: 1.000006]\n",
      "61350 [D loss: 0.999989] [G loss: 1.000014]\n",
      "61360 [D loss: 0.999916] [G loss: 1.000096]\n",
      "61370 [D loss: 0.999974] [G loss: 1.000299]\n",
      "61380 [D loss: 0.999970] [G loss: 0.999976]\n",
      "61390 [D loss: 0.999934] [G loss: 1.000161]\n",
      "61400 [D loss: 0.999772] [G loss: 1.000274]\n",
      "61410 [D loss: 0.999929] [G loss: 0.999949]\n",
      "61420 [D loss: 0.999951] [G loss: 1.000102]\n",
      "61430 [D loss: 0.999909] [G loss: 1.000164]\n",
      "61440 [D loss: 0.999978] [G loss: 1.000030]\n",
      "61450 [D loss: 0.999900] [G loss: 1.000193]\n",
      "61460 [D loss: 1.000009] [G loss: 1.000069]\n",
      "61470 [D loss: 0.999944] [G loss: 1.000034]\n",
      "61480 [D loss: 0.999945] [G loss: 1.000084]\n",
      "61490 [D loss: 1.000056] [G loss: 1.000108]\n",
      "61500 [D loss: 0.999994] [G loss: 0.999954]\n",
      "61510 [D loss: 0.999969] [G loss: 1.000139]\n",
      "61520 [D loss: 1.000137] [G loss: 1.000061]\n",
      "61530 [D loss: 0.999996] [G loss: 1.000060]\n",
      "61540 [D loss: 0.999960] [G loss: 1.000124]\n",
      "61550 [D loss: 1.000226] [G loss: 0.999979]\n",
      "61560 [D loss: 0.999935] [G loss: 1.000116]\n",
      "61570 [D loss: 0.999843] [G loss: 1.000064]\n",
      "61580 [D loss: 1.000081] [G loss: 0.999807]\n",
      "61590 [D loss: 0.999930] [G loss: 1.000139]\n",
      "61600 [D loss: 0.999961] [G loss: 1.000150]\n",
      "61610 [D loss: 1.000010] [G loss: 0.999704]\n",
      "61620 [D loss: 0.999909] [G loss: 1.000114]\n",
      "61630 [D loss: 0.999947] [G loss: 1.000172]\n",
      "61640 [D loss: 1.000062] [G loss: 1.000051]\n",
      "61650 [D loss: 0.999957] [G loss: 1.000079]\n",
      "61660 [D loss: 0.999976] [G loss: 1.000168]\n",
      "61670 [D loss: 0.999993] [G loss: 1.000216]\n",
      "61680 [D loss: 0.999962] [G loss: 1.000030]\n",
      "61690 [D loss: 0.999949] [G loss: 1.000172]\n",
      "61700 [D loss: 0.999964] [G loss: 1.000169]\n",
      "61710 [D loss: 0.999986] [G loss: 1.000149]\n",
      "61720 [D loss: 0.999963] [G loss: 1.000066]\n",
      "61730 [D loss: 0.999925] [G loss: 1.000109]\n",
      "61740 [D loss: 0.999976] [G loss: 1.000108]\n",
      "61750 [D loss: 0.999962] [G loss: 1.000143]\n",
      "61760 [D loss: 0.999992] [G loss: 0.999885]\n",
      "61770 [D loss: 0.999911] [G loss: 1.000187]\n",
      "61780 [D loss: 0.999990] [G loss: 1.000067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61790 [D loss: 0.999970] [G loss: 1.000063]\n",
      "61800 [D loss: 0.999938] [G loss: 1.000128]\n",
      "61810 [D loss: 0.999973] [G loss: 0.999974]\n",
      "61820 [D loss: 0.999994] [G loss: 1.000098]\n",
      "61830 [D loss: 1.000059] [G loss: 1.000076]\n",
      "61840 [D loss: 0.999927] [G loss: 1.000126]\n",
      "61850 [D loss: 0.999977] [G loss: 1.000031]\n",
      "61860 [D loss: 0.999902] [G loss: 1.000269]\n",
      "61870 [D loss: 0.999978] [G loss: 1.000067]\n",
      "61880 [D loss: 0.999983] [G loss: 1.000182]\n",
      "61890 [D loss: 1.000044] [G loss: 1.000255]\n",
      "61900 [D loss: 0.999956] [G loss: 1.000078]\n",
      "61910 [D loss: 0.999987] [G loss: 0.999792]\n",
      "61920 [D loss: 0.999980] [G loss: 1.000136]\n",
      "61930 [D loss: 1.000012] [G loss: 1.000113]\n",
      "61940 [D loss: 1.000012] [G loss: 1.000022]\n",
      "61950 [D loss: 0.999965] [G loss: 1.000051]\n",
      "61960 [D loss: 0.999967] [G loss: 1.000054]\n",
      "61970 [D loss: 1.000043] [G loss: 0.999973]\n",
      "61980 [D loss: 0.999907] [G loss: 1.000051]\n",
      "61990 [D loss: 0.999984] [G loss: 0.999992]\n",
      "62000 [D loss: 0.999965] [G loss: 0.999989]\n",
      "62010 [D loss: 0.999999] [G loss: 1.000073]\n",
      "62020 [D loss: 1.000050] [G loss: 0.999825]\n",
      "62030 [D loss: 0.999964] [G loss: 1.000073]\n",
      "62040 [D loss: 0.999891] [G loss: 1.000219]\n",
      "62050 [D loss: 1.000066] [G loss: 0.999880]\n",
      "62060 [D loss: 0.999963] [G loss: 1.000049]\n",
      "62070 [D loss: 0.999995] [G loss: 1.000110]\n",
      "62080 [D loss: 1.000063] [G loss: 0.999967]\n",
      "62090 [D loss: 1.000002] [G loss: 0.999994]\n",
      "62100 [D loss: 0.999942] [G loss: 0.999981]\n",
      "62110 [D loss: 0.999897] [G loss: 1.000096]\n",
      "62120 [D loss: 1.000006] [G loss: 1.000241]\n",
      "62130 [D loss: 0.999992] [G loss: 0.999939]\n",
      "62140 [D loss: 0.999946] [G loss: 1.000058]\n",
      "62150 [D loss: 0.999931] [G loss: 1.000095]\n",
      "62160 [D loss: 1.000040] [G loss: 1.000026]\n",
      "62170 [D loss: 0.999924] [G loss: 1.000100]\n",
      "62180 [D loss: 1.000002] [G loss: 1.000122]\n",
      "62190 [D loss: 0.999938] [G loss: 1.000032]\n",
      "62200 [D loss: 1.000088] [G loss: 0.999813]\n",
      "62210 [D loss: 0.999961] [G loss: 1.000080]\n",
      "62220 [D loss: 1.000064] [G loss: 1.000021]\n",
      "62230 [D loss: 0.999960] [G loss: 0.999983]\n",
      "62240 [D loss: 0.999969] [G loss: 1.000179]\n",
      "62250 [D loss: 1.000028] [G loss: 1.000007]\n",
      "62260 [D loss: 0.999950] [G loss: 1.000094]\n",
      "62270 [D loss: 0.999967] [G loss: 0.999977]\n",
      "62280 [D loss: 0.999943] [G loss: 1.000096]\n",
      "62290 [D loss: 0.999957] [G loss: 1.000061]\n",
      "62300 [D loss: 0.999963] [G loss: 1.000005]\n",
      "62310 [D loss: 0.999896] [G loss: 1.000068]\n",
      "62320 [D loss: 0.999955] [G loss: 1.000170]\n",
      "62330 [D loss: 0.999987] [G loss: 1.000019]\n",
      "62340 [D loss: 1.000008] [G loss: 1.000182]\n",
      "62350 [D loss: 0.999985] [G loss: 0.999834]\n",
      "62360 [D loss: 0.999990] [G loss: 1.000071]\n",
      "62370 [D loss: 1.000055] [G loss: 1.000244]\n",
      "62380 [D loss: 1.000032] [G loss: 1.000134]\n",
      "62390 [D loss: 0.999906] [G loss: 1.000128]\n",
      "62400 [D loss: 0.999994] [G loss: 0.999973]\n",
      "62410 [D loss: 0.999970] [G loss: 1.000071]\n",
      "62420 [D loss: 0.999986] [G loss: 0.999981]\n",
      "62430 [D loss: 0.999959] [G loss: 1.000043]\n",
      "62440 [D loss: 0.999951] [G loss: 1.000175]\n",
      "62450 [D loss: 0.999975] [G loss: 1.000012]\n",
      "62460 [D loss: 1.000021] [G loss: 1.000042]\n",
      "62470 [D loss: 0.999900] [G loss: 1.000009]\n",
      "62480 [D loss: 0.999959] [G loss: 1.000064]\n",
      "62490 [D loss: 1.000032] [G loss: 0.999992]\n",
      "62500 [D loss: 0.999975] [G loss: 1.000033]\n",
      "62510 [D loss: 1.000001] [G loss: 1.000167]\n",
      "62520 [D loss: 0.999961] [G loss: 1.000042]\n",
      "62530 [D loss: 0.999869] [G loss: 1.000006]\n",
      "62540 [D loss: 0.999923] [G loss: 1.000011]\n",
      "62550 [D loss: 0.999925] [G loss: 1.000040]\n",
      "62560 [D loss: 0.999954] [G loss: 0.999980]\n",
      "62570 [D loss: 0.999967] [G loss: 1.000091]\n",
      "62580 [D loss: 1.000011] [G loss: 1.000134]\n",
      "62590 [D loss: 1.000049] [G loss: 0.999985]\n",
      "62600 [D loss: 0.999915] [G loss: 1.000085]\n",
      "62610 [D loss: 0.999977] [G loss: 1.000008]\n",
      "62620 [D loss: 0.999920] [G loss: 0.999965]\n",
      "62630 [D loss: 0.999937] [G loss: 1.000071]\n",
      "62640 [D loss: 0.999946] [G loss: 0.999977]\n",
      "62650 [D loss: 0.999968] [G loss: 0.999981]\n",
      "62660 [D loss: 0.999992] [G loss: 1.000010]\n",
      "62670 [D loss: 0.999996] [G loss: 1.000027]\n",
      "62680 [D loss: 0.999917] [G loss: 1.000104]\n",
      "62690 [D loss: 1.000004] [G loss: 0.999982]\n",
      "62700 [D loss: 0.999945] [G loss: 1.000114]\n",
      "62710 [D loss: 0.999921] [G loss: 1.000115]\n",
      "62720 [D loss: 1.000004] [G loss: 1.000029]\n",
      "62730 [D loss: 0.999974] [G loss: 1.000070]\n",
      "62740 [D loss: 0.999965] [G loss: 0.999998]\n",
      "62750 [D loss: 0.999958] [G loss: 1.000121]\n",
      "62760 [D loss: 0.999924] [G loss: 1.000082]\n",
      "62770 [D loss: 0.999992] [G loss: 1.000090]\n",
      "62780 [D loss: 0.999955] [G loss: 1.000103]\n",
      "62790 [D loss: 0.999897] [G loss: 1.000123]\n",
      "62800 [D loss: 0.999955] [G loss: 1.000034]\n",
      "62810 [D loss: 0.999952] [G loss: 1.000221]\n",
      "62820 [D loss: 0.999972] [G loss: 0.999930]\n",
      "62830 [D loss: 0.999952] [G loss: 1.000131]\n",
      "62840 [D loss: 1.000025] [G loss: 1.000013]\n",
      "62850 [D loss: 0.999973] [G loss: 1.000115]\n",
      "62860 [D loss: 0.999951] [G loss: 1.000041]\n",
      "62870 [D loss: 0.999983] [G loss: 1.000041]\n",
      "62880 [D loss: 0.999972] [G loss: 1.000054]\n",
      "62890 [D loss: 0.999994] [G loss: 1.000044]\n",
      "62900 [D loss: 0.999987] [G loss: 1.000039]\n",
      "62910 [D loss: 0.999988] [G loss: 1.000093]\n",
      "62920 [D loss: 0.999943] [G loss: 1.000064]\n",
      "62930 [D loss: 1.000002] [G loss: 0.999895]\n",
      "62940 [D loss: 0.999971] [G loss: 1.000061]\n",
      "62950 [D loss: 0.999970] [G loss: 1.000113]\n",
      "62960 [D loss: 1.000108] [G loss: 0.999885]\n",
      "62970 [D loss: 0.999938] [G loss: 0.999994]\n",
      "62980 [D loss: 0.999939] [G loss: 1.000099]\n",
      "62990 [D loss: 0.999946] [G loss: 1.000010]\n",
      "63000 [D loss: 0.999831] [G loss: 1.000220]\n",
      "63010 [D loss: 0.999972] [G loss: 1.000101]\n",
      "63020 [D loss: 0.999989] [G loss: 1.000040]\n",
      "63030 [D loss: 0.999854] [G loss: 1.000052]\n",
      "63040 [D loss: 0.999963] [G loss: 1.000006]\n",
      "63050 [D loss: 0.999969] [G loss: 0.999979]\n",
      "63060 [D loss: 0.999919] [G loss: 0.999932]\n",
      "63070 [D loss: 0.999982] [G loss: 1.000178]\n",
      "63080 [D loss: 0.999935] [G loss: 0.999994]\n",
      "63090 [D loss: 0.999980] [G loss: 1.000026]\n",
      "63100 [D loss: 0.999951] [G loss: 1.000149]\n",
      "63110 [D loss: 0.999991] [G loss: 0.999825]\n",
      "63120 [D loss: 0.999995] [G loss: 1.000013]\n",
      "63130 [D loss: 0.999954] [G loss: 1.000148]\n",
      "63140 [D loss: 1.000013] [G loss: 0.999948]\n",
      "63150 [D loss: 0.999946] [G loss: 1.000004]\n",
      "63160 [D loss: 0.999922] [G loss: 1.000067]\n",
      "63170 [D loss: 0.999912] [G loss: 1.000080]\n",
      "63180 [D loss: 0.999983] [G loss: 1.000007]\n",
      "63190 [D loss: 0.999962] [G loss: 1.000078]\n",
      "63200 [D loss: 1.000033] [G loss: 1.000173]\n",
      "63210 [D loss: 1.000154] [G loss: 0.999866]\n",
      "63220 [D loss: 0.999992] [G loss: 1.000048]\n",
      "63230 [D loss: 0.999892] [G loss: 1.000147]\n",
      "63240 [D loss: 0.999944] [G loss: 1.000049]\n",
      "63250 [D loss: 0.999986] [G loss: 1.000078]\n",
      "63260 [D loss: 0.999938] [G loss: 1.000149]\n",
      "63270 [D loss: 0.999977] [G loss: 0.999967]\n",
      "63280 [D loss: 0.999998] [G loss: 1.000086]\n",
      "63290 [D loss: 0.999961] [G loss: 1.000081]\n",
      "63300 [D loss: 1.000035] [G loss: 0.999932]\n",
      "63310 [D loss: 1.000032] [G loss: 0.999941]\n",
      "63320 [D loss: 1.000016] [G loss: 1.000031]\n",
      "63330 [D loss: 1.000014] [G loss: 1.000163]\n",
      "63340 [D loss: 0.999970] [G loss: 1.000119]\n",
      "63350 [D loss: 0.999997] [G loss: 1.000011]\n",
      "63360 [D loss: 0.999976] [G loss: 1.000116]\n",
      "63370 [D loss: 0.999931] [G loss: 1.000086]\n",
      "63380 [D loss: 1.000008] [G loss: 0.999916]\n",
      "63390 [D loss: 0.999964] [G loss: 1.000118]\n",
      "63400 [D loss: 0.999936] [G loss: 1.000104]\n",
      "63410 [D loss: 0.999978] [G loss: 1.000049]\n",
      "63420 [D loss: 0.999968] [G loss: 1.000174]\n",
      "63430 [D loss: 1.000119] [G loss: 0.999780]\n",
      "63440 [D loss: 0.999980] [G loss: 1.000112]\n",
      "63450 [D loss: 0.999924] [G loss: 0.999986]\n",
      "63460 [D loss: 1.000001] [G loss: 1.000047]\n",
      "63470 [D loss: 1.000048] [G loss: 1.000291]\n",
      "63480 [D loss: 1.000016] [G loss: 1.000188]\n",
      "63490 [D loss: 0.999922] [G loss: 1.000160]\n",
      "63500 [D loss: 0.999931] [G loss: 0.999934]\n",
      "63510 [D loss: 0.999938] [G loss: 1.000156]\n",
      "63520 [D loss: 1.000037] [G loss: 1.000213]\n",
      "63530 [D loss: 0.999994] [G loss: 1.000067]\n",
      "63540 [D loss: 0.999979] [G loss: 1.000155]\n",
      "63550 [D loss: 1.000090] [G loss: 0.999973]\n",
      "63560 [D loss: 0.999970] [G loss: 1.000096]\n",
      "63570 [D loss: 0.999920] [G loss: 1.000145]\n",
      "63580 [D loss: 1.000015] [G loss: 0.999891]\n",
      "63590 [D loss: 0.999946] [G loss: 1.000039]\n",
      "63600 [D loss: 1.000044] [G loss: 1.000101]\n",
      "63610 [D loss: 1.000037] [G loss: 0.999984]\n",
      "63620 [D loss: 0.999966] [G loss: 1.000053]\n",
      "63630 [D loss: 0.999952] [G loss: 1.000153]\n",
      "63640 [D loss: 0.999985] [G loss: 1.000064]\n",
      "63650 [D loss: 0.999934] [G loss: 1.000126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63660 [D loss: 0.999998] [G loss: 1.000242]\n",
      "63670 [D loss: 1.000008] [G loss: 1.000066]\n",
      "63680 [D loss: 0.999976] [G loss: 1.000046]\n",
      "63690 [D loss: 0.999961] [G loss: 1.000032]\n",
      "63700 [D loss: 0.999938] [G loss: 1.000018]\n",
      "63710 [D loss: 0.999985] [G loss: 1.000191]\n",
      "63720 [D loss: 1.000148] [G loss: 0.999858]\n",
      "63730 [D loss: 0.999905] [G loss: 1.000187]\n",
      "63740 [D loss: 1.000130] [G loss: 1.000088]\n",
      "63750 [D loss: 0.999946] [G loss: 1.000090]\n",
      "63760 [D loss: 0.999903] [G loss: 1.000200]\n",
      "63770 [D loss: 0.999956] [G loss: 1.000100]\n",
      "63780 [D loss: 1.000032] [G loss: 0.999820]\n",
      "63790 [D loss: 1.000092] [G loss: 1.000031]\n",
      "63800 [D loss: 0.999912] [G loss: 1.000088]\n",
      "63810 [D loss: 0.999921] [G loss: 1.000124]\n",
      "63820 [D loss: 0.999887] [G loss: 1.000128]\n",
      "63830 [D loss: 0.999955] [G loss: 0.999841]\n",
      "63840 [D loss: 0.999902] [G loss: 1.000165]\n",
      "63850 [D loss: 0.999884] [G loss: 0.999889]\n",
      "63860 [D loss: 0.999981] [G loss: 0.999991]\n",
      "63870 [D loss: 0.999951] [G loss: 1.000107]\n",
      "63880 [D loss: 1.000070] [G loss: 0.999881]\n",
      "63890 [D loss: 0.999928] [G loss: 1.000052]\n",
      "63900 [D loss: 0.999972] [G loss: 1.000043]\n",
      "63910 [D loss: 1.000062] [G loss: 0.999747]\n",
      "63920 [D loss: 0.999966] [G loss: 1.000091]\n",
      "63930 [D loss: 0.999853] [G loss: 1.000022]\n",
      "63940 [D loss: 0.999953] [G loss: 0.999992]\n",
      "63950 [D loss: 0.999946] [G loss: 1.000218]\n",
      "63960 [D loss: 0.999952] [G loss: 0.999828]\n",
      "63970 [D loss: 0.999958] [G loss: 1.000216]\n",
      "63980 [D loss: 1.000039] [G loss: 1.000087]\n",
      "63990 [D loss: 0.999995] [G loss: 1.000103]\n",
      "64000 [D loss: 1.000031] [G loss: 1.000335]\n",
      "64010 [D loss: 0.999985] [G loss: 1.000073]\n",
      "64020 [D loss: 0.999970] [G loss: 1.000155]\n",
      "64030 [D loss: 0.999967] [G loss: 1.000087]\n",
      "64040 [D loss: 0.999928] [G loss: 1.000170]\n",
      "64050 [D loss: 0.999977] [G loss: 1.000086]\n",
      "64060 [D loss: 0.999939] [G loss: 1.000195]\n",
      "64070 [D loss: 0.999992] [G loss: 1.000076]\n",
      "64080 [D loss: 0.999930] [G loss: 1.000133]\n",
      "64090 [D loss: 0.999903] [G loss: 1.000054]\n",
      "64100 [D loss: 0.999947] [G loss: 1.000080]\n",
      "64110 [D loss: 0.999993] [G loss: 1.000029]\n",
      "64120 [D loss: 0.999962] [G loss: 1.000042]\n",
      "64130 [D loss: 0.999900] [G loss: 1.000131]\n",
      "64140 [D loss: 1.000026] [G loss: 0.999938]\n",
      "64150 [D loss: 0.999967] [G loss: 1.000163]\n",
      "64160 [D loss: 0.999960] [G loss: 0.999941]\n",
      "64170 [D loss: 1.000036] [G loss: 1.000013]\n",
      "64180 [D loss: 0.999872] [G loss: 1.000234]\n",
      "64190 [D loss: 1.000070] [G loss: 0.999980]\n",
      "64200 [D loss: 0.999938] [G loss: 1.000121]\n",
      "64210 [D loss: 0.999949] [G loss: 1.000191]\n",
      "64220 [D loss: 0.999987] [G loss: 1.000066]\n",
      "64230 [D loss: 0.999958] [G loss: 1.000069]\n",
      "64240 [D loss: 0.999974] [G loss: 0.999965]\n",
      "64250 [D loss: 0.999982] [G loss: 1.000097]\n",
      "64260 [D loss: 0.999950] [G loss: 1.000029]\n",
      "64270 [D loss: 1.000057] [G loss: 1.000012]\n",
      "64280 [D loss: 0.999964] [G loss: 1.000178]\n",
      "64290 [D loss: 1.000063] [G loss: 0.999901]\n",
      "64300 [D loss: 0.999958] [G loss: 1.000067]\n",
      "64310 [D loss: 0.999892] [G loss: 1.000086]\n",
      "64320 [D loss: 0.999949] [G loss: 1.000017]\n",
      "64330 [D loss: 0.999972] [G loss: 1.000127]\n",
      "64340 [D loss: 1.000201] [G loss: 0.999920]\n",
      "64350 [D loss: 0.999949] [G loss: 1.000067]\n",
      "64360 [D loss: 1.000053] [G loss: 1.000030]\n",
      "64370 [D loss: 0.999959] [G loss: 1.000086]\n",
      "64380 [D loss: 0.999891] [G loss: 1.000193]\n",
      "64390 [D loss: 0.999998] [G loss: 1.000072]\n",
      "64400 [D loss: 0.999957] [G loss: 1.000166]\n",
      "64410 [D loss: 0.999989] [G loss: 0.999925]\n",
      "64420 [D loss: 0.999941] [G loss: 1.000085]\n",
      "64430 [D loss: 0.999953] [G loss: 1.000151]\n",
      "64440 [D loss: 0.999993] [G loss: 1.000057]\n",
      "64450 [D loss: 0.999981] [G loss: 1.000106]\n",
      "64460 [D loss: 1.000084] [G loss: 0.999903]\n",
      "64470 [D loss: 0.999960] [G loss: 1.000101]\n",
      "64480 [D loss: 0.999892] [G loss: 1.000258]\n",
      "64490 [D loss: 1.000047] [G loss: 0.999874]\n",
      "64500 [D loss: 0.999986] [G loss: 1.000071]\n",
      "64510 [D loss: 0.999851] [G loss: 1.000201]\n",
      "64520 [D loss: 0.999992] [G loss: 0.999954]\n",
      "64530 [D loss: 0.999950] [G loss: 1.000183]\n",
      "64540 [D loss: 0.999993] [G loss: 1.000024]\n",
      "64550 [D loss: 0.999950] [G loss: 0.999956]\n",
      "64560 [D loss: 0.999968] [G loss: 1.000136]\n",
      "64570 [D loss: 0.999980] [G loss: 0.999856]\n",
      "64580 [D loss: 0.999984] [G loss: 1.000030]\n",
      "64590 [D loss: 1.000015] [G loss: 1.000076]\n",
      "64600 [D loss: 0.999967] [G loss: 1.000138]\n",
      "64610 [D loss: 0.999927] [G loss: 1.000094]\n",
      "64620 [D loss: 0.999956] [G loss: 1.000182]\n",
      "64630 [D loss: 0.999927] [G loss: 1.000083]\n",
      "64640 [D loss: 0.999942] [G loss: 1.000056]\n",
      "64650 [D loss: 0.999951] [G loss: 1.000197]\n",
      "64660 [D loss: 1.000094] [G loss: 1.000110]\n",
      "64670 [D loss: 0.999975] [G loss: 1.000108]\n",
      "64680 [D loss: 0.999935] [G loss: 0.999852]\n",
      "64690 [D loss: 0.999922] [G loss: 1.000046]\n",
      "64700 [D loss: 0.999987] [G loss: 1.000102]\n",
      "64710 [D loss: 1.000006] [G loss: 0.999844]\n",
      "64720 [D loss: 0.999962] [G loss: 1.000130]\n",
      "64730 [D loss: 1.000006] [G loss: 1.000028]\n",
      "64740 [D loss: 0.999936] [G loss: 1.000093]\n",
      "64750 [D loss: 0.999926] [G loss: 1.000052]\n",
      "64760 [D loss: 0.999981] [G loss: 0.999873]\n",
      "64770 [D loss: 0.999956] [G loss: 1.000081]\n",
      "64780 [D loss: 0.999915] [G loss: 1.000336]\n",
      "64790 [D loss: 1.000014] [G loss: 1.000061]\n",
      "64800 [D loss: 0.999976] [G loss: 1.000129]\n",
      "64810 [D loss: 0.999921] [G loss: 0.999888]\n",
      "64820 [D loss: 0.999933] [G loss: 0.999997]\n",
      "64830 [D loss: 1.000003] [G loss: 1.000231]\n",
      "64840 [D loss: 1.000071] [G loss: 0.999916]\n",
      "64850 [D loss: 0.999967] [G loss: 1.000162]\n",
      "64860 [D loss: 0.999960] [G loss: 0.999926]\n",
      "64870 [D loss: 0.999977] [G loss: 1.000156]\n",
      "64880 [D loss: 1.000081] [G loss: 1.000140]\n",
      "64890 [D loss: 0.999968] [G loss: 0.999994]\n",
      "64900 [D loss: 0.999910] [G loss: 1.000134]\n",
      "64910 [D loss: 1.000149] [G loss: 0.999717]\n",
      "64920 [D loss: 0.999945] [G loss: 1.000115]\n",
      "64930 [D loss: 1.000052] [G loss: 0.999944]\n",
      "64940 [D loss: 0.999908] [G loss: 1.000090]\n",
      "64950 [D loss: 0.999935] [G loss: 1.000125]\n",
      "64960 [D loss: 0.999955] [G loss: 1.000097]\n",
      "64970 [D loss: 0.999988] [G loss: 0.999889]\n",
      "64980 [D loss: 1.000007] [G loss: 1.000098]\n",
      "64990 [D loss: 0.999899] [G loss: 1.000150]\n",
      "65000 [D loss: 0.999970] [G loss: 1.000137]\n",
      "65010 [D loss: 0.999945] [G loss: 1.000117]\n",
      "65020 [D loss: 0.999998] [G loss: 1.000035]\n",
      "65030 [D loss: 0.999953] [G loss: 1.000149]\n",
      "65040 [D loss: 1.000082] [G loss: 0.999853]\n",
      "65050 [D loss: 0.999911] [G loss: 1.000114]\n",
      "65060 [D loss: 0.999990] [G loss: 0.999864]\n",
      "65070 [D loss: 0.999921] [G loss: 1.000037]\n",
      "65080 [D loss: 0.999902] [G loss: 1.000304]\n",
      "65090 [D loss: 0.999938] [G loss: 1.000071]\n",
      "65100 [D loss: 0.999874] [G loss: 1.000073]\n",
      "65110 [D loss: 1.000035] [G loss: 1.000150]\n",
      "65120 [D loss: 0.999975] [G loss: 1.000054]\n",
      "65130 [D loss: 1.000009] [G loss: 0.999995]\n",
      "65140 [D loss: 0.999962] [G loss: 1.000060]\n",
      "65150 [D loss: 0.999989] [G loss: 1.000135]\n",
      "65160 [D loss: 0.999997] [G loss: 1.000155]\n",
      "65170 [D loss: 1.000060] [G loss: 1.000211]\n",
      "65180 [D loss: 1.000007] [G loss: 1.000053]\n",
      "65190 [D loss: 0.999961] [G loss: 1.000151]\n",
      "65200 [D loss: 0.999979] [G loss: 1.000029]\n",
      "65210 [D loss: 0.999912] [G loss: 1.000128]\n",
      "65220 [D loss: 0.999959] [G loss: 1.000050]\n",
      "65230 [D loss: 0.999990] [G loss: 1.000127]\n",
      "65240 [D loss: 0.999836] [G loss: 0.999894]\n",
      "65250 [D loss: 0.999965] [G loss: 1.000012]\n",
      "65260 [D loss: 0.999943] [G loss: 1.000140]\n",
      "65270 [D loss: 0.999964] [G loss: 1.000001]\n",
      "65280 [D loss: 0.999979] [G loss: 1.000116]\n",
      "65290 [D loss: 1.000120] [G loss: 0.999804]\n",
      "65300 [D loss: 0.999965] [G loss: 1.000132]\n",
      "65310 [D loss: 1.000073] [G loss: 0.999613]\n",
      "65320 [D loss: 0.999959] [G loss: 1.000054]\n",
      "65330 [D loss: 0.999932] [G loss: 1.000062]\n",
      "65340 [D loss: 0.999973] [G loss: 1.000102]\n",
      "65350 [D loss: 1.000019] [G loss: 0.999946]\n",
      "65360 [D loss: 0.999956] [G loss: 1.000038]\n",
      "65370 [D loss: 0.999902] [G loss: 1.000227]\n",
      "65380 [D loss: 1.000024] [G loss: 0.999998]\n",
      "65390 [D loss: 0.999945] [G loss: 1.000073]\n",
      "65400 [D loss: 0.999954] [G loss: 0.999865]\n",
      "65410 [D loss: 0.999950] [G loss: 1.000112]\n",
      "65420 [D loss: 0.999911] [G loss: 1.000118]\n",
      "65430 [D loss: 1.000059] [G loss: 0.999787]\n",
      "65440 [D loss: 0.999951] [G loss: 1.000152]\n",
      "65450 [D loss: 1.000047] [G loss: 0.999854]\n",
      "65460 [D loss: 0.999988] [G loss: 1.000010]\n",
      "65470 [D loss: 0.999937] [G loss: 1.000074]\n",
      "65480 [D loss: 1.000050] [G loss: 1.000061]\n",
      "65490 [D loss: 0.999992] [G loss: 1.000102]\n",
      "65500 [D loss: 0.999857] [G loss: 1.000226]\n",
      "65510 [D loss: 0.999966] [G loss: 0.999991]\n",
      "65520 [D loss: 0.999949] [G loss: 1.000082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65530 [D loss: 1.000011] [G loss: 0.999936]\n",
      "65540 [D loss: 0.999972] [G loss: 1.000046]\n",
      "65550 [D loss: 0.999934] [G loss: 1.000181]\n",
      "65560 [D loss: 1.000002] [G loss: 1.000021]\n",
      "65570 [D loss: 0.999936] [G loss: 1.000057]\n",
      "65580 [D loss: 0.999954] [G loss: 1.000039]\n",
      "65590 [D loss: 1.000045] [G loss: 0.999948]\n",
      "65600 [D loss: 0.999948] [G loss: 1.000135]\n",
      "65610 [D loss: 0.999964] [G loss: 1.000043]\n",
      "65620 [D loss: 0.999954] [G loss: 0.999931]\n",
      "65630 [D loss: 0.999940] [G loss: 1.000097]\n",
      "65640 [D loss: 1.000028] [G loss: 1.000103]\n",
      "65650 [D loss: 1.000035] [G loss: 1.000049]\n",
      "65660 [D loss: 0.999982] [G loss: 1.000107]\n",
      "65670 [D loss: 0.999956] [G loss: 1.000091]\n",
      "65680 [D loss: 0.999914] [G loss: 1.000006]\n",
      "65690 [D loss: 0.999969] [G loss: 1.000180]\n",
      "65700 [D loss: 1.000022] [G loss: 0.999837]\n",
      "65710 [D loss: 0.999980] [G loss: 1.000120]\n",
      "65720 [D loss: 0.999943] [G loss: 1.000099]\n",
      "65730 [D loss: 1.000137] [G loss: 0.999974]\n",
      "65740 [D loss: 0.999928] [G loss: 0.999946]\n",
      "65750 [D loss: 0.999961] [G loss: 1.000144]\n",
      "65760 [D loss: 0.999907] [G loss: 0.999991]\n",
      "65770 [D loss: 0.999972] [G loss: 0.999884]\n",
      "65780 [D loss: 0.999930] [G loss: 1.000131]\n",
      "65790 [D loss: 1.000168] [G loss: 1.000201]\n",
      "65800 [D loss: 1.000049] [G loss: 0.999969]\n",
      "65810 [D loss: 0.999951] [G loss: 1.000170]\n",
      "65820 [D loss: 1.000073] [G loss: 0.999752]\n",
      "65830 [D loss: 0.999998] [G loss: 1.000051]\n",
      "65840 [D loss: 0.999956] [G loss: 1.000058]\n",
      "65850 [D loss: 1.000093] [G loss: 1.000004]\n",
      "65860 [D loss: 0.999993] [G loss: 1.000017]\n",
      "65870 [D loss: 0.999929] [G loss: 1.000188]\n",
      "65880 [D loss: 1.000082] [G loss: 0.999753]\n",
      "65890 [D loss: 0.999945] [G loss: 1.000112]\n",
      "65900 [D loss: 0.999910] [G loss: 1.000269]\n",
      "65910 [D loss: 0.999987] [G loss: 1.000033]\n",
      "65920 [D loss: 0.999962] [G loss: 1.000148]\n",
      "65930 [D loss: 0.999988] [G loss: 1.000098]\n",
      "65940 [D loss: 0.999958] [G loss: 1.000002]\n",
      "65950 [D loss: 0.999911] [G loss: 1.000154]\n",
      "65960 [D loss: 0.999989] [G loss: 1.000025]\n",
      "65970 [D loss: 0.999925] [G loss: 1.000090]\n",
      "65980 [D loss: 0.999934] [G loss: 1.000028]\n",
      "65990 [D loss: 0.999990] [G loss: 0.999957]\n",
      "66000 [D loss: 0.999921] [G loss: 1.000132]\n",
      "66010 [D loss: 1.000056] [G loss: 1.000139]\n",
      "66020 [D loss: 0.999984] [G loss: 1.000103]\n",
      "66030 [D loss: 0.999927] [G loss: 1.000123]\n",
      "66040 [D loss: 1.000016] [G loss: 1.000028]\n",
      "66050 [D loss: 0.999998] [G loss: 1.000048]\n",
      "66060 [D loss: 1.000022] [G loss: 1.000094]\n",
      "66070 [D loss: 0.999932] [G loss: 0.999948]\n",
      "66080 [D loss: 0.999953] [G loss: 1.000132]\n",
      "66090 [D loss: 0.999916] [G loss: 1.000176]\n",
      "66100 [D loss: 0.999997] [G loss: 1.000037]\n",
      "66110 [D loss: 0.999928] [G loss: 1.000108]\n",
      "66120 [D loss: 1.000032] [G loss: 0.999944]\n",
      "66130 [D loss: 0.999973] [G loss: 1.000066]\n",
      "66140 [D loss: 0.999959] [G loss: 1.000012]\n",
      "66150 [D loss: 1.000061] [G loss: 0.999824]\n",
      "66160 [D loss: 0.999930] [G loss: 1.000130]\n",
      "66170 [D loss: 1.000026] [G loss: 0.999900]\n",
      "66180 [D loss: 0.999935] [G loss: 1.000093]\n",
      "66190 [D loss: 0.999895] [G loss: 1.000362]\n",
      "66200 [D loss: 0.999948] [G loss: 1.000030]\n",
      "66210 [D loss: 0.999880] [G loss: 1.000370]\n",
      "66220 [D loss: 1.000206] [G loss: 0.999488]\n",
      "66230 [D loss: 0.999944] [G loss: 1.000144]\n",
      "66240 [D loss: 1.000075] [G loss: 0.999885]\n",
      "66250 [D loss: 0.999964] [G loss: 1.000072]\n",
      "66260 [D loss: 1.000050] [G loss: 1.000122]\n",
      "66270 [D loss: 0.999986] [G loss: 1.000086]\n",
      "66280 [D loss: 0.999955] [G loss: 1.000152]\n",
      "66290 [D loss: 0.999987] [G loss: 1.000067]\n",
      "66300 [D loss: 0.999968] [G loss: 1.000065]\n",
      "66310 [D loss: 0.999993] [G loss: 1.000226]\n",
      "66320 [D loss: 0.999984] [G loss: 0.999968]\n",
      "66330 [D loss: 1.000014] [G loss: 1.000146]\n",
      "66340 [D loss: 1.000013] [G loss: 1.000029]\n",
      "66350 [D loss: 0.999962] [G loss: 1.000042]\n",
      "66360 [D loss: 0.999967] [G loss: 1.000136]\n",
      "66370 [D loss: 0.999954] [G loss: 0.999810]\n",
      "66380 [D loss: 0.999969] [G loss: 1.000051]\n",
      "66390 [D loss: 0.999894] [G loss: 1.000187]\n",
      "66400 [D loss: 0.999961] [G loss: 0.999984]\n",
      "66410 [D loss: 0.999973] [G loss: 1.000135]\n",
      "66420 [D loss: 1.000004] [G loss: 1.000054]\n",
      "66430 [D loss: 0.999945] [G loss: 1.000069]\n",
      "66440 [D loss: 0.999940] [G loss: 1.000123]\n",
      "66450 [D loss: 0.999971] [G loss: 1.000128]\n",
      "66460 [D loss: 0.999958] [G loss: 1.000096]\n",
      "66470 [D loss: 0.999965] [G loss: 1.000191]\n",
      "66480 [D loss: 1.000084] [G loss: 0.999925]\n",
      "66490 [D loss: 0.999931] [G loss: 1.000096]\n",
      "66500 [D loss: 1.000090] [G loss: 0.999939]\n",
      "66510 [D loss: 0.999973] [G loss: 1.000075]\n",
      "66520 [D loss: 0.999844] [G loss: 1.000161]\n",
      "66530 [D loss: 0.999968] [G loss: 0.999923]\n",
      "66540 [D loss: 0.999909] [G loss: 1.000191]\n",
      "66550 [D loss: 1.000168] [G loss: 0.999715]\n",
      "66560 [D loss: 0.999953] [G loss: 1.000125]\n",
      "66570 [D loss: 1.000100] [G loss: 0.999923]\n",
      "66580 [D loss: 0.999974] [G loss: 1.000104]\n",
      "66590 [D loss: 0.999942] [G loss: 1.000215]\n",
      "66600 [D loss: 0.999953] [G loss: 1.000093]\n",
      "66610 [D loss: 0.999925] [G loss: 1.000159]\n",
      "66620 [D loss: 1.000024] [G loss: 0.999892]\n",
      "66630 [D loss: 0.999967] [G loss: 1.000116]\n",
      "66640 [D loss: 0.999951] [G loss: 1.000066]\n",
      "66650 [D loss: 1.000023] [G loss: 0.999947]\n",
      "66660 [D loss: 0.999931] [G loss: 1.000134]\n",
      "66670 [D loss: 1.000069] [G loss: 0.999731]\n",
      "66680 [D loss: 0.999940] [G loss: 1.000137]\n",
      "66690 [D loss: 1.000002] [G loss: 1.000172]\n",
      "66700 [D loss: 0.999937] [G loss: 1.000061]\n",
      "66710 [D loss: 0.999876] [G loss: 1.000288]\n",
      "66720 [D loss: 1.000055] [G loss: 0.999747]\n",
      "66730 [D loss: 0.999940] [G loss: 1.000109]\n",
      "66740 [D loss: 1.000111] [G loss: 0.999975]\n",
      "66750 [D loss: 0.999951] [G loss: 1.000088]\n",
      "66760 [D loss: 0.999967] [G loss: 1.000313]\n",
      "66770 [D loss: 0.999967] [G loss: 1.000004]\n",
      "66780 [D loss: 0.999889] [G loss: 1.000179]\n",
      "66790 [D loss: 1.000086] [G loss: 0.999614]\n",
      "66800 [D loss: 0.999948] [G loss: 1.000099]\n",
      "66810 [D loss: 1.000045] [G loss: 1.000031]\n",
      "66820 [D loss: 0.999980] [G loss: 1.000094]\n",
      "66830 [D loss: 0.999934] [G loss: 1.000227]\n",
      "66840 [D loss: 1.000051] [G loss: 0.999906]\n",
      "66850 [D loss: 0.999939] [G loss: 1.000167]\n",
      "66860 [D loss: 0.999918] [G loss: 1.000272]\n",
      "66870 [D loss: 0.999997] [G loss: 0.999963]\n",
      "66880 [D loss: 0.999902] [G loss: 1.000177]\n",
      "66890 [D loss: 1.000027] [G loss: 0.999811]\n",
      "66900 [D loss: 0.999957] [G loss: 1.000137]\n",
      "66910 [D loss: 1.000041] [G loss: 1.000047]\n",
      "66920 [D loss: 0.999991] [G loss: 1.000023]\n",
      "66930 [D loss: 0.999932] [G loss: 1.000203]\n",
      "66940 [D loss: 1.000058] [G loss: 1.000020]\n",
      "66950 [D loss: 0.999972] [G loss: 1.000174]\n",
      "66960 [D loss: 0.999961] [G loss: 1.000337]\n",
      "66970 [D loss: 0.999999] [G loss: 1.000040]\n",
      "66980 [D loss: 0.999931] [G loss: 1.000126]\n",
      "66990 [D loss: 1.000001] [G loss: 1.000052]\n",
      "67000 [D loss: 0.999922] [G loss: 1.000143]\n",
      "67010 [D loss: 1.000003] [G loss: 0.999960]\n",
      "67020 [D loss: 0.999948] [G loss: 1.000037]\n",
      "67030 [D loss: 0.999981] [G loss: 1.000271]\n",
      "67040 [D loss: 0.999958] [G loss: 0.999932]\n",
      "67050 [D loss: 0.999948] [G loss: 1.000111]\n",
      "67060 [D loss: 0.999999] [G loss: 1.000357]\n",
      "67070 [D loss: 0.999986] [G loss: 1.000028]\n",
      "67080 [D loss: 0.999916] [G loss: 1.000102]\n",
      "67090 [D loss: 1.000032] [G loss: 0.999980]\n",
      "67100 [D loss: 0.999948] [G loss: 1.000057]\n",
      "67110 [D loss: 0.999959] [G loss: 1.000332]\n",
      "67120 [D loss: 1.000114] [G loss: 0.999692]\n",
      "67130 [D loss: 0.999983] [G loss: 1.000181]\n",
      "67140 [D loss: 1.000024] [G loss: 1.000054]\n",
      "67150 [D loss: 0.999951] [G loss: 1.000136]\n",
      "67160 [D loss: 0.999861] [G loss: 1.000209]\n",
      "67170 [D loss: 0.999990] [G loss: 1.000089]\n",
      "67180 [D loss: 0.999919] [G loss: 1.000161]\n",
      "67190 [D loss: 1.000006] [G loss: 0.999978]\n",
      "67200 [D loss: 0.999980] [G loss: 1.000079]\n",
      "67210 [D loss: 0.999972] [G loss: 1.000289]\n",
      "67220 [D loss: 1.000047] [G loss: 0.999959]\n",
      "67230 [D loss: 0.999943] [G loss: 1.000086]\n",
      "67240 [D loss: 0.999927] [G loss: 1.000210]\n",
      "67250 [D loss: 0.999964] [G loss: 1.000257]\n",
      "67260 [D loss: 0.999984] [G loss: 0.999996]\n",
      "67270 [D loss: 0.999973] [G loss: 1.000160]\n",
      "67280 [D loss: 0.999874] [G loss: 1.000162]\n",
      "67290 [D loss: 0.999969] [G loss: 1.000050]\n",
      "67300 [D loss: 0.999946] [G loss: 1.000082]\n",
      "67310 [D loss: 0.999930] [G loss: 1.000156]\n",
      "67320 [D loss: 1.000187] [G loss: 0.999870]\n",
      "67330 [D loss: 0.999998] [G loss: 1.000001]\n",
      "67340 [D loss: 0.999937] [G loss: 1.000198]\n",
      "67350 [D loss: 1.000015] [G loss: 1.000063]\n",
      "67360 [D loss: 1.000067] [G loss: 0.999834]\n",
      "67370 [D loss: 0.999974] [G loss: 1.000069]\n",
      "67380 [D loss: 0.999874] [G loss: 1.000153]\n",
      "67390 [D loss: 0.999976] [G loss: 1.000012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67400 [D loss: 0.999929] [G loss: 1.000035]\n",
      "67410 [D loss: 0.999975] [G loss: 1.000118]\n",
      "67420 [D loss: 0.999948] [G loss: 1.000307]\n",
      "67430 [D loss: 0.999949] [G loss: 1.000075]\n",
      "67440 [D loss: 0.999923] [G loss: 1.000095]\n",
      "67450 [D loss: 0.999999] [G loss: 1.000135]\n",
      "67460 [D loss: 1.000068] [G loss: 0.999935]\n",
      "67470 [D loss: 0.999972] [G loss: 1.000106]\n",
      "67480 [D loss: 0.999895] [G loss: 1.000177]\n",
      "67490 [D loss: 1.000001] [G loss: 0.999907]\n",
      "67500 [D loss: 0.999952] [G loss: 1.000046]\n",
      "67510 [D loss: 0.999971] [G loss: 1.000124]\n",
      "67520 [D loss: 1.000021] [G loss: 1.000129]\n",
      "67530 [D loss: 1.000099] [G loss: 1.000036]\n",
      "67540 [D loss: 0.999962] [G loss: 1.000111]\n",
      "67550 [D loss: 0.999960] [G loss: 1.000195]\n",
      "67560 [D loss: 0.999997] [G loss: 1.000059]\n",
      "67570 [D loss: 0.999996] [G loss: 1.000021]\n",
      "67580 [D loss: 0.999938] [G loss: 1.000092]\n",
      "67590 [D loss: 1.000039] [G loss: 1.000138]\n",
      "67600 [D loss: 1.000013] [G loss: 0.999948]\n",
      "67610 [D loss: 0.999890] [G loss: 1.000186]\n",
      "67620 [D loss: 1.000206] [G loss: 0.999836]\n",
      "67630 [D loss: 0.999950] [G loss: 1.000093]\n",
      "67640 [D loss: 0.999935] [G loss: 1.000270]\n",
      "67650 [D loss: 0.999972] [G loss: 1.000006]\n",
      "67660 [D loss: 0.999932] [G loss: 1.000179]\n",
      "67670 [D loss: 1.000094] [G loss: 0.999981]\n",
      "67680 [D loss: 0.999964] [G loss: 1.000089]\n",
      "67690 [D loss: 1.000064] [G loss: 1.000157]\n",
      "67700 [D loss: 1.000036] [G loss: 0.999884]\n",
      "67710 [D loss: 0.999906] [G loss: 1.000130]\n",
      "67720 [D loss: 1.000105] [G loss: 1.000194]\n",
      "67730 [D loss: 0.999966] [G loss: 1.000046]\n",
      "67740 [D loss: 0.999962] [G loss: 1.000174]\n",
      "67750 [D loss: 1.000230] [G loss: 0.999872]\n",
      "67760 [D loss: 0.999960] [G loss: 1.000092]\n",
      "67770 [D loss: 1.000152] [G loss: 1.000183]\n",
      "67780 [D loss: 1.000162] [G loss: 0.999687]\n",
      "67790 [D loss: 0.999937] [G loss: 1.000147]\n",
      "67800 [D loss: 1.000136] [G loss: 0.999976]\n",
      "67810 [D loss: 0.999992] [G loss: 1.000084]\n",
      "67820 [D loss: 1.000016] [G loss: 1.000303]\n",
      "67830 [D loss: 0.999990] [G loss: 1.000089]\n",
      "67840 [D loss: 0.999916] [G loss: 1.000496]\n",
      "67850 [D loss: 1.000209] [G loss: 0.999989]\n",
      "67860 [D loss: 0.999932] [G loss: 1.000317]\n",
      "67870 [D loss: 1.000269] [G loss: 0.999594]\n",
      "67880 [D loss: 0.999969] [G loss: 1.000143]\n",
      "67890 [D loss: 0.999995] [G loss: 1.000199]\n",
      "67900 [D loss: 0.999943] [G loss: 1.000211]\n",
      "67910 [D loss: 1.000030] [G loss: 1.000115]\n",
      "67920 [D loss: 0.999979] [G loss: 1.000067]\n",
      "67930 [D loss: 0.999921] [G loss: 1.000377]\n",
      "67940 [D loss: 0.999983] [G loss: 1.000091]\n",
      "67950 [D loss: 0.999896] [G loss: 1.000128]\n",
      "67960 [D loss: 0.999858] [G loss: 1.000138]\n",
      "67970 [D loss: 0.999958] [G loss: 1.000134]\n",
      "67980 [D loss: 0.999949] [G loss: 1.000146]\n",
      "67990 [D loss: 0.999989] [G loss: 0.999972]\n",
      "68000 [D loss: 0.999950] [G loss: 1.000027]\n",
      "68010 [D loss: 1.000029] [G loss: 1.000105]\n",
      "68020 [D loss: 1.000002] [G loss: 1.000259]\n",
      "68030 [D loss: 1.000008] [G loss: 0.999982]\n",
      "68040 [D loss: 0.999945] [G loss: 1.000158]\n",
      "68050 [D loss: 0.999947] [G loss: 1.000058]\n",
      "68060 [D loss: 0.999969] [G loss: 1.000069]\n",
      "68070 [D loss: 0.999949] [G loss: 1.000101]\n",
      "68080 [D loss: 1.000050] [G loss: 1.000244]\n",
      "68090 [D loss: 1.000033] [G loss: 1.000002]\n",
      "68100 [D loss: 0.999963] [G loss: 1.000075]\n",
      "68110 [D loss: 0.999928] [G loss: 1.000083]\n",
      "68120 [D loss: 0.999971] [G loss: 1.000184]\n",
      "68130 [D loss: 1.000058] [G loss: 1.000182]\n",
      "68140 [D loss: 1.000029] [G loss: 1.000006]\n",
      "68150 [D loss: 0.999936] [G loss: 1.000112]\n",
      "68160 [D loss: 0.999970] [G loss: 1.000300]\n",
      "68170 [D loss: 1.000038] [G loss: 1.000032]\n",
      "68180 [D loss: 1.000015] [G loss: 1.000044]\n",
      "68190 [D loss: 0.999926] [G loss: 1.000068]\n",
      "68200 [D loss: 0.999958] [G loss: 1.000149]\n",
      "68210 [D loss: 0.999956] [G loss: 0.999975]\n",
      "68220 [D loss: 0.999949] [G loss: 1.000077]\n",
      "68230 [D loss: 0.999963] [G loss: 1.000070]\n",
      "68240 [D loss: 0.999953] [G loss: 1.000095]\n",
      "68250 [D loss: 0.999980] [G loss: 1.000257]\n",
      "68260 [D loss: 0.999928] [G loss: 0.999989]\n",
      "68270 [D loss: 1.000001] [G loss: 0.999857]\n",
      "68280 [D loss: 0.999935] [G loss: 0.999963]\n",
      "68290 [D loss: 0.999931] [G loss: 1.000146]\n",
      "68300 [D loss: 1.000172] [G loss: 1.000030]\n",
      "68310 [D loss: 0.999962] [G loss: 0.999994]\n",
      "68320 [D loss: 0.999893] [G loss: 1.000111]\n",
      "68330 [D loss: 0.999911] [G loss: 1.000048]\n",
      "68340 [D loss: 0.999949] [G loss: 1.000136]\n",
      "68350 [D loss: 1.000023] [G loss: 1.000169]\n",
      "68360 [D loss: 0.999953] [G loss: 1.000051]\n",
      "68370 [D loss: 0.999896] [G loss: 1.000306]\n",
      "68380 [D loss: 0.999925] [G loss: 0.999947]\n",
      "68390 [D loss: 0.999950] [G loss: 1.000080]\n",
      "68400 [D loss: 0.999907] [G loss: 1.000088]\n",
      "68410 [D loss: 1.000054] [G loss: 1.000149]\n",
      "68420 [D loss: 1.000007] [G loss: 0.999992]\n",
      "68430 [D loss: 0.999915] [G loss: 1.000184]\n",
      "68440 [D loss: 0.999956] [G loss: 1.000090]\n",
      "68450 [D loss: 0.999976] [G loss: 1.000026]\n",
      "68460 [D loss: 0.999970] [G loss: 1.000101]\n",
      "68470 [D loss: 0.999968] [G loss: 1.000185]\n",
      "68480 [D loss: 1.000053] [G loss: 0.999996]\n",
      "68490 [D loss: 0.999968] [G loss: 1.000095]\n",
      "68500 [D loss: 0.999837] [G loss: 1.000266]\n",
      "68510 [D loss: 1.000067] [G loss: 0.999922]\n",
      "68520 [D loss: 0.999928] [G loss: 1.000075]\n",
      "68530 [D loss: 0.999861] [G loss: 1.000160]\n",
      "68540 [D loss: 1.000015] [G loss: 1.000025]\n",
      "68550 [D loss: 0.999943] [G loss: 1.000101]\n",
      "68560 [D loss: 0.999951] [G loss: 1.000168]\n",
      "68570 [D loss: 1.000101] [G loss: 1.000111]\n",
      "68580 [D loss: 0.999956] [G loss: 1.000033]\n",
      "68590 [D loss: 0.999904] [G loss: 1.000246]\n",
      "68600 [D loss: 1.000096] [G loss: 0.999933]\n",
      "68610 [D loss: 0.999966] [G loss: 1.000147]\n",
      "68620 [D loss: 0.999861] [G loss: 1.000304]\n",
      "68630 [D loss: 0.999995] [G loss: 1.000092]\n",
      "68640 [D loss: 0.999946] [G loss: 1.000122]\n",
      "68650 [D loss: 0.999940] [G loss: 1.000218]\n",
      "68660 [D loss: 0.999963] [G loss: 1.000012]\n",
      "68670 [D loss: 0.999967] [G loss: 1.000099]\n",
      "68680 [D loss: 1.000034] [G loss: 1.000228]\n",
      "68690 [D loss: 0.999998] [G loss: 1.000108]\n",
      "68700 [D loss: 0.999961] [G loss: 1.000221]\n",
      "68710 [D loss: 0.999985] [G loss: 1.000247]\n",
      "68720 [D loss: 1.000008] [G loss: 0.999964]\n",
      "68730 [D loss: 0.999940] [G loss: 1.000102]\n",
      "68740 [D loss: 0.999981] [G loss: 1.000110]\n",
      "68750 [D loss: 1.000005] [G loss: 1.000079]\n",
      "68760 [D loss: 0.999983] [G loss: 1.000146]\n",
      "68770 [D loss: 0.999974] [G loss: 1.000171]\n",
      "68780 [D loss: 1.000040] [G loss: 0.999934]\n",
      "68790 [D loss: 0.999975] [G loss: 1.000020]\n",
      "68800 [D loss: 0.999960] [G loss: 1.000221]\n",
      "68810 [D loss: 0.999952] [G loss: 0.999871]\n",
      "68820 [D loss: 0.999955] [G loss: 1.000093]\n",
      "68830 [D loss: 1.000012] [G loss: 1.000114]\n",
      "68840 [D loss: 1.000006] [G loss: 0.999938]\n",
      "68850 [D loss: 0.999928] [G loss: 1.000149]\n",
      "68860 [D loss: 0.999982] [G loss: 0.999988]\n",
      "68870 [D loss: 0.999980] [G loss: 1.000041]\n",
      "68880 [D loss: 0.999931] [G loss: 1.000097]\n",
      "68890 [D loss: 0.999962] [G loss: 1.000158]\n",
      "68900 [D loss: 0.999948] [G loss: 1.000146]\n",
      "68910 [D loss: 1.000023] [G loss: 1.000129]\n",
      "68920 [D loss: 0.999962] [G loss: 1.000019]\n",
      "68930 [D loss: 0.999950] [G loss: 1.000099]\n",
      "68940 [D loss: 0.999884] [G loss: 1.000066]\n",
      "68950 [D loss: 0.999978] [G loss: 1.000111]\n",
      "68960 [D loss: 0.999937] [G loss: 1.000223]\n",
      "68970 [D loss: 0.999982] [G loss: 1.000065]\n",
      "68980 [D loss: 0.999962] [G loss: 1.000088]\n",
      "68990 [D loss: 0.999903] [G loss: 1.000174]\n",
      "69000 [D loss: 1.000059] [G loss: 0.999903]\n",
      "69010 [D loss: 0.999948] [G loss: 1.000127]\n",
      "69020 [D loss: 0.999994] [G loss: 1.000054]\n",
      "69030 [D loss: 1.000006] [G loss: 1.000050]\n",
      "69040 [D loss: 0.999908] [G loss: 1.000237]\n",
      "69050 [D loss: 1.000013] [G loss: 0.999811]\n",
      "69060 [D loss: 0.999935] [G loss: 1.000079]\n",
      "69070 [D loss: 0.999922] [G loss: 1.000195]\n",
      "69080 [D loss: 0.999996] [G loss: 0.999994]\n",
      "69090 [D loss: 0.999987] [G loss: 1.000104]\n",
      "69100 [D loss: 1.000010] [G loss: 1.000164]\n",
      "69110 [D loss: 0.999998] [G loss: 1.000038]\n",
      "69120 [D loss: 0.999965] [G loss: 1.000058]\n",
      "69130 [D loss: 0.999892] [G loss: 1.000358]\n",
      "69140 [D loss: 0.999981] [G loss: 0.999896]\n",
      "69150 [D loss: 0.999946] [G loss: 1.000183]\n",
      "69160 [D loss: 1.000072] [G loss: 1.000272]\n",
      "69170 [D loss: 0.999959] [G loss: 1.000056]\n",
      "69180 [D loss: 0.999942] [G loss: 1.000227]\n",
      "69190 [D loss: 1.000074] [G loss: 1.000093]\n",
      "69200 [D loss: 1.000004] [G loss: 1.000109]\n",
      "69210 [D loss: 0.999932] [G loss: 1.000219]\n",
      "69220 [D loss: 1.000128] [G loss: 1.000112]\n",
      "69230 [D loss: 0.999954] [G loss: 1.000062]\n",
      "69240 [D loss: 0.999919] [G loss: 1.000262]\n",
      "69250 [D loss: 0.999993] [G loss: 0.999962]\n",
      "69260 [D loss: 0.999971] [G loss: 1.000106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69270 [D loss: 0.999926] [G loss: 1.000260]\n",
      "69280 [D loss: 1.000175] [G loss: 0.999896]\n",
      "69290 [D loss: 0.999943] [G loss: 1.000134]\n",
      "69300 [D loss: 0.999999] [G loss: 1.000258]\n",
      "69310 [D loss: 0.999960] [G loss: 0.999946]\n",
      "69320 [D loss: 0.999967] [G loss: 1.000066]\n",
      "69330 [D loss: 0.999928] [G loss: 1.000115]\n",
      "69340 [D loss: 1.000070] [G loss: 1.000136]\n",
      "69350 [D loss: 1.000048] [G loss: 0.999822]\n",
      "69360 [D loss: 1.000013] [G loss: 1.000000]\n",
      "69370 [D loss: 0.999984] [G loss: 1.000141]\n",
      "69380 [D loss: 0.999953] [G loss: 1.000184]\n",
      "69390 [D loss: 0.999966] [G loss: 1.000121]\n",
      "69400 [D loss: 0.999900] [G loss: 0.999981]\n",
      "69410 [D loss: 1.000038] [G loss: 0.999982]\n",
      "69420 [D loss: 0.999975] [G loss: 1.000199]\n",
      "69430 [D loss: 1.000080] [G loss: 0.999976]\n",
      "69440 [D loss: 1.000014] [G loss: 0.999866]\n",
      "69450 [D loss: 0.999990] [G loss: 1.000118]\n",
      "69460 [D loss: 1.000043] [G loss: 1.000033]\n",
      "69470 [D loss: 0.999906] [G loss: 0.999973]\n",
      "69480 [D loss: 0.999965] [G loss: 1.000135]\n",
      "69490 [D loss: 1.000009] [G loss: 1.000297]\n",
      "69500 [D loss: 1.000181] [G loss: 0.999902]\n",
      "69510 [D loss: 0.999972] [G loss: 1.000003]\n",
      "69520 [D loss: 0.999962] [G loss: 1.000113]\n",
      "69530 [D loss: 0.999895] [G loss: 1.000194]\n",
      "69540 [D loss: 0.999961] [G loss: 0.999997]\n",
      "69550 [D loss: 1.000028] [G loss: 0.999809]\n",
      "69560 [D loss: 0.999952] [G loss: 1.000151]\n",
      "69570 [D loss: 0.999966] [G loss: 1.000122]\n",
      "69580 [D loss: 1.000091] [G loss: 0.999851]\n",
      "69590 [D loss: 0.999956] [G loss: 1.000129]\n",
      "69600 [D loss: 0.999993] [G loss: 1.000284]\n",
      "69610 [D loss: 1.000201] [G loss: 1.000033]\n",
      "69620 [D loss: 0.999951] [G loss: 1.000124]\n",
      "69630 [D loss: 1.000028] [G loss: 1.000133]\n",
      "69640 [D loss: 1.000175] [G loss: 0.999864]\n",
      "69650 [D loss: 0.999931] [G loss: 1.000167]\n",
      "69660 [D loss: 0.999987] [G loss: 1.000333]\n",
      "69670 [D loss: 1.000053] [G loss: 0.999835]\n",
      "69680 [D loss: 0.999943] [G loss: 1.000112]\n",
      "69690 [D loss: 0.999931] [G loss: 1.000194]\n",
      "69700 [D loss: 0.999968] [G loss: 1.000241]\n",
      "69710 [D loss: 1.000045] [G loss: 0.999836]\n",
      "69720 [D loss: 0.999930] [G loss: 1.000117]\n",
      "69730 [D loss: 0.999859] [G loss: 1.000555]\n",
      "69740 [D loss: 1.000137] [G loss: 0.999848]\n",
      "69750 [D loss: 0.999945] [G loss: 1.000177]\n",
      "69760 [D loss: 0.999969] [G loss: 1.000527]\n",
      "69770 [D loss: 0.999960] [G loss: 1.000058]\n",
      "69780 [D loss: 1.000035] [G loss: 1.000142]\n",
      "69790 [D loss: 1.000234] [G loss: 0.999563]\n",
      "69800 [D loss: 0.999896] [G loss: 1.000252]\n",
      "69810 [D loss: 1.000171] [G loss: 0.999572]\n",
      "69820 [D loss: 0.999965] [G loss: 1.000112]\n",
      "69830 [D loss: 1.000023] [G loss: 1.000780]\n",
      "69840 [D loss: 0.999995] [G loss: 1.000025]\n",
      "69850 [D loss: 0.999895] [G loss: 1.000585]\n",
      "69860 [D loss: 0.999960] [G loss: 1.000042]\n",
      "69870 [D loss: 0.999869] [G loss: 1.000620]\n",
      "69880 [D loss: 0.999980] [G loss: 0.999895]\n",
      "69890 [D loss: 0.999859] [G loss: 1.000484]\n",
      "69900 [D loss: 1.000194] [G loss: 0.999573]\n",
      "69910 [D loss: 0.999952] [G loss: 1.000352]\n",
      "69920 [D loss: 1.000177] [G loss: 0.999777]\n",
      "69930 [D loss: 0.999924] [G loss: 1.000352]\n",
      "69940 [D loss: 1.000297] [G loss: 0.999997]\n",
      "69950 [D loss: 0.999925] [G loss: 1.000171]\n",
      "69960 [D loss: 1.000135] [G loss: 1.000147]\n",
      "69970 [D loss: 0.999948] [G loss: 1.000112]\n",
      "69980 [D loss: 0.999652] [G loss: 1.000656]\n",
      "69990 [D loss: 0.999998] [G loss: 0.999967]\n",
      "70000 [D loss: 0.999914] [G loss: 1.000252]\n",
      "70010 [D loss: 1.000165] [G loss: 0.999925]\n",
      "70020 [D loss: 0.999935] [G loss: 1.000204]\n",
      "70030 [D loss: 1.000040] [G loss: 1.000506]\n",
      "70040 [D loss: 1.000021] [G loss: 1.000046]\n",
      "70050 [D loss: 1.000086] [G loss: 1.000789]\n",
      "70060 [D loss: 0.999967] [G loss: 1.000048]\n",
      "70070 [D loss: 0.999706] [G loss: 1.000857]\n",
      "70080 [D loss: 0.999998] [G loss: 1.000009]\n",
      "70090 [D loss: 0.999684] [G loss: 1.001156]\n",
      "70100 [D loss: 1.000035] [G loss: 1.000006]\n",
      "70110 [D loss: 1.000179] [G loss: 1.000923]\n",
      "70120 [D loss: 1.000019] [G loss: 1.000075]\n",
      "70130 [D loss: 0.999968] [G loss: 1.001389]\n",
      "70140 [D loss: 0.999989] [G loss: 1.000053]\n",
      "70150 [D loss: 0.999834] [G loss: 1.000777]\n",
      "70160 [D loss: 1.000019] [G loss: 1.000026]\n",
      "70170 [D loss: 0.999885] [G loss: 1.000667]\n",
      "70180 [D loss: 1.000222] [G loss: 0.999620]\n",
      "70190 [D loss: 0.999893] [G loss: 1.000285]\n",
      "70200 [D loss: 1.000419] [G loss: 1.000017]\n",
      "70210 [D loss: 0.999963] [G loss: 1.000260]\n",
      "70220 [D loss: 1.000082] [G loss: 1.000418]\n",
      "70230 [D loss: 0.999973] [G loss: 1.000154]\n",
      "70240 [D loss: 1.000257] [G loss: 1.000913]\n",
      "70250 [D loss: 0.999948] [G loss: 1.000132]\n",
      "70260 [D loss: 0.999936] [G loss: 1.000931]\n",
      "70270 [D loss: 1.000034] [G loss: 0.999942]\n",
      "70280 [D loss: 0.999723] [G loss: 1.000863]\n",
      "70290 [D loss: 1.000026] [G loss: 0.999841]\n",
      "70300 [D loss: 0.999919] [G loss: 1.000610]\n",
      "70310 [D loss: 1.000634] [G loss: 0.999530]\n",
      "70320 [D loss: 0.999836] [G loss: 1.000424]\n",
      "70330 [D loss: 1.000526] [G loss: 0.999059]\n",
      "70340 [D loss: 0.999865] [G loss: 1.000305]\n",
      "70350 [D loss: 1.000417] [G loss: 1.000460]\n",
      "70360 [D loss: 0.999900] [G loss: 1.000307]\n",
      "70370 [D loss: 1.000194] [G loss: 0.999706]\n",
      "70380 [D loss: 0.999965] [G loss: 1.000101]\n",
      "70390 [D loss: 0.999922] [G loss: 1.001020]\n",
      "70400 [D loss: 1.000071] [G loss: 0.999760]\n",
      "70410 [D loss: 0.999714] [G loss: 1.000722]\n",
      "70420 [D loss: 1.000775] [G loss: 0.999531]\n",
      "70430 [D loss: 0.999877] [G loss: 1.000873]\n",
      "70440 [D loss: 1.000449] [G loss: 0.999331]\n",
      "70450 [D loss: 0.999787] [G loss: 1.000656]\n",
      "70460 [D loss: 1.000515] [G loss: 0.999725]\n",
      "70470 [D loss: 0.999903] [G loss: 1.000478]\n",
      "70480 [D loss: 1.000416] [G loss: 0.999295]\n",
      "70490 [D loss: 0.999874] [G loss: 1.000538]\n",
      "70500 [D loss: 1.000580] [G loss: 1.000363]\n",
      "70510 [D loss: 0.999921] [G loss: 1.000392]\n",
      "70520 [D loss: 1.000040] [G loss: 0.999472]\n",
      "70530 [D loss: 0.999948] [G loss: 1.000238]\n",
      "70540 [D loss: 1.000226] [G loss: 1.000092]\n",
      "70550 [D loss: 0.999905] [G loss: 1.000367]\n",
      "70560 [D loss: 1.000275] [G loss: 0.999621]\n",
      "70570 [D loss: 0.999920] [G loss: 1.000291]\n",
      "70580 [D loss: 1.000035] [G loss: 1.001825]\n",
      "70590 [D loss: 0.999939] [G loss: 1.000145]\n",
      "70600 [D loss: 1.000241] [G loss: 1.000039]\n",
      "70610 [D loss: 0.999965] [G loss: 1.000138]\n",
      "70620 [D loss: 0.999884] [G loss: 1.001929]\n",
      "70630 [D loss: 1.000033] [G loss: 1.000167]\n",
      "70640 [D loss: 1.000386] [G loss: 0.999701]\n",
      "70650 [D loss: 0.999958] [G loss: 1.000200]\n",
      "70660 [D loss: 0.999788] [G loss: 1.001634]\n",
      "70670 [D loss: 0.999942] [G loss: 1.000124]\n",
      "70680 [D loss: 1.000018] [G loss: 0.999965]\n",
      "70690 [D loss: 0.999957] [G loss: 1.000153]\n",
      "70700 [D loss: 0.999744] [G loss: 1.001530]\n",
      "70710 [D loss: 1.000016] [G loss: 0.999937]\n",
      "70720 [D loss: 0.999866] [G loss: 1.000568]\n",
      "70730 [D loss: 1.000350] [G loss: 0.999729]\n",
      "70740 [D loss: 0.999814] [G loss: 1.000592]\n",
      "70750 [D loss: 1.000530] [G loss: 0.998813]\n",
      "70760 [D loss: 0.999904] [G loss: 1.000339]\n",
      "70770 [D loss: 1.000707] [G loss: 1.000393]\n",
      "70780 [D loss: 0.999884] [G loss: 1.000404]\n",
      "70790 [D loss: 1.000399] [G loss: 0.999375]\n",
      "70800 [D loss: 0.999922] [G loss: 1.000227]\n",
      "70810 [D loss: 1.000044] [G loss: 1.001946]\n",
      "70820 [D loss: 0.999945] [G loss: 1.000265]\n",
      "70830 [D loss: 1.000553] [G loss: 0.999478]\n",
      "70840 [D loss: 0.999932] [G loss: 1.000205]\n",
      "70850 [D loss: 0.999955] [G loss: 1.001899]\n",
      "70860 [D loss: 0.999970] [G loss: 1.000199]\n",
      "70870 [D loss: 1.000327] [G loss: 1.000444]\n",
      "70880 [D loss: 0.999996] [G loss: 1.000103]\n",
      "70890 [D loss: 0.999691] [G loss: 1.001428]\n",
      "70900 [D loss: 1.000076] [G loss: 0.999964]\n",
      "70910 [D loss: 0.999629] [G loss: 1.000735]\n",
      "70920 [D loss: 1.000510] [G loss: 0.999716]\n",
      "70930 [D loss: 0.999788] [G loss: 1.000655]\n",
      "70940 [D loss: 1.000589] [G loss: 0.999018]\n",
      "70950 [D loss: 0.999900] [G loss: 1.000449]\n",
      "70960 [D loss: 1.000587] [G loss: 1.000728]\n",
      "70970 [D loss: 0.999884] [G loss: 1.000399]\n",
      "70980 [D loss: 1.000356] [G loss: 1.000249]\n",
      "70990 [D loss: 0.999929] [G loss: 1.000188]\n",
      "71000 [D loss: 1.000108] [G loss: 1.001724]\n",
      "71010 [D loss: 0.999981] [G loss: 1.000177]\n",
      "71020 [D loss: 1.000087] [G loss: 1.000248]\n",
      "71030 [D loss: 0.999974] [G loss: 1.000154]\n",
      "71040 [D loss: 0.999911] [G loss: 1.002847]\n",
      "71050 [D loss: 0.999983] [G loss: 1.000135]\n",
      "71060 [D loss: 1.000192] [G loss: 1.000966]\n",
      "71070 [D loss: 0.999981] [G loss: 1.000041]\n",
      "71080 [D loss: 0.999571] [G loss: 1.002634]\n",
      "71090 [D loss: 1.000048] [G loss: 0.999835]\n",
      "71100 [D loss: 0.999951] [G loss: 1.000952]\n",
      "71110 [D loss: 1.000142] [G loss: 0.999602]\n",
      "71120 [D loss: 0.999726] [G loss: 1.001467]\n",
      "71130 [D loss: 1.000163] [G loss: 0.999545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71140 [D loss: 0.999957] [G loss: 1.000514]\n",
      "71150 [D loss: 1.000626] [G loss: 0.999679]\n",
      "71160 [D loss: 0.999830] [G loss: 1.000548]\n",
      "71170 [D loss: 1.000555] [G loss: 0.999199]\n",
      "71180 [D loss: 0.999863] [G loss: 1.000351]\n",
      "71190 [D loss: 1.000264] [G loss: 1.000938]\n",
      "71200 [D loss: 0.999907] [G loss: 1.000300]\n",
      "71210 [D loss: 1.000416] [G loss: 0.999812]\n",
      "71220 [D loss: 0.999926] [G loss: 1.000159]\n",
      "71230 [D loss: 1.000083] [G loss: 1.001306]\n",
      "71240 [D loss: 0.999968] [G loss: 1.000117]\n",
      "71250 [D loss: 1.000065] [G loss: 1.001315]\n",
      "71260 [D loss: 1.000018] [G loss: 0.999931]\n",
      "71270 [D loss: 0.999731] [G loss: 1.001940]\n",
      "71280 [D loss: 0.999942] [G loss: 1.000146]\n",
      "71290 [D loss: 1.000401] [G loss: 1.000702]\n",
      "71300 [D loss: 0.999996] [G loss: 1.000018]\n",
      "71310 [D loss: 0.999679] [G loss: 1.002158]\n",
      "71320 [D loss: 0.999970] [G loss: 1.000094]\n",
      "71330 [D loss: 0.999646] [G loss: 1.001454]\n",
      "71340 [D loss: 0.999944] [G loss: 1.000103]\n",
      "71350 [D loss: 0.999859] [G loss: 1.002753]\n",
      "71360 [D loss: 0.999984] [G loss: 1.000181]\n",
      "71370 [D loss: 1.000150] [G loss: 1.001464]\n",
      "71380 [D loss: 0.999946] [G loss: 1.000180]\n",
      "71390 [D loss: 1.000427] [G loss: 1.002363]\n",
      "71400 [D loss: 0.999888] [G loss: 1.000305]\n",
      "71410 [D loss: 1.000737] [G loss: 0.999565]\n",
      "71420 [D loss: 0.999895] [G loss: 1.000304]\n",
      "71430 [D loss: 1.000200] [G loss: 1.001525]\n",
      "71440 [D loss: 0.999895] [G loss: 1.000466]\n",
      "71450 [D loss: 1.000469] [G loss: 0.999874]\n",
      "71460 [D loss: 0.999884] [G loss: 1.000450]\n",
      "71470 [D loss: 1.001130] [G loss: 0.999378]\n",
      "71480 [D loss: 0.999841] [G loss: 1.000423]\n",
      "71490 [D loss: 1.000672] [G loss: 0.998623]\n",
      "71500 [D loss: 0.999859] [G loss: 1.000581]\n",
      "71510 [D loss: 1.000470] [G loss: 0.999796]\n",
      "71520 [D loss: 0.999793] [G loss: 1.000872]\n",
      "71530 [D loss: 1.000238] [G loss: 0.999528]\n",
      "71540 [D loss: 0.999645] [G loss: 1.002377]\n",
      "71550 [D loss: 1.000060] [G loss: 0.999881]\n",
      "71560 [D loss: 0.999711] [G loss: 1.003131]\n",
      "71570 [D loss: 0.999982] [G loss: 1.000111]\n",
      "71580 [D loss: 0.999964] [G loss: 1.001717]\n",
      "71590 [D loss: 0.999927] [G loss: 1.000190]\n",
      "71600 [D loss: 1.000569] [G loss: 1.003930]\n",
      "71610 [D loss: 0.999856] [G loss: 1.000417]\n",
      "71620 [D loss: 1.001397] [G loss: 0.998576]\n",
      "71630 [D loss: 0.999797] [G loss: 1.000785]\n",
      "71640 [D loss: 1.001210] [G loss: 0.998084]\n",
      "71650 [D loss: 0.999465] [G loss: 1.004379]\n",
      "71660 [D loss: 0.999900] [G loss: 1.000352]\n",
      "71670 [D loss: 1.001126] [G loss: 0.999805]\n",
      "71680 [D loss: 0.999562] [G loss: 1.001192]\n",
      "71690 [D loss: 1.000633] [G loss: 0.998838]\n",
      "71700 [D loss: 0.999738] [G loss: 1.005475]\n",
      "71710 [D loss: 0.999850] [G loss: 1.000608]\n",
      "71720 [D loss: 1.001276] [G loss: 0.998949]\n",
      "71730 [D loss: 0.999392] [G loss: 1.001967]\n",
      "71740 [D loss: 1.000486] [G loss: 0.999466]\n",
      "71750 [D loss: 1.000376] [G loss: 1.004029]\n",
      "71760 [D loss: 0.999759] [G loss: 1.002025]\n",
      "71770 [D loss: 1.001525] [G loss: 0.998584]\n",
      "71780 [D loss: 0.998679] [G loss: 1.004082]\n",
      "71790 [D loss: 0.999918] [G loss: 1.000183]\n",
      "71800 [D loss: 1.000883] [G loss: 1.000663]\n",
      "71810 [D loss: 0.999381] [G loss: 1.003330]\n",
      "71820 [D loss: 1.000031] [G loss: 1.000464]\n",
      "71830 [D loss: 0.999442] [G loss: 1.004066]\n",
      "71840 [D loss: 0.999877] [G loss: 0.999716]\n",
      "71850 [D loss: 1.000796] [G loss: 1.000619]\n",
      "71860 [D loss: 0.999899] [G loss: 1.003673]\n",
      "71870 [D loss: 0.999951] [G loss: 1.003201]\n",
      "71880 [D loss: 0.999679] [G loss: 1.002540]\n",
      "71890 [D loss: 1.000626] [G loss: 0.999489]\n",
      "71900 [D loss: 1.000286] [G loss: 1.000821]\n",
      "71910 [D loss: 0.999939] [G loss: 1.003008]\n",
      "71920 [D loss: 1.000348] [G loss: 1.002238]\n",
      "71930 [D loss: 0.999869] [G loss: 1.001193]\n",
      "71940 [D loss: 1.000111] [G loss: 1.000259]\n",
      "71950 [D loss: 1.000190] [G loss: 1.001555]\n",
      "71960 [D loss: 1.000046] [G loss: 1.001272]\n",
      "71970 [D loss: 0.999630] [G loss: 1.002395]\n",
      "71980 [D loss: 0.999911] [G loss: 1.000509]\n",
      "71990 [D loss: 0.999657] [G loss: 1.001720]\n",
      "72000 [D loss: 0.999957] [G loss: 1.000337]\n",
      "72010 [D loss: 0.999669] [G loss: 1.001745]\n",
      "72020 [D loss: 0.999931] [G loss: 1.000960]\n",
      "72030 [D loss: 1.000061] [G loss: 1.000258]\n",
      "72040 [D loss: 0.999753] [G loss: 1.000714]\n",
      "72050 [D loss: 1.000173] [G loss: 1.000348]\n",
      "72060 [D loss: 0.999878] [G loss: 1.000800]\n",
      "72070 [D loss: 1.000067] [G loss: 0.999768]\n",
      "72080 [D loss: 0.999892] [G loss: 1.000364]\n",
      "72090 [D loss: 1.000121] [G loss: 1.000680]\n",
      "72100 [D loss: 0.999874] [G loss: 1.000576]\n",
      "72110 [D loss: 0.999926] [G loss: 1.000560]\n",
      "72120 [D loss: 1.000006] [G loss: 1.000144]\n",
      "72130 [D loss: 0.999931] [G loss: 1.000356]\n",
      "72140 [D loss: 1.000062] [G loss: 0.999982]\n",
      "72150 [D loss: 0.999971] [G loss: 1.000317]\n",
      "72160 [D loss: 1.000008] [G loss: 1.000197]\n",
      "72170 [D loss: 0.999925] [G loss: 1.000217]\n",
      "72180 [D loss: 0.999985] [G loss: 1.000047]\n",
      "72190 [D loss: 0.999997] [G loss: 1.000090]\n",
      "72200 [D loss: 0.999951] [G loss: 1.000097]\n",
      "72210 [D loss: 0.999973] [G loss: 1.000097]\n",
      "72220 [D loss: 0.999975] [G loss: 1.000025]\n",
      "72230 [D loss: 1.000029] [G loss: 0.999968]\n",
      "72240 [D loss: 0.999948] [G loss: 1.000049]\n",
      "72250 [D loss: 0.999997] [G loss: 1.000055]\n",
      "72260 [D loss: 0.999899] [G loss: 1.000180]\n",
      "72270 [D loss: 1.000015] [G loss: 0.999979]\n",
      "72280 [D loss: 0.999932] [G loss: 0.999906]\n",
      "72290 [D loss: 0.999947] [G loss: 1.000096]\n",
      "72300 [D loss: 1.000094] [G loss: 0.999847]\n",
      "72310 [D loss: 1.000031] [G loss: 0.999939]\n",
      "72320 [D loss: 0.999960] [G loss: 1.000066]\n",
      "72330 [D loss: 0.999963] [G loss: 1.000300]\n",
      "72340 [D loss: 0.999948] [G loss: 1.000052]\n",
      "72350 [D loss: 0.999966] [G loss: 1.000038]\n",
      "72360 [D loss: 0.999969] [G loss: 1.000116]\n",
      "72370 [D loss: 0.999961] [G loss: 1.000117]\n",
      "72380 [D loss: 0.999975] [G loss: 0.999963]\n",
      "72390 [D loss: 0.999958] [G loss: 1.000090]\n",
      "72400 [D loss: 0.999980] [G loss: 1.000083]\n",
      "72410 [D loss: 0.999937] [G loss: 1.000033]\n",
      "72420 [D loss: 0.999949] [G loss: 1.000065]\n",
      "72430 [D loss: 0.999993] [G loss: 0.999968]\n",
      "72440 [D loss: 0.999965] [G loss: 1.000086]\n",
      "72450 [D loss: 0.999962] [G loss: 1.000074]\n",
      "72460 [D loss: 0.999964] [G loss: 1.000096]\n",
      "72470 [D loss: 0.999942] [G loss: 1.000118]\n",
      "72480 [D loss: 0.999943] [G loss: 0.999947]\n",
      "72490 [D loss: 0.999960] [G loss: 1.000072]\n",
      "72500 [D loss: 0.999930] [G loss: 1.000089]\n",
      "72510 [D loss: 0.999897] [G loss: 0.999973]\n",
      "72520 [D loss: 1.000047] [G loss: 0.999820]\n",
      "72530 [D loss: 0.999944] [G loss: 1.000059]\n",
      "72540 [D loss: 0.999972] [G loss: 1.000118]\n",
      "72550 [D loss: 0.999998] [G loss: 1.000151]\n",
      "72560 [D loss: 0.999967] [G loss: 0.999961]\n",
      "72570 [D loss: 0.999963] [G loss: 1.000062]\n",
      "72580 [D loss: 0.999986] [G loss: 1.000224]\n",
      "72590 [D loss: 1.000001] [G loss: 0.999893]\n",
      "72600 [D loss: 0.999982] [G loss: 1.000007]\n",
      "72610 [D loss: 0.999931] [G loss: 1.000218]\n",
      "72620 [D loss: 1.000082] [G loss: 0.999737]\n",
      "72630 [D loss: 0.999928] [G loss: 1.000088]\n",
      "72640 [D loss: 0.999904] [G loss: 0.999990]\n",
      "72650 [D loss: 0.999926] [G loss: 1.000021]\n",
      "72660 [D loss: 1.000141] [G loss: 0.999989]\n",
      "72670 [D loss: 1.000010] [G loss: 1.000319]\n",
      "72680 [D loss: 0.999996] [G loss: 1.000019]\n",
      "72690 [D loss: 0.999972] [G loss: 1.000071]\n",
      "72700 [D loss: 1.000005] [G loss: 1.000391]\n",
      "72710 [D loss: 1.000078] [G loss: 1.000031]\n",
      "72720 [D loss: 0.999913] [G loss: 1.000353]\n",
      "72730 [D loss: 1.000085] [G loss: 0.999898]\n",
      "72740 [D loss: 0.999947] [G loss: 1.000331]\n",
      "72750 [D loss: 1.000124] [G loss: 0.999632]\n",
      "72760 [D loss: 0.999931] [G loss: 1.000164]\n",
      "72770 [D loss: 0.999877] [G loss: 1.000180]\n",
      "72780 [D loss: 0.999971] [G loss: 0.999954]\n",
      "72790 [D loss: 0.999989] [G loss: 1.000135]\n",
      "72800 [D loss: 1.000064] [G loss: 0.999930]\n",
      "72810 [D loss: 0.999926] [G loss: 1.000101]\n",
      "72820 [D loss: 1.000051] [G loss: 0.999641]\n",
      "72830 [D loss: 0.999971] [G loss: 1.000022]\n",
      "72840 [D loss: 1.000019] [G loss: 1.000299]\n",
      "72850 [D loss: 0.999985] [G loss: 1.000042]\n",
      "72860 [D loss: 0.999953] [G loss: 1.000173]\n",
      "72870 [D loss: 0.999932] [G loss: 1.000050]\n",
      "72880 [D loss: 0.999967] [G loss: 1.000143]\n",
      "72890 [D loss: 0.999951] [G loss: 1.000061]\n",
      "72900 [D loss: 1.000012] [G loss: 1.000150]\n",
      "72910 [D loss: 0.999979] [G loss: 0.999968]\n",
      "72920 [D loss: 0.999972] [G loss: 1.000093]\n",
      "72930 [D loss: 0.999941] [G loss: 1.000014]\n",
      "72940 [D loss: 0.999866] [G loss: 1.000167]\n",
      "72950 [D loss: 0.999952] [G loss: 1.000128]\n",
      "72960 [D loss: 1.000010] [G loss: 0.999994]\n",
      "72970 [D loss: 0.999956] [G loss: 1.000114]\n",
      "72980 [D loss: 0.999996] [G loss: 1.000104]\n",
      "72990 [D loss: 1.000068] [G loss: 0.999921]\n",
      "73000 [D loss: 0.999936] [G loss: 1.000059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73010 [D loss: 0.999936] [G loss: 1.000235]\n",
      "73020 [D loss: 1.000043] [G loss: 0.999777]\n",
      "73030 [D loss: 0.999907] [G loss: 1.000106]\n",
      "73040 [D loss: 1.000110] [G loss: 1.000174]\n",
      "73050 [D loss: 0.999989] [G loss: 1.000004]\n",
      "73060 [D loss: 0.999942] [G loss: 1.000133]\n",
      "73070 [D loss: 1.000228] [G loss: 1.000087]\n",
      "73080 [D loss: 0.999946] [G loss: 1.000155]\n",
      "73090 [D loss: 0.999992] [G loss: 0.999941]\n",
      "73100 [D loss: 0.999932] [G loss: 1.000134]\n",
      "73110 [D loss: 0.999887] [G loss: 1.000181]\n",
      "73120 [D loss: 0.999975] [G loss: 1.000121]\n",
      "73130 [D loss: 0.999866] [G loss: 1.000297]\n",
      "73140 [D loss: 0.999949] [G loss: 1.000072]\n",
      "73150 [D loss: 1.000066] [G loss: 1.000153]\n",
      "73160 [D loss: 0.999941] [G loss: 1.000136]\n",
      "73170 [D loss: 1.000101] [G loss: 0.999732]\n",
      "73180 [D loss: 0.999956] [G loss: 1.000048]\n",
      "73190 [D loss: 1.000054] [G loss: 1.000517]\n",
      "73200 [D loss: 1.000089] [G loss: 0.999861]\n",
      "73210 [D loss: 0.999865] [G loss: 1.000196]\n",
      "73220 [D loss: 1.000036] [G loss: 0.999788]\n",
      "73230 [D loss: 0.999964] [G loss: 1.000233]\n",
      "73240 [D loss: 1.000144] [G loss: 0.999907]\n",
      "73250 [D loss: 0.999993] [G loss: 1.000004]\n",
      "73260 [D loss: 0.999974] [G loss: 1.000154]\n",
      "73270 [D loss: 0.999910] [G loss: 0.999880]\n",
      "73280 [D loss: 1.000022] [G loss: 0.999818]\n",
      "73290 [D loss: 0.999914] [G loss: 1.000144]\n",
      "73300 [D loss: 1.000123] [G loss: 1.000089]\n",
      "73310 [D loss: 1.000208] [G loss: 0.999856]\n",
      "73320 [D loss: 0.999924] [G loss: 1.000235]\n",
      "73330 [D loss: 0.999991] [G loss: 1.000265]\n",
      "73340 [D loss: 0.999948] [G loss: 1.000037]\n",
      "73350 [D loss: 0.999746] [G loss: 1.000556]\n",
      "73360 [D loss: 1.000510] [G loss: 0.999375]\n",
      "73370 [D loss: 0.999886] [G loss: 1.000143]\n",
      "73380 [D loss: 1.000209] [G loss: 0.999880]\n",
      "73390 [D loss: 0.999995] [G loss: 1.000109]\n",
      "73400 [D loss: 0.999943] [G loss: 1.000556]\n",
      "73410 [D loss: 0.999929] [G loss: 1.000060]\n",
      "73420 [D loss: 0.999783] [G loss: 1.000726]\n",
      "73430 [D loss: 1.000288] [G loss: 0.999688]\n",
      "73440 [D loss: 0.999866] [G loss: 1.000199]\n",
      "73450 [D loss: 1.000285] [G loss: 1.000551]\n",
      "73460 [D loss: 1.000050] [G loss: 1.000078]\n",
      "73470 [D loss: 1.000028] [G loss: 1.000343]\n",
      "73480 [D loss: 0.999957] [G loss: 1.000021]\n",
      "73490 [D loss: 0.999855] [G loss: 1.000873]\n",
      "73500 [D loss: 1.000595] [G loss: 0.998886]\n",
      "73510 [D loss: 0.999698] [G loss: 1.000452]\n",
      "73520 [D loss: 1.000624] [G loss: 0.999016]\n",
      "73530 [D loss: 0.999978] [G loss: 1.000397]\n",
      "73540 [D loss: 1.000120] [G loss: 0.999728]\n",
      "73550 [D loss: 0.999865] [G loss: 1.000210]\n",
      "73560 [D loss: 1.000538] [G loss: 0.999673]\n",
      "73570 [D loss: 0.999954] [G loss: 1.000208]\n",
      "73580 [D loss: 0.999691] [G loss: 1.001110]\n",
      "73590 [D loss: 0.999953] [G loss: 1.000067]\n",
      "73600 [D loss: 0.999788] [G loss: 1.001318]\n",
      "73610 [D loss: 1.000019] [G loss: 1.000006]\n",
      "73620 [D loss: 1.000103] [G loss: 1.000516]\n",
      "73630 [D loss: 1.000053] [G loss: 0.999782]\n",
      "73640 [D loss: 0.999931] [G loss: 1.000388]\n",
      "73650 [D loss: 1.000100] [G loss: 0.999459]\n",
      "73660 [D loss: 0.999932] [G loss: 1.000218]\n",
      "73670 [D loss: 1.000033] [G loss: 0.999806]\n",
      "73680 [D loss: 0.999954] [G loss: 1.000010]\n",
      "73690 [D loss: 1.000006] [G loss: 1.000935]\n",
      "73700 [D loss: 0.999971] [G loss: 0.999889]\n",
      "73710 [D loss: 0.999941] [G loss: 1.000416]\n",
      "73720 [D loss: 1.000208] [G loss: 0.999625]\n",
      "73730 [D loss: 0.999998] [G loss: 1.000188]\n",
      "73740 [D loss: 1.000015] [G loss: 0.999760]\n",
      "73750 [D loss: 0.999970] [G loss: 1.000255]\n",
      "73760 [D loss: 1.000335] [G loss: 0.999530]\n",
      "73770 [D loss: 0.999900] [G loss: 1.000411]\n",
      "73780 [D loss: 1.000186] [G loss: 0.999569]\n",
      "73790 [D loss: 0.999903] [G loss: 1.000414]\n",
      "73800 [D loss: 1.000243] [G loss: 0.999268]\n",
      "73810 [D loss: 0.999956] [G loss: 1.000127]\n",
      "73820 [D loss: 1.000138] [G loss: 0.999611]\n",
      "73830 [D loss: 1.000014] [G loss: 1.000059]\n",
      "73840 [D loss: 0.999934] [G loss: 1.000218]\n",
      "73850 [D loss: 0.999918] [G loss: 0.999917]\n",
      "73860 [D loss: 0.999827] [G loss: 1.000243]\n",
      "73870 [D loss: 1.000001] [G loss: 0.999972]\n",
      "73880 [D loss: 0.999931] [G loss: 0.999892]\n",
      "73890 [D loss: 1.000042] [G loss: 0.999992]\n",
      "73900 [D loss: 1.000152] [G loss: 0.999826]\n",
      "73910 [D loss: 0.999952] [G loss: 1.000185]\n",
      "73920 [D loss: 1.000219] [G loss: 0.999629]\n",
      "73930 [D loss: 0.999931] [G loss: 1.000049]\n",
      "73940 [D loss: 0.999647] [G loss: 1.000239]\n",
      "73950 [D loss: 0.999945] [G loss: 1.000219]\n",
      "73960 [D loss: 0.999989] [G loss: 1.000359]\n",
      "73970 [D loss: 1.000092] [G loss: 0.999817]\n",
      "73980 [D loss: 0.999930] [G loss: 1.000041]\n",
      "73990 [D loss: 1.000315] [G loss: 1.000088]\n",
      "74000 [D loss: 1.000143] [G loss: 0.999850]\n",
      "74010 [D loss: 0.999924] [G loss: 1.000537]\n",
      "74020 [D loss: 0.999917] [G loss: 1.000102]\n",
      "74030 [D loss: 1.000328] [G loss: 1.000178]\n",
      "74040 [D loss: 0.999966] [G loss: 1.000148]\n",
      "74050 [D loss: 1.000373] [G loss: 0.999303]\n",
      "74060 [D loss: 1.000004] [G loss: 1.000050]\n",
      "74070 [D loss: 1.000040] [G loss: 1.000210]\n",
      "74080 [D loss: 0.999950] [G loss: 1.000111]\n",
      "74090 [D loss: 0.999887] [G loss: 1.000528]\n",
      "74100 [D loss: 1.000244] [G loss: 0.999677]\n",
      "74110 [D loss: 1.000358] [G loss: 1.000111]\n",
      "74120 [D loss: 0.999941] [G loss: 1.000124]\n",
      "74130 [D loss: 0.999917] [G loss: 1.000193]\n",
      "74140 [D loss: 1.000141] [G loss: 0.999618]\n",
      "74150 [D loss: 0.999950] [G loss: 0.999985]\n",
      "74160 [D loss: 0.999972] [G loss: 1.000059]\n",
      "74170 [D loss: 0.999963] [G loss: 1.000090]\n",
      "74180 [D loss: 1.000074] [G loss: 0.999855]\n",
      "74190 [D loss: 0.999951] [G loss: 1.000120]\n",
      "74200 [D loss: 0.999916] [G loss: 1.000238]\n",
      "74210 [D loss: 1.000061] [G loss: 0.999938]\n",
      "74220 [D loss: 1.000069] [G loss: 1.000028]\n",
      "74230 [D loss: 0.999952] [G loss: 1.000027]\n",
      "74240 [D loss: 1.000061] [G loss: 1.000176]\n",
      "74250 [D loss: 1.000142] [G loss: 0.999941]\n",
      "74260 [D loss: 0.999894] [G loss: 0.999918]\n",
      "74270 [D loss: 1.000686] [G loss: 0.999842]\n",
      "74280 [D loss: 0.999875] [G loss: 1.000344]\n",
      "74290 [D loss: 0.999945] [G loss: 1.000193]\n",
      "74300 [D loss: 1.000393] [G loss: 0.999604]\n",
      "74310 [D loss: 0.999930] [G loss: 1.000037]\n",
      "74320 [D loss: 1.000467] [G loss: 0.999700]\n",
      "74330 [D loss: 0.999895] [G loss: 1.000610]\n",
      "74340 [D loss: 0.999785] [G loss: 1.000531]\n",
      "74350 [D loss: 0.999966] [G loss: 1.000154]\n",
      "74360 [D loss: 1.000664] [G loss: 0.999219]\n",
      "74370 [D loss: 0.999854] [G loss: 1.000246]\n",
      "74380 [D loss: 0.999932] [G loss: 1.000053]\n",
      "74390 [D loss: 1.000134] [G loss: 0.999987]\n",
      "74400 [D loss: 1.000073] [G loss: 1.000302]\n",
      "74410 [D loss: 0.999911] [G loss: 1.000179]\n",
      "74420 [D loss: 0.999990] [G loss: 0.999919]\n",
      "74430 [D loss: 0.999961] [G loss: 1.000010]\n",
      "74440 [D loss: 1.000078] [G loss: 0.999989]\n",
      "74450 [D loss: 0.999982] [G loss: 1.000250]\n",
      "74460 [D loss: 0.999904] [G loss: 1.000166]\n",
      "74470 [D loss: 0.999997] [G loss: 1.000059]\n",
      "74480 [D loss: 0.999944] [G loss: 1.000046]\n",
      "74490 [D loss: 0.999938] [G loss: 1.000146]\n",
      "74500 [D loss: 1.000001] [G loss: 1.000320]\n",
      "74510 [D loss: 1.000000] [G loss: 1.000274]\n",
      "74520 [D loss: 0.999951] [G loss: 1.000084]\n",
      "74530 [D loss: 1.000029] [G loss: 1.000062]\n",
      "74540 [D loss: 1.000012] [G loss: 1.000143]\n",
      "74550 [D loss: 0.999999] [G loss: 1.000186]\n",
      "74560 [D loss: 0.999979] [G loss: 1.000082]\n",
      "74570 [D loss: 1.000008] [G loss: 1.000049]\n",
      "74580 [D loss: 0.999987] [G loss: 1.000026]\n",
      "74590 [D loss: 0.999994] [G loss: 1.000038]\n",
      "74600 [D loss: 0.999985] [G loss: 1.000064]\n",
      "74610 [D loss: 1.000003] [G loss: 1.000188]\n",
      "74620 [D loss: 0.999929] [G loss: 1.000173]\n",
      "74630 [D loss: 0.999950] [G loss: 1.000135]\n",
      "74640 [D loss: 0.999958] [G loss: 1.000058]\n",
      "74650 [D loss: 0.999947] [G loss: 1.000041]\n",
      "74660 [D loss: 0.999988] [G loss: 1.000047]\n",
      "74670 [D loss: 0.999930] [G loss: 1.000109]\n",
      "74680 [D loss: 0.999959] [G loss: 1.000079]\n",
      "74690 [D loss: 0.999973] [G loss: 1.000067]\n",
      "74700 [D loss: 0.999950] [G loss: 1.000101]\n",
      "74710 [D loss: 0.999972] [G loss: 1.000083]\n",
      "74720 [D loss: 0.999962] [G loss: 1.000127]\n",
      "74730 [D loss: 0.999968] [G loss: 1.000081]\n",
      "74740 [D loss: 0.999986] [G loss: 1.000071]\n",
      "74750 [D loss: 0.999971] [G loss: 1.000069]\n",
      "74760 [D loss: 0.999967] [G loss: 1.000094]\n",
      "74770 [D loss: 0.999952] [G loss: 1.000091]\n",
      "74780 [D loss: 0.999973] [G loss: 1.000105]\n",
      "74790 [D loss: 0.999983] [G loss: 1.000126]\n",
      "74800 [D loss: 0.999955] [G loss: 1.000150]\n",
      "74810 [D loss: 0.999975] [G loss: 1.000094]\n",
      "74820 [D loss: 0.999957] [G loss: 1.000095]\n",
      "74830 [D loss: 0.999967] [G loss: 1.000088]\n",
      "74840 [D loss: 0.999948] [G loss: 1.000095]\n",
      "74850 [D loss: 0.999980] [G loss: 1.000072]\n",
      "74860 [D loss: 0.999957] [G loss: 1.000108]\n",
      "74870 [D loss: 0.999965] [G loss: 1.000075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74880 [D loss: 0.999958] [G loss: 1.000060]\n",
      "74890 [D loss: 0.999961] [G loss: 1.000087]\n",
      "74900 [D loss: 0.999965] [G loss: 1.000050]\n",
      "74910 [D loss: 0.999968] [G loss: 1.000053]\n",
      "74920 [D loss: 0.999962] [G loss: 1.000053]\n",
      "74930 [D loss: 0.999963] [G loss: 1.000054]\n",
      "74940 [D loss: 0.999959] [G loss: 1.000065]\n",
      "74950 [D loss: 0.999965] [G loss: 1.000058]\n",
      "74960 [D loss: 0.999967] [G loss: 1.000044]\n",
      "74970 [D loss: 0.999970] [G loss: 1.000054]\n",
      "74980 [D loss: 0.999952] [G loss: 1.000049]\n",
      "74990 [D loss: 0.999962] [G loss: 1.000060]\n",
      "75000 [D loss: 0.999966] [G loss: 1.000058]\n",
      "75010 [D loss: 0.999968] [G loss: 1.000053]\n",
      "75020 [D loss: 0.999962] [G loss: 1.000056]\n",
      "75030 [D loss: 0.999964] [G loss: 1.000057]\n",
      "75040 [D loss: 0.999969] [G loss: 1.000051]\n",
      "75050 [D loss: 0.999969] [G loss: 1.000055]\n",
      "75060 [D loss: 0.999968] [G loss: 1.000062]\n",
      "75070 [D loss: 0.999964] [G loss: 1.000058]\n",
      "75080 [D loss: 0.999966] [G loss: 1.000063]\n",
      "75090 [D loss: 0.999966] [G loss: 1.000059]\n",
      "75100 [D loss: 0.999967] [G loss: 1.000061]\n",
      "75110 [D loss: 0.999961] [G loss: 1.000061]\n",
      "75120 [D loss: 0.999966] [G loss: 1.000063]\n",
      "75130 [D loss: 0.999968] [G loss: 1.000061]\n",
      "75140 [D loss: 0.999963] [G loss: 1.000066]\n",
      "75150 [D loss: 0.999963] [G loss: 1.000070]\n",
      "75160 [D loss: 0.999969] [G loss: 1.000059]\n",
      "75170 [D loss: 0.999964] [G loss: 1.000058]\n",
      "75180 [D loss: 0.999963] [G loss: 1.000053]\n",
      "75190 [D loss: 0.999968] [G loss: 1.000065]\n",
      "75200 [D loss: 0.999970] [G loss: 1.000051]\n",
      "75210 [D loss: 0.999967] [G loss: 1.000057]\n",
      "75220 [D loss: 0.999965] [G loss: 1.000055]\n",
      "75230 [D loss: 0.999969] [G loss: 1.000059]\n",
      "75240 [D loss: 0.999966] [G loss: 1.000063]\n",
      "75250 [D loss: 0.999971] [G loss: 1.000061]\n",
      "75260 [D loss: 0.999970] [G loss: 1.000052]\n",
      "75270 [D loss: 0.999966] [G loss: 1.000054]\n",
      "75280 [D loss: 0.999973] [G loss: 1.000064]\n",
      "75290 [D loss: 0.999970] [G loss: 1.000059]\n",
      "75300 [D loss: 0.999972] [G loss: 1.000068]\n",
      "75310 [D loss: 0.999964] [G loss: 1.000063]\n",
      "75320 [D loss: 0.999971] [G loss: 1.000060]\n",
      "75330 [D loss: 0.999964] [G loss: 1.000060]\n",
      "75340 [D loss: 0.999970] [G loss: 1.000059]\n",
      "75350 [D loss: 0.999970] [G loss: 1.000058]\n",
      "75360 [D loss: 0.999969] [G loss: 1.000059]\n",
      "75370 [D loss: 0.999966] [G loss: 1.000054]\n",
      "75380 [D loss: 0.999974] [G loss: 1.000053]\n",
      "75390 [D loss: 0.999968] [G loss: 1.000060]\n",
      "75400 [D loss: 0.999971] [G loss: 1.000059]\n",
      "75410 [D loss: 0.999968] [G loss: 1.000059]\n",
      "75420 [D loss: 0.999963] [G loss: 1.000065]\n",
      "75430 [D loss: 0.999969] [G loss: 1.000055]\n",
      "75440 [D loss: 0.999972] [G loss: 1.000050]\n",
      "75450 [D loss: 0.999966] [G loss: 1.000063]\n",
      "75460 [D loss: 0.999971] [G loss: 1.000058]\n",
      "75470 [D loss: 0.999968] [G loss: 1.000060]\n",
      "75480 [D loss: 0.999970] [G loss: 1.000059]\n",
      "75490 [D loss: 0.999969] [G loss: 1.000060]\n",
      "75500 [D loss: 0.999965] [G loss: 1.000057]\n",
      "75510 [D loss: 0.999967] [G loss: 1.000052]\n",
      "75520 [D loss: 0.999968] [G loss: 1.000060]\n",
      "75530 [D loss: 0.999967] [G loss: 1.000060]\n",
      "75540 [D loss: 0.999968] [G loss: 1.000061]\n",
      "75550 [D loss: 0.999972] [G loss: 1.000061]\n",
      "75560 [D loss: 0.999966] [G loss: 1.000059]\n",
      "75570 [D loss: 0.999962] [G loss: 1.000061]\n",
      "75580 [D loss: 0.999967] [G loss: 1.000060]\n",
      "75590 [D loss: 0.999969] [G loss: 1.000068]\n",
      "75600 [D loss: 0.999968] [G loss: 1.000062]\n",
      "75610 [D loss: 0.999969] [G loss: 1.000049]\n",
      "75620 [D loss: 0.999963] [G loss: 1.000057]\n",
      "75630 [D loss: 0.999962] [G loss: 1.000055]\n",
      "75640 [D loss: 0.999966] [G loss: 1.000061]\n",
      "75650 [D loss: 0.999962] [G loss: 1.000065]\n",
      "75660 [D loss: 0.999968] [G loss: 1.000060]\n",
      "75670 [D loss: 0.999964] [G loss: 1.000064]\n",
      "75680 [D loss: 0.999963] [G loss: 1.000063]\n",
      "75690 [D loss: 0.999968] [G loss: 1.000067]\n",
      "75700 [D loss: 0.999970] [G loss: 1.000061]\n",
      "75710 [D loss: 0.999963] [G loss: 1.000058]\n",
      "75720 [D loss: 0.999971] [G loss: 1.000062]\n",
      "75730 [D loss: 0.999969] [G loss: 1.000059]\n",
      "75740 [D loss: 0.999966] [G loss: 1.000066]\n",
      "75750 [D loss: 0.999969] [G loss: 1.000063]\n",
      "75760 [D loss: 0.999972] [G loss: 1.000061]\n",
      "75770 [D loss: 0.999964] [G loss: 1.000065]\n",
      "75780 [D loss: 0.999963] [G loss: 1.000059]\n",
      "75790 [D loss: 0.999971] [G loss: 1.000061]\n",
      "75800 [D loss: 0.999968] [G loss: 1.000061]\n",
      "75810 [D loss: 0.999971] [G loss: 1.000057]\n",
      "75820 [D loss: 0.999966] [G loss: 1.000064]\n",
      "75830 [D loss: 0.999966] [G loss: 1.000064]\n",
      "75840 [D loss: 0.999968] [G loss: 1.000054]\n",
      "75850 [D loss: 0.999967] [G loss: 1.000057]\n",
      "75860 [D loss: 0.999970] [G loss: 1.000052]\n",
      "75870 [D loss: 0.999963] [G loss: 1.000059]\n",
      "75880 [D loss: 0.999969] [G loss: 1.000063]\n",
      "75890 [D loss: 0.999963] [G loss: 1.000065]\n",
      "75900 [D loss: 0.999963] [G loss: 1.000063]\n",
      "75910 [D loss: 0.999970] [G loss: 1.000065]\n",
      "75920 [D loss: 0.999966] [G loss: 1.000065]\n",
      "75930 [D loss: 0.999972] [G loss: 1.000044]\n",
      "75940 [D loss: 0.999972] [G loss: 1.000052]\n",
      "75950 [D loss: 0.999966] [G loss: 1.000058]\n",
      "75960 [D loss: 0.999967] [G loss: 1.000066]\n",
      "75970 [D loss: 0.999968] [G loss: 1.000060]\n",
      "75980 [D loss: 0.999963] [G loss: 1.000060]\n",
      "75990 [D loss: 0.999964] [G loss: 1.000073]\n",
      "76000 [D loss: 0.999970] [G loss: 1.000061]\n",
      "76010 [D loss: 0.999968] [G loss: 1.000060]\n",
      "76020 [D loss: 0.999971] [G loss: 1.000062]\n",
      "76030 [D loss: 0.999972] [G loss: 1.000060]\n",
      "76040 [D loss: 0.999963] [G loss: 1.000046]\n",
      "76050 [D loss: 0.999967] [G loss: 1.000069]\n",
      "76060 [D loss: 0.999966] [G loss: 1.000061]\n",
      "76070 [D loss: 0.999968] [G loss: 1.000069]\n",
      "76080 [D loss: 0.999963] [G loss: 1.000066]\n",
      "76090 [D loss: 0.999971] [G loss: 1.000059]\n",
      "76100 [D loss: 0.999969] [G loss: 1.000054]\n",
      "76110 [D loss: 0.999976] [G loss: 1.000051]\n",
      "76120 [D loss: 0.999972] [G loss: 1.000049]\n",
      "76130 [D loss: 0.999975] [G loss: 1.000048]\n",
      "76140 [D loss: 0.999966] [G loss: 1.000059]\n",
      "76150 [D loss: 0.999963] [G loss: 1.000069]\n",
      "76160 [D loss: 0.999966] [G loss: 1.000065]\n",
      "76170 [D loss: 0.999964] [G loss: 1.000058]\n",
      "76180 [D loss: 0.999966] [G loss: 1.000054]\n",
      "76190 [D loss: 0.999966] [G loss: 1.000055]\n",
      "76200 [D loss: 0.999962] [G loss: 1.000055]\n",
      "76210 [D loss: 0.999966] [G loss: 1.000066]\n",
      "76220 [D loss: 0.999965] [G loss: 1.000059]\n",
      "76230 [D loss: 0.999965] [G loss: 1.000066]\n",
      "76240 [D loss: 0.999967] [G loss: 1.000058]\n",
      "76250 [D loss: 0.999959] [G loss: 1.000060]\n",
      "76260 [D loss: 0.999971] [G loss: 1.000047]\n",
      "76270 [D loss: 0.999971] [G loss: 1.000063]\n",
      "76280 [D loss: 0.999969] [G loss: 1.000079]\n",
      "76290 [D loss: 0.999964] [G loss: 1.000039]\n",
      "76300 [D loss: 0.999974] [G loss: 1.000046]\n",
      "76310 [D loss: 0.999968] [G loss: 1.000064]\n",
      "76320 [D loss: 0.999973] [G loss: 1.000056]\n",
      "76330 [D loss: 0.999963] [G loss: 1.000056]\n",
      "76340 [D loss: 0.999965] [G loss: 1.000062]\n",
      "76350 [D loss: 0.999983] [G loss: 1.000044]\n",
      "76360 [D loss: 0.999964] [G loss: 1.000081]\n",
      "76370 [D loss: 0.999969] [G loss: 1.000072]\n",
      "76380 [D loss: 0.999956] [G loss: 1.000067]\n",
      "76390 [D loss: 0.999977] [G loss: 1.000048]\n",
      "76400 [D loss: 0.999964] [G loss: 1.000057]\n",
      "76410 [D loss: 0.999963] [G loss: 1.000058]\n",
      "76420 [D loss: 0.999970] [G loss: 1.000059]\n",
      "76430 [D loss: 0.999970] [G loss: 1.000056]\n",
      "76440 [D loss: 0.999971] [G loss: 1.000062]\n",
      "76450 [D loss: 0.999969] [G loss: 1.000067]\n",
      "76460 [D loss: 0.999963] [G loss: 1.000066]\n",
      "76470 [D loss: 0.999968] [G loss: 1.000049]\n",
      "76480 [D loss: 0.999971] [G loss: 1.000059]\n",
      "76490 [D loss: 0.999969] [G loss: 1.000062]\n",
      "76500 [D loss: 0.999967] [G loss: 1.000059]\n",
      "76510 [D loss: 0.999967] [G loss: 1.000053]\n",
      "76520 [D loss: 0.999975] [G loss: 1.000063]\n",
      "76530 [D loss: 0.999969] [G loss: 1.000065]\n",
      "76540 [D loss: 0.999964] [G loss: 1.000059]\n",
      "76550 [D loss: 0.999966] [G loss: 1.000049]\n",
      "76560 [D loss: 0.999968] [G loss: 1.000060]\n",
      "76570 [D loss: 0.999973] [G loss: 1.000067]\n",
      "76580 [D loss: 0.999963] [G loss: 1.000061]\n",
      "76590 [D loss: 0.999980] [G loss: 1.000045]\n",
      "76600 [D loss: 0.999968] [G loss: 1.000068]\n",
      "76610 [D loss: 0.999969] [G loss: 1.000056]\n",
      "76620 [D loss: 0.999972] [G loss: 1.000073]\n",
      "76630 [D loss: 0.999968] [G loss: 1.000043]\n",
      "76640 [D loss: 0.999985] [G loss: 1.000041]\n",
      "76650 [D loss: 0.999969] [G loss: 1.000054]\n",
      "76660 [D loss: 0.999970] [G loss: 1.000066]\n",
      "76670 [D loss: 0.999964] [G loss: 1.000059]\n",
      "76680 [D loss: 0.999968] [G loss: 1.000057]\n",
      "76690 [D loss: 0.999981] [G loss: 1.000030]\n",
      "76700 [D loss: 0.999958] [G loss: 1.000070]\n",
      "76710 [D loss: 0.999964] [G loss: 1.000055]\n",
      "76720 [D loss: 0.999965] [G loss: 1.000068]\n",
      "76730 [D loss: 0.999972] [G loss: 1.000063]\n",
      "76740 [D loss: 0.999961] [G loss: 1.000064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76750 [D loss: 0.999963] [G loss: 1.000044]\n",
      "76760 [D loss: 0.999960] [G loss: 1.000069]\n",
      "76770 [D loss: 0.999962] [G loss: 1.000075]\n",
      "76780 [D loss: 0.999972] [G loss: 1.000049]\n",
      "76790 [D loss: 0.999962] [G loss: 1.000056]\n",
      "76800 [D loss: 0.999981] [G loss: 1.000073]\n",
      "76810 [D loss: 0.999974] [G loss: 1.000059]\n",
      "76820 [D loss: 0.999974] [G loss: 1.000075]\n",
      "76830 [D loss: 0.999972] [G loss: 1.000059]\n",
      "76840 [D loss: 0.999962] [G loss: 1.000059]\n",
      "76850 [D loss: 0.999970] [G loss: 1.000067]\n",
      "76860 [D loss: 0.999970] [G loss: 1.000055]\n",
      "76870 [D loss: 0.999985] [G loss: 1.000071]\n",
      "76880 [D loss: 0.999965] [G loss: 1.000060]\n",
      "76890 [D loss: 0.999966] [G loss: 1.000061]\n",
      "76900 [D loss: 0.999974] [G loss: 1.000062]\n",
      "76910 [D loss: 0.999962] [G loss: 1.000051]\n",
      "76920 [D loss: 0.999970] [G loss: 1.000062]\n",
      "76930 [D loss: 0.999970] [G loss: 1.000064]\n",
      "76940 [D loss: 0.999974] [G loss: 1.000042]\n",
      "76950 [D loss: 0.999965] [G loss: 1.000079]\n",
      "76960 [D loss: 0.999958] [G loss: 1.000069]\n",
      "76970 [D loss: 0.999967] [G loss: 1.000038]\n",
      "76980 [D loss: 0.999954] [G loss: 1.000072]\n",
      "76990 [D loss: 0.999957] [G loss: 1.000060]\n",
      "77000 [D loss: 0.999978] [G loss: 1.000045]\n",
      "77010 [D loss: 0.999986] [G loss: 1.000053]\n",
      "77020 [D loss: 0.999962] [G loss: 1.000075]\n",
      "77030 [D loss: 0.999954] [G loss: 1.000048]\n",
      "77040 [D loss: 0.999976] [G loss: 1.000063]\n",
      "77050 [D loss: 0.999965] [G loss: 1.000068]\n",
      "77060 [D loss: 0.999970] [G loss: 1.000055]\n",
      "77070 [D loss: 0.999959] [G loss: 1.000062]\n",
      "77080 [D loss: 0.999959] [G loss: 1.000066]\n",
      "77090 [D loss: 0.999960] [G loss: 1.000048]\n",
      "77100 [D loss: 0.999962] [G loss: 1.000063]\n",
      "77110 [D loss: 0.999958] [G loss: 1.000054]\n",
      "77120 [D loss: 0.999965] [G loss: 1.000056]\n",
      "77130 [D loss: 0.999976] [G loss: 1.000074]\n",
      "77140 [D loss: 0.999966] [G loss: 1.000051]\n",
      "77150 [D loss: 0.999969] [G loss: 1.000070]\n",
      "77160 [D loss: 0.999944] [G loss: 1.000062]\n",
      "77170 [D loss: 0.999965] [G loss: 1.000071]\n",
      "77180 [D loss: 0.999963] [G loss: 1.000058]\n",
      "77190 [D loss: 0.999954] [G loss: 1.000070]\n",
      "77200 [D loss: 0.999969] [G loss: 1.000054]\n",
      "77210 [D loss: 0.999979] [G loss: 1.000059]\n",
      "77220 [D loss: 0.999967] [G loss: 1.000072]\n",
      "77230 [D loss: 0.999954] [G loss: 1.000054]\n",
      "77240 [D loss: 0.999957] [G loss: 1.000076]\n",
      "77250 [D loss: 0.999978] [G loss: 1.000064]\n",
      "77260 [D loss: 0.999954] [G loss: 1.000044]\n",
      "77270 [D loss: 0.999981] [G loss: 1.000077]\n",
      "77280 [D loss: 0.999968] [G loss: 1.000060]\n",
      "77290 [D loss: 0.999967] [G loss: 1.000055]\n",
      "77300 [D loss: 0.999960] [G loss: 1.000062]\n",
      "77310 [D loss: 0.999963] [G loss: 1.000051]\n",
      "77320 [D loss: 0.999957] [G loss: 1.000061]\n",
      "77330 [D loss: 0.999964] [G loss: 1.000046]\n",
      "77340 [D loss: 0.999979] [G loss: 1.000057]\n",
      "77350 [D loss: 0.999966] [G loss: 1.000041]\n",
      "77360 [D loss: 0.999972] [G loss: 1.000064]\n",
      "77370 [D loss: 0.999987] [G loss: 1.000060]\n",
      "77380 [D loss: 0.999964] [G loss: 1.000065]\n",
      "77390 [D loss: 0.999967] [G loss: 1.000065]\n",
      "77400 [D loss: 0.999974] [G loss: 1.000071]\n",
      "77410 [D loss: 0.999967] [G loss: 1.000054]\n",
      "77420 [D loss: 0.999966] [G loss: 1.000049]\n",
      "77430 [D loss: 0.999971] [G loss: 1.000057]\n",
      "77440 [D loss: 0.999965] [G loss: 1.000052]\n",
      "77450 [D loss: 0.999964] [G loss: 1.000066]\n",
      "77460 [D loss: 0.999961] [G loss: 1.000067]\n",
      "77470 [D loss: 0.999972] [G loss: 1.000049]\n",
      "77480 [D loss: 0.999953] [G loss: 1.000055]\n",
      "77490 [D loss: 0.999975] [G loss: 1.000076]\n",
      "77500 [D loss: 0.999960] [G loss: 1.000046]\n",
      "77510 [D loss: 0.999952] [G loss: 1.000038]\n",
      "77520 [D loss: 0.999968] [G loss: 1.000056]\n",
      "77530 [D loss: 0.999963] [G loss: 1.000054]\n",
      "77540 [D loss: 0.999974] [G loss: 1.000068]\n",
      "77550 [D loss: 0.999967] [G loss: 1.000055]\n",
      "77560 [D loss: 0.999960] [G loss: 1.000099]\n",
      "77570 [D loss: 0.999971] [G loss: 1.000103]\n",
      "77580 [D loss: 0.999971] [G loss: 1.000076]\n",
      "77590 [D loss: 0.999971] [G loss: 1.000036]\n",
      "77600 [D loss: 0.999976] [G loss: 1.000053]\n",
      "77610 [D loss: 0.999960] [G loss: 1.000049]\n",
      "77620 [D loss: 0.999987] [G loss: 1.000069]\n",
      "77630 [D loss: 0.999968] [G loss: 1.000061]\n",
      "77640 [D loss: 0.999949] [G loss: 1.000089]\n",
      "77650 [D loss: 0.999977] [G loss: 1.000056]\n",
      "77660 [D loss: 0.999953] [G loss: 1.000057]\n",
      "77670 [D loss: 0.999963] [G loss: 1.000073]\n",
      "77680 [D loss: 0.999963] [G loss: 1.000044]\n",
      "77690 [D loss: 0.999957] [G loss: 1.000052]\n",
      "77700 [D loss: 0.999948] [G loss: 1.000068]\n",
      "77710 [D loss: 0.999959] [G loss: 1.000048]\n",
      "77720 [D loss: 0.999970] [G loss: 1.000064]\n",
      "77730 [D loss: 0.999962] [G loss: 1.000047]\n",
      "77740 [D loss: 0.999962] [G loss: 1.000062]\n",
      "77750 [D loss: 0.999949] [G loss: 1.000068]\n",
      "77760 [D loss: 0.999981] [G loss: 1.000046]\n",
      "77770 [D loss: 0.999970] [G loss: 1.000037]\n",
      "77780 [D loss: 0.999968] [G loss: 1.000065]\n",
      "77790 [D loss: 0.999968] [G loss: 1.000072]\n",
      "77800 [D loss: 0.999980] [G loss: 1.000073]\n",
      "77810 [D loss: 0.999974] [G loss: 1.000081]\n",
      "77820 [D loss: 0.999971] [G loss: 1.000064]\n",
      "77830 [D loss: 0.999967] [G loss: 1.000072]\n",
      "77840 [D loss: 0.999959] [G loss: 1.000065]\n",
      "77850 [D loss: 0.999963] [G loss: 1.000055]\n",
      "77860 [D loss: 0.999965] [G loss: 1.000073]\n",
      "77870 [D loss: 0.999953] [G loss: 1.000067]\n",
      "77880 [D loss: 0.999969] [G loss: 1.000066]\n",
      "77890 [D loss: 0.999969] [G loss: 1.000058]\n",
      "77900 [D loss: 0.999967] [G loss: 1.000077]\n",
      "77910 [D loss: 0.999973] [G loss: 1.000070]\n",
      "77920 [D loss: 0.999969] [G loss: 1.000078]\n",
      "77930 [D loss: 0.999972] [G loss: 1.000054]\n",
      "77940 [D loss: 0.999974] [G loss: 1.000057]\n",
      "77950 [D loss: 0.999991] [G loss: 1.000053]\n",
      "77960 [D loss: 0.999955] [G loss: 1.000029]\n",
      "77970 [D loss: 0.999958] [G loss: 1.000044]\n",
      "77980 [D loss: 0.999962] [G loss: 1.000070]\n",
      "77990 [D loss: 0.999946] [G loss: 1.000069]\n",
      "78000 [D loss: 0.999970] [G loss: 1.000056]\n",
      "78010 [D loss: 0.999971] [G loss: 1.000079]\n",
      "78020 [D loss: 0.999976] [G loss: 1.000055]\n",
      "78030 [D loss: 0.999947] [G loss: 1.000048]\n",
      "78040 [D loss: 0.999965] [G loss: 1.000078]\n",
      "78050 [D loss: 0.999955] [G loss: 1.000076]\n",
      "78060 [D loss: 0.999937] [G loss: 1.000071]\n",
      "78070 [D loss: 0.999966] [G loss: 1.000092]\n",
      "78080 [D loss: 0.999968] [G loss: 1.000053]\n",
      "78090 [D loss: 0.999959] [G loss: 1.000007]\n",
      "78100 [D loss: 0.999973] [G loss: 1.000066]\n",
      "78110 [D loss: 0.999975] [G loss: 1.000081]\n",
      "78120 [D loss: 0.999968] [G loss: 1.000085]\n",
      "78130 [D loss: 0.999961] [G loss: 1.000043]\n",
      "78140 [D loss: 0.999975] [G loss: 1.000059]\n",
      "78150 [D loss: 0.999980] [G loss: 1.000055]\n",
      "78160 [D loss: 0.999958] [G loss: 1.000033]\n",
      "78170 [D loss: 0.999978] [G loss: 1.000068]\n",
      "78180 [D loss: 0.999970] [G loss: 1.000086]\n",
      "78190 [D loss: 0.999960] [G loss: 1.000064]\n",
      "78200 [D loss: 0.999970] [G loss: 1.000050]\n",
      "78210 [D loss: 0.999956] [G loss: 1.000035]\n",
      "78220 [D loss: 0.999972] [G loss: 1.000045]\n",
      "78230 [D loss: 0.999982] [G loss: 1.000097]\n",
      "78240 [D loss: 0.999945] [G loss: 1.000062]\n",
      "78250 [D loss: 0.999975] [G loss: 1.000049]\n",
      "78260 [D loss: 0.999965] [G loss: 1.000067]\n",
      "78270 [D loss: 0.999966] [G loss: 1.000090]\n",
      "78280 [D loss: 0.999973] [G loss: 1.000077]\n",
      "78290 [D loss: 0.999935] [G loss: 1.000062]\n",
      "78300 [D loss: 0.999949] [G loss: 1.000060]\n",
      "78310 [D loss: 0.999965] [G loss: 1.000043]\n",
      "78320 [D loss: 0.999966] [G loss: 1.000081]\n",
      "78330 [D loss: 0.999939] [G loss: 1.000051]\n",
      "78340 [D loss: 0.999947] [G loss: 1.000047]\n",
      "78350 [D loss: 0.999963] [G loss: 1.000072]\n",
      "78360 [D loss: 0.999966] [G loss: 1.000049]\n",
      "78370 [D loss: 0.999971] [G loss: 1.000072]\n",
      "78380 [D loss: 0.999960] [G loss: 1.000035]\n",
      "78390 [D loss: 0.999972] [G loss: 1.000063]\n",
      "78400 [D loss: 0.999966] [G loss: 1.000059]\n",
      "78410 [D loss: 0.999956] [G loss: 1.000070]\n",
      "78420 [D loss: 0.999987] [G loss: 1.000032]\n",
      "78430 [D loss: 0.999955] [G loss: 1.000100]\n",
      "78440 [D loss: 0.999969] [G loss: 1.000029]\n",
      "78450 [D loss: 0.999972] [G loss: 1.000041]\n",
      "78460 [D loss: 0.999951] [G loss: 1.000037]\n",
      "78470 [D loss: 0.999968] [G loss: 1.000032]\n",
      "78480 [D loss: 0.999970] [G loss: 1.000068]\n",
      "78490 [D loss: 0.999966] [G loss: 1.000017]\n",
      "78500 [D loss: 0.999975] [G loss: 1.000069]\n",
      "78510 [D loss: 0.999964] [G loss: 1.000070]\n",
      "78520 [D loss: 0.999977] [G loss: 1.000053]\n",
      "78530 [D loss: 0.999974] [G loss: 1.000052]\n",
      "78540 [D loss: 0.999976] [G loss: 1.000044]\n",
      "78550 [D loss: 0.999957] [G loss: 1.000066]\n",
      "78560 [D loss: 0.999995] [G loss: 1.000032]\n",
      "78570 [D loss: 0.999953] [G loss: 1.000089]\n",
      "78580 [D loss: 0.999992] [G loss: 1.000039]\n",
      "78590 [D loss: 0.999951] [G loss: 1.000030]\n",
      "78600 [D loss: 0.999964] [G loss: 1.000054]\n",
      "78610 [D loss: 0.999959] [G loss: 1.000052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78620 [D loss: 0.999978] [G loss: 1.000109]\n",
      "78630 [D loss: 0.999977] [G loss: 1.000068]\n",
      "78640 [D loss: 0.999974] [G loss: 1.000064]\n",
      "78650 [D loss: 0.999914] [G loss: 1.000066]\n",
      "78660 [D loss: 0.999976] [G loss: 1.000095]\n",
      "78670 [D loss: 0.999974] [G loss: 1.000107]\n",
      "78680 [D loss: 0.999980] [G loss: 1.000019]\n",
      "78690 [D loss: 0.999972] [G loss: 1.000052]\n",
      "78700 [D loss: 0.999969] [G loss: 1.000058]\n",
      "78710 [D loss: 0.999953] [G loss: 1.000082]\n",
      "78720 [D loss: 0.999974] [G loss: 1.000051]\n",
      "78730 [D loss: 0.999948] [G loss: 1.000078]\n",
      "78740 [D loss: 0.999966] [G loss: 1.000067]\n",
      "78750 [D loss: 0.999983] [G loss: 1.000034]\n",
      "78760 [D loss: 0.999951] [G loss: 1.000093]\n",
      "78770 [D loss: 0.999983] [G loss: 1.000070]\n",
      "78780 [D loss: 0.999955] [G loss: 1.000071]\n",
      "78790 [D loss: 0.999958] [G loss: 1.000076]\n",
      "78800 [D loss: 0.999995] [G loss: 1.000038]\n",
      "78810 [D loss: 0.999963] [G loss: 1.000081]\n",
      "78820 [D loss: 0.999952] [G loss: 1.000047]\n",
      "78830 [D loss: 0.999959] [G loss: 1.000083]\n",
      "78840 [D loss: 0.999966] [G loss: 1.000069]\n",
      "78850 [D loss: 0.999964] [G loss: 1.000034]\n",
      "78860 [D loss: 0.999947] [G loss: 1.000056]\n",
      "78870 [D loss: 0.999962] [G loss: 1.000071]\n",
      "78880 [D loss: 0.999965] [G loss: 1.000059]\n",
      "78890 [D loss: 0.999947] [G loss: 1.000025]\n",
      "78900 [D loss: 0.999977] [G loss: 1.000055]\n",
      "78910 [D loss: 0.999970] [G loss: 1.000057]\n",
      "78920 [D loss: 0.999973] [G loss: 1.000046]\n",
      "78930 [D loss: 0.999957] [G loss: 1.000052]\n",
      "78940 [D loss: 0.999960] [G loss: 1.000061]\n",
      "78950 [D loss: 0.999969] [G loss: 1.000064]\n",
      "78960 [D loss: 0.999967] [G loss: 1.000047]\n",
      "78970 [D loss: 0.999943] [G loss: 1.000071]\n",
      "78980 [D loss: 0.999961] [G loss: 1.000074]\n",
      "78990 [D loss: 0.999972] [G loss: 1.000059]\n",
      "79000 [D loss: 0.999954] [G loss: 1.000043]\n",
      "79010 [D loss: 0.999962] [G loss: 1.000062]\n",
      "79020 [D loss: 0.999975] [G loss: 1.000051]\n",
      "79030 [D loss: 0.999962] [G loss: 1.000065]\n",
      "79040 [D loss: 0.999963] [G loss: 1.000065]\n",
      "79050 [D loss: 0.999943] [G loss: 1.000084]\n",
      "79060 [D loss: 0.999980] [G loss: 1.000067]\n",
      "79070 [D loss: 0.999963] [G loss: 1.000081]\n",
      "79080 [D loss: 0.999973] [G loss: 1.000085]\n",
      "79090 [D loss: 0.999964] [G loss: 1.000071]\n",
      "79100 [D loss: 0.999963] [G loss: 1.000063]\n",
      "79110 [D loss: 0.999966] [G loss: 1.000066]\n",
      "79120 [D loss: 0.999952] [G loss: 1.000058]\n",
      "79130 [D loss: 0.999976] [G loss: 1.000059]\n",
      "79140 [D loss: 0.999989] [G loss: 1.000021]\n",
      "79150 [D loss: 0.999953] [G loss: 1.000063]\n",
      "79160 [D loss: 0.999972] [G loss: 1.000093]\n",
      "79170 [D loss: 0.999962] [G loss: 1.000070]\n",
      "79180 [D loss: 0.999976] [G loss: 1.000048]\n",
      "79190 [D loss: 0.999978] [G loss: 1.000052]\n",
      "79200 [D loss: 0.999963] [G loss: 1.000055]\n",
      "79210 [D loss: 0.999949] [G loss: 1.000061]\n",
      "79220 [D loss: 0.999977] [G loss: 1.000057]\n",
      "79230 [D loss: 0.999972] [G loss: 1.000060]\n",
      "79240 [D loss: 0.999984] [G loss: 1.000032]\n",
      "79250 [D loss: 0.999973] [G loss: 1.000049]\n",
      "79260 [D loss: 0.999963] [G loss: 1.000061]\n",
      "79270 [D loss: 0.999981] [G loss: 1.000076]\n",
      "79280 [D loss: 0.999956] [G loss: 1.000077]\n",
      "79290 [D loss: 0.999956] [G loss: 1.000077]\n",
      "79300 [D loss: 0.999977] [G loss: 1.000036]\n",
      "79310 [D loss: 0.999962] [G loss: 1.000082]\n",
      "79320 [D loss: 0.999987] [G loss: 1.000055]\n",
      "79330 [D loss: 0.999966] [G loss: 1.000069]\n",
      "79340 [D loss: 0.999956] [G loss: 1.000067]\n",
      "79350 [D loss: 0.999964] [G loss: 1.000054]\n",
      "79360 [D loss: 0.999943] [G loss: 1.000062]\n",
      "79370 [D loss: 0.999966] [G loss: 1.000065]\n",
      "79380 [D loss: 0.999964] [G loss: 1.000068]\n",
      "79390 [D loss: 0.999965] [G loss: 1.000054]\n",
      "79400 [D loss: 0.999957] [G loss: 1.000071]\n",
      "79410 [D loss: 0.999952] [G loss: 1.000075]\n",
      "79420 [D loss: 0.999958] [G loss: 1.000100]\n",
      "79430 [D loss: 0.999949] [G loss: 1.000043]\n",
      "79440 [D loss: 0.999975] [G loss: 1.000055]\n",
      "79450 [D loss: 0.999963] [G loss: 1.000050]\n",
      "79460 [D loss: 0.999974] [G loss: 1.000096]\n",
      "79470 [D loss: 0.999967] [G loss: 1.000084]\n",
      "79480 [D loss: 0.999972] [G loss: 1.000038]\n",
      "79490 [D loss: 0.999952] [G loss: 1.000050]\n",
      "79500 [D loss: 0.999959] [G loss: 1.000065]\n",
      "79510 [D loss: 0.999965] [G loss: 1.000061]\n",
      "79520 [D loss: 0.999963] [G loss: 1.000060]\n",
      "79530 [D loss: 0.999962] [G loss: 1.000068]\n",
      "79540 [D loss: 0.999958] [G loss: 1.000074]\n",
      "79550 [D loss: 0.999958] [G loss: 1.000057]\n",
      "79560 [D loss: 0.999961] [G loss: 1.000040]\n",
      "79570 [D loss: 0.999966] [G loss: 1.000069]\n",
      "79580 [D loss: 0.999968] [G loss: 1.000074]\n",
      "79590 [D loss: 0.999957] [G loss: 1.000078]\n",
      "79600 [D loss: 0.999955] [G loss: 1.000074]\n",
      "79610 [D loss: 0.999975] [G loss: 1.000059]\n",
      "79620 [D loss: 0.999978] [G loss: 1.000075]\n",
      "79630 [D loss: 0.999953] [G loss: 1.000069]\n",
      "79640 [D loss: 0.999964] [G loss: 1.000066]\n",
      "79650 [D loss: 0.999966] [G loss: 1.000077]\n",
      "79660 [D loss: 0.999960] [G loss: 1.000063]\n",
      "79670 [D loss: 0.999973] [G loss: 1.000055]\n",
      "79680 [D loss: 0.999961] [G loss: 1.000051]\n",
      "79690 [D loss: 0.999972] [G loss: 1.000066]\n",
      "79700 [D loss: 0.999970] [G loss: 1.000062]\n",
      "79710 [D loss: 0.999986] [G loss: 1.000050]\n",
      "79720 [D loss: 0.999979] [G loss: 1.000048]\n",
      "79730 [D loss: 0.999981] [G loss: 1.000064]\n",
      "79740 [D loss: 0.999963] [G loss: 1.000046]\n",
      "79750 [D loss: 0.999971] [G loss: 1.000057]\n",
      "79760 [D loss: 0.999959] [G loss: 1.000042]\n",
      "79770 [D loss: 0.999948] [G loss: 1.000053]\n",
      "79780 [D loss: 0.999971] [G loss: 1.000066]\n",
      "79790 [D loss: 0.999970] [G loss: 1.000060]\n",
      "79800 [D loss: 0.999958] [G loss: 1.000032]\n",
      "79810 [D loss: 0.999970] [G loss: 1.000050]\n",
      "79820 [D loss: 0.999983] [G loss: 1.000055]\n",
      "79830 [D loss: 0.999964] [G loss: 1.000070]\n",
      "79840 [D loss: 0.999987] [G loss: 1.000056]\n",
      "79850 [D loss: 0.999962] [G loss: 1.000067]\n",
      "79860 [D loss: 0.999967] [G loss: 1.000074]\n",
      "79870 [D loss: 0.999983] [G loss: 1.000047]\n",
      "79880 [D loss: 0.999967] [G loss: 1.000048]\n",
      "79890 [D loss: 0.999964] [G loss: 1.000070]\n",
      "79900 [D loss: 0.999989] [G loss: 1.000084]\n",
      "79910 [D loss: 0.999953] [G loss: 1.000068]\n",
      "79920 [D loss: 0.999980] [G loss: 1.000050]\n",
      "79930 [D loss: 0.999983] [G loss: 1.000012]\n",
      "79940 [D loss: 0.999970] [G loss: 1.000062]\n",
      "79950 [D loss: 0.999965] [G loss: 1.000056]\n",
      "79960 [D loss: 0.999943] [G loss: 1.000065]\n",
      "79970 [D loss: 0.999999] [G loss: 1.000020]\n",
      "79980 [D loss: 0.999974] [G loss: 1.000063]\n",
      "79990 [D loss: 0.999959] [G loss: 1.000058]\n",
      "80000 [D loss: 0.999949] [G loss: 1.000095]\n",
      "80010 [D loss: 0.999976] [G loss: 1.000045]\n",
      "80020 [D loss: 0.999954] [G loss: 1.000046]\n",
      "80030 [D loss: 0.999978] [G loss: 1.000064]\n",
      "80040 [D loss: 0.999985] [G loss: 1.000065]\n",
      "80050 [D loss: 0.999964] [G loss: 1.000042]\n",
      "80060 [D loss: 0.999965] [G loss: 1.000047]\n",
      "80070 [D loss: 0.999960] [G loss: 1.000053]\n",
      "80080 [D loss: 0.999964] [G loss: 1.000078]\n",
      "80090 [D loss: 0.999954] [G loss: 1.000067]\n",
      "80100 [D loss: 0.999980] [G loss: 1.000053]\n",
      "80110 [D loss: 0.999970] [G loss: 1.000085]\n",
      "80120 [D loss: 0.999985] [G loss: 1.000073]\n",
      "80130 [D loss: 0.999981] [G loss: 1.000048]\n",
      "80140 [D loss: 0.999984] [G loss: 1.000063]\n",
      "80150 [D loss: 0.999975] [G loss: 1.000079]\n",
      "80160 [D loss: 0.999998] [G loss: 1.000078]\n",
      "80170 [D loss: 0.999975] [G loss: 1.000039]\n",
      "80180 [D loss: 0.999989] [G loss: 1.000054]\n",
      "80190 [D loss: 0.999962] [G loss: 1.000070]\n",
      "80200 [D loss: 0.999964] [G loss: 1.000132]\n",
      "80210 [D loss: 0.999985] [G loss: 1.000020]\n",
      "80220 [D loss: 0.999961] [G loss: 1.000058]\n",
      "80230 [D loss: 0.999971] [G loss: 1.000055]\n",
      "80240 [D loss: 0.999974] [G loss: 1.000054]\n",
      "80250 [D loss: 0.999970] [G loss: 1.000057]\n",
      "80260 [D loss: 0.999983] [G loss: 1.000042]\n",
      "80270 [D loss: 0.999980] [G loss: 1.000063]\n",
      "80280 [D loss: 0.999961] [G loss: 1.000080]\n",
      "80290 [D loss: 0.999988] [G loss: 1.000053]\n",
      "80300 [D loss: 0.999956] [G loss: 1.000068]\n",
      "80310 [D loss: 0.999972] [G loss: 1.000051]\n",
      "80320 [D loss: 0.999955] [G loss: 1.000072]\n",
      "80330 [D loss: 0.999983] [G loss: 1.000075]\n",
      "80340 [D loss: 0.999955] [G loss: 1.000050]\n",
      "80350 [D loss: 0.999979] [G loss: 1.000082]\n",
      "80360 [D loss: 0.999965] [G loss: 1.000078]\n",
      "80370 [D loss: 0.999980] [G loss: 1.000050]\n",
      "80380 [D loss: 0.999962] [G loss: 1.000079]\n",
      "80390 [D loss: 0.999948] [G loss: 1.000067]\n",
      "80400 [D loss: 0.999972] [G loss: 1.000060]\n",
      "80410 [D loss: 0.999980] [G loss: 1.000060]\n",
      "80420 [D loss: 0.999982] [G loss: 1.000065]\n",
      "80430 [D loss: 0.999967] [G loss: 1.000090]\n",
      "80440 [D loss: 0.999961] [G loss: 1.000084]\n",
      "80450 [D loss: 0.999981] [G loss: 1.000077]\n",
      "80460 [D loss: 0.999964] [G loss: 1.000096]\n",
      "80470 [D loss: 0.999978] [G loss: 1.000121]\n",
      "80480 [D loss: 0.999962] [G loss: 1.000069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80490 [D loss: 0.999959] [G loss: 1.000085]\n",
      "80500 [D loss: 0.999943] [G loss: 1.000085]\n",
      "80510 [D loss: 1.000011] [G loss: 1.000026]\n",
      "80520 [D loss: 0.999956] [G loss: 1.000035]\n",
      "80530 [D loss: 0.999952] [G loss: 1.000089]\n",
      "80540 [D loss: 0.999948] [G loss: 1.000073]\n",
      "80550 [D loss: 0.999984] [G loss: 1.000010]\n",
      "80560 [D loss: 0.999958] [G loss: 1.000062]\n",
      "80570 [D loss: 0.999956] [G loss: 1.000088]\n",
      "80580 [D loss: 0.999986] [G loss: 1.000056]\n",
      "80590 [D loss: 0.999951] [G loss: 1.000068]\n",
      "80600 [D loss: 0.999962] [G loss: 1.000043]\n",
      "80610 [D loss: 0.999961] [G loss: 1.000059]\n",
      "80620 [D loss: 0.999977] [G loss: 1.000038]\n",
      "80630 [D loss: 0.999973] [G loss: 1.000073]\n",
      "80640 [D loss: 0.999965] [G loss: 1.000073]\n",
      "80650 [D loss: 0.999959] [G loss: 1.000060]\n",
      "80660 [D loss: 0.999961] [G loss: 1.000066]\n",
      "80670 [D loss: 0.999959] [G loss: 1.000094]\n",
      "80680 [D loss: 0.999960] [G loss: 1.000060]\n",
      "80690 [D loss: 0.999961] [G loss: 1.000091]\n",
      "80700 [D loss: 0.999981] [G loss: 1.000085]\n",
      "80710 [D loss: 0.999976] [G loss: 1.000077]\n",
      "80720 [D loss: 0.999954] [G loss: 1.000076]\n",
      "80730 [D loss: 0.999987] [G loss: 1.000030]\n",
      "80740 [D loss: 0.999966] [G loss: 1.000040]\n",
      "80750 [D loss: 0.999947] [G loss: 1.000075]\n",
      "80760 [D loss: 0.999969] [G loss: 1.000075]\n",
      "80770 [D loss: 0.999991] [G loss: 1.000039]\n",
      "80780 [D loss: 0.999962] [G loss: 1.000086]\n",
      "80790 [D loss: 0.999970] [G loss: 1.000050]\n",
      "80800 [D loss: 0.999985] [G loss: 1.000055]\n",
      "80810 [D loss: 0.999970] [G loss: 1.000045]\n",
      "80820 [D loss: 0.999968] [G loss: 1.000034]\n",
      "80830 [D loss: 0.999993] [G loss: 1.000072]\n",
      "80840 [D loss: 0.999960] [G loss: 1.000073]\n",
      "80850 [D loss: 0.999974] [G loss: 1.000072]\n",
      "80860 [D loss: 0.999973] [G loss: 1.000033]\n",
      "80870 [D loss: 0.999933] [G loss: 1.000057]\n",
      "80880 [D loss: 0.999959] [G loss: 1.000058]\n",
      "80890 [D loss: 0.999970] [G loss: 1.000074]\n",
      "80900 [D loss: 0.999974] [G loss: 1.000093]\n",
      "80910 [D loss: 0.999962] [G loss: 1.000083]\n",
      "80920 [D loss: 0.999955] [G loss: 1.000068]\n",
      "80930 [D loss: 0.999972] [G loss: 1.000083]\n",
      "80940 [D loss: 0.999986] [G loss: 1.000044]\n",
      "80950 [D loss: 0.999952] [G loss: 1.000068]\n",
      "80960 [D loss: 0.999972] [G loss: 1.000069]\n",
      "80970 [D loss: 0.999967] [G loss: 1.000021]\n",
      "80980 [D loss: 0.999948] [G loss: 1.000067]\n",
      "80990 [D loss: 0.999980] [G loss: 1.000075]\n",
      "81000 [D loss: 0.999976] [G loss: 1.000076]\n",
      "81010 [D loss: 1.000002] [G loss: 1.000013]\n",
      "81020 [D loss: 0.999992] [G loss: 1.000046]\n",
      "81030 [D loss: 0.999958] [G loss: 1.000068]\n",
      "81040 [D loss: 0.999961] [G loss: 1.000113]\n",
      "81050 [D loss: 0.999968] [G loss: 1.000030]\n",
      "81060 [D loss: 0.999981] [G loss: 1.000053]\n",
      "81070 [D loss: 0.999984] [G loss: 0.999996]\n",
      "81080 [D loss: 0.999967] [G loss: 1.000071]\n",
      "81090 [D loss: 0.999962] [G loss: 1.000073]\n",
      "81100 [D loss: 0.999946] [G loss: 1.000110]\n",
      "81110 [D loss: 0.999974] [G loss: 1.000070]\n",
      "81120 [D loss: 0.999968] [G loss: 1.000084]\n",
      "81130 [D loss: 0.999984] [G loss: 1.000170]\n",
      "81140 [D loss: 0.999966] [G loss: 1.000047]\n",
      "81150 [D loss: 0.999973] [G loss: 1.000089]\n",
      "81160 [D loss: 0.999957] [G loss: 1.000060]\n",
      "81170 [D loss: 0.999958] [G loss: 1.000053]\n",
      "81180 [D loss: 0.999948] [G loss: 1.000105]\n",
      "81190 [D loss: 0.999970] [G loss: 1.000040]\n",
      "81200 [D loss: 0.999988] [G loss: 1.000074]\n",
      "81210 [D loss: 0.999956] [G loss: 1.000054]\n",
      "81220 [D loss: 0.999971] [G loss: 1.000086]\n",
      "81230 [D loss: 0.999985] [G loss: 1.000024]\n",
      "81240 [D loss: 0.999976] [G loss: 1.000084]\n",
      "81250 [D loss: 0.999961] [G loss: 1.000076]\n",
      "81260 [D loss: 0.999943] [G loss: 1.000073]\n",
      "81270 [D loss: 0.999997] [G loss: 1.000109]\n",
      "81280 [D loss: 0.999969] [G loss: 1.000065]\n",
      "81290 [D loss: 0.999959] [G loss: 1.000073]\n",
      "81300 [D loss: 0.999974] [G loss: 1.000081]\n",
      "81310 [D loss: 0.999967] [G loss: 1.000073]\n",
      "81320 [D loss: 0.999976] [G loss: 1.000016]\n",
      "81330 [D loss: 0.999951] [G loss: 1.000068]\n",
      "81340 [D loss: 0.999992] [G loss: 1.000084]\n",
      "81350 [D loss: 0.999971] [G loss: 1.000059]\n",
      "81360 [D loss: 0.999961] [G loss: 1.000075]\n",
      "81370 [D loss: 0.999961] [G loss: 1.000090]\n",
      "81380 [D loss: 0.999994] [G loss: 1.000069]\n",
      "81390 [D loss: 0.999964] [G loss: 1.000047]\n",
      "81400 [D loss: 0.999976] [G loss: 1.000057]\n",
      "81410 [D loss: 0.999960] [G loss: 1.000040]\n",
      "81420 [D loss: 0.999967] [G loss: 1.000044]\n",
      "81430 [D loss: 0.999972] [G loss: 1.000090]\n",
      "81440 [D loss: 0.999978] [G loss: 1.000060]\n",
      "81450 [D loss: 0.999963] [G loss: 1.000093]\n",
      "81460 [D loss: 0.999971] [G loss: 1.000119]\n",
      "81470 [D loss: 0.999952] [G loss: 1.000057]\n",
      "81480 [D loss: 0.999960] [G loss: 1.000091]\n",
      "81490 [D loss: 0.999971] [G loss: 1.000102]\n",
      "81500 [D loss: 0.999985] [G loss: 1.000028]\n",
      "81510 [D loss: 0.999972] [G loss: 1.000103]\n",
      "81520 [D loss: 1.000005] [G loss: 1.000034]\n",
      "81530 [D loss: 0.999969] [G loss: 1.000061]\n",
      "81540 [D loss: 0.999972] [G loss: 1.000079]\n",
      "81550 [D loss: 0.999963] [G loss: 1.000020]\n",
      "81560 [D loss: 0.999950] [G loss: 1.000069]\n",
      "81570 [D loss: 0.999970] [G loss: 1.000040]\n",
      "81580 [D loss: 0.999938] [G loss: 1.000048]\n",
      "81590 [D loss: 0.999949] [G loss: 1.000096]\n",
      "81600 [D loss: 0.999954] [G loss: 1.000082]\n",
      "81610 [D loss: 0.999966] [G loss: 1.000050]\n",
      "81620 [D loss: 0.999944] [G loss: 1.000051]\n",
      "81630 [D loss: 0.999973] [G loss: 1.000072]\n",
      "81640 [D loss: 0.999979] [G loss: 1.000061]\n",
      "81650 [D loss: 0.999943] [G loss: 1.000094]\n",
      "81660 [D loss: 0.999971] [G loss: 1.000004]\n",
      "81670 [D loss: 0.999969] [G loss: 1.000109]\n",
      "81680 [D loss: 0.999971] [G loss: 1.000120]\n",
      "81690 [D loss: 0.999969] [G loss: 1.000043]\n",
      "81700 [D loss: 0.999954] [G loss: 1.000100]\n",
      "81710 [D loss: 0.999991] [G loss: 1.000036]\n",
      "81720 [D loss: 0.999982] [G loss: 1.000033]\n",
      "81730 [D loss: 0.999965] [G loss: 1.000077]\n",
      "81740 [D loss: 0.999959] [G loss: 1.000036]\n",
      "81750 [D loss: 0.999993] [G loss: 1.000052]\n",
      "81760 [D loss: 0.999943] [G loss: 1.000081]\n",
      "81770 [D loss: 0.999998] [G loss: 1.000089]\n",
      "81780 [D loss: 0.999954] [G loss: 1.000047]\n",
      "81790 [D loss: 0.999973] [G loss: 1.000057]\n",
      "81800 [D loss: 0.999917] [G loss: 1.000036]\n",
      "81810 [D loss: 0.999973] [G loss: 1.000024]\n",
      "81820 [D loss: 0.999956] [G loss: 1.000089]\n",
      "81830 [D loss: 0.999980] [G loss: 1.000054]\n",
      "81840 [D loss: 0.999969] [G loss: 1.000087]\n",
      "81850 [D loss: 0.999977] [G loss: 1.000097]\n",
      "81860 [D loss: 0.999982] [G loss: 1.000070]\n",
      "81870 [D loss: 0.999971] [G loss: 1.000087]\n",
      "81880 [D loss: 0.999957] [G loss: 1.000082]\n",
      "81890 [D loss: 0.999955] [G loss: 1.000063]\n",
      "81900 [D loss: 0.999963] [G loss: 1.000087]\n",
      "81910 [D loss: 1.000012] [G loss: 1.000036]\n",
      "81920 [D loss: 0.999976] [G loss: 1.000087]\n",
      "81930 [D loss: 0.999953] [G loss: 1.000068]\n",
      "81940 [D loss: 0.999983] [G loss: 1.000038]\n",
      "81950 [D loss: 0.999960] [G loss: 1.000068]\n",
      "81960 [D loss: 0.999980] [G loss: 1.000071]\n",
      "81970 [D loss: 0.999969] [G loss: 1.000083]\n",
      "81980 [D loss: 0.999964] [G loss: 1.000111]\n",
      "81990 [D loss: 0.999968] [G loss: 1.000064]\n",
      "82000 [D loss: 0.999967] [G loss: 1.000068]\n",
      "82010 [D loss: 0.999967] [G loss: 1.000091]\n",
      "82020 [D loss: 0.999979] [G loss: 1.000066]\n",
      "82030 [D loss: 0.999965] [G loss: 1.000063]\n",
      "82040 [D loss: 0.999956] [G loss: 1.000088]\n",
      "82050 [D loss: 0.999978] [G loss: 1.000053]\n",
      "82060 [D loss: 0.999976] [G loss: 1.000055]\n",
      "82070 [D loss: 0.999961] [G loss: 1.000067]\n",
      "82080 [D loss: 0.999965] [G loss: 1.000071]\n",
      "82090 [D loss: 0.999983] [G loss: 1.000055]\n",
      "82100 [D loss: 0.999966] [G loss: 1.000033]\n",
      "82110 [D loss: 0.999994] [G loss: 1.000053]\n",
      "82120 [D loss: 0.999971] [G loss: 1.000068]\n",
      "82130 [D loss: 0.999955] [G loss: 1.000095]\n",
      "82140 [D loss: 0.999974] [G loss: 1.000097]\n",
      "82150 [D loss: 0.999960] [G loss: 1.000070]\n",
      "82160 [D loss: 0.999977] [G loss: 1.000086]\n",
      "82170 [D loss: 0.999955] [G loss: 1.000090]\n",
      "82180 [D loss: 0.999942] [G loss: 1.000048]\n",
      "82190 [D loss: 0.999982] [G loss: 1.000073]\n",
      "82200 [D loss: 0.999949] [G loss: 1.000081]\n",
      "82210 [D loss: 0.999967] [G loss: 1.000049]\n",
      "82220 [D loss: 0.999971] [G loss: 1.000064]\n",
      "82230 [D loss: 0.999987] [G loss: 1.000083]\n",
      "82240 [D loss: 0.999972] [G loss: 1.000042]\n",
      "82250 [D loss: 0.999976] [G loss: 1.000050]\n",
      "82260 [D loss: 0.999967] [G loss: 1.000078]\n",
      "82270 [D loss: 0.999967] [G loss: 1.000055]\n",
      "82280 [D loss: 0.999966] [G loss: 1.000043]\n",
      "82290 [D loss: 0.999967] [G loss: 1.000060]\n",
      "82300 [D loss: 0.999975] [G loss: 1.000080]\n",
      "82310 [D loss: 0.999942] [G loss: 1.000065]\n",
      "82320 [D loss: 0.999966] [G loss: 1.000056]\n",
      "82330 [D loss: 0.999985] [G loss: 1.000051]\n",
      "82340 [D loss: 0.999960] [G loss: 1.000083]\n",
      "82350 [D loss: 1.000003] [G loss: 1.000036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82360 [D loss: 0.999950] [G loss: 1.000083]\n",
      "82370 [D loss: 0.999942] [G loss: 1.000079]\n",
      "82380 [D loss: 0.999951] [G loss: 1.000048]\n",
      "82390 [D loss: 0.999965] [G loss: 1.000063]\n",
      "82400 [D loss: 0.999970] [G loss: 1.000046]\n",
      "82410 [D loss: 1.000000] [G loss: 1.000061]\n",
      "82420 [D loss: 0.999967] [G loss: 1.000050]\n",
      "82430 [D loss: 0.999980] [G loss: 1.000042]\n",
      "82440 [D loss: 0.999974] [G loss: 1.000037]\n",
      "82450 [D loss: 0.999961] [G loss: 1.000078]\n",
      "82460 [D loss: 0.999975] [G loss: 1.000058]\n",
      "82470 [D loss: 0.999967] [G loss: 1.000060]\n",
      "82480 [D loss: 0.999956] [G loss: 1.000037]\n",
      "82490 [D loss: 0.999975] [G loss: 1.000096]\n",
      "82500 [D loss: 0.999990] [G loss: 1.000066]\n",
      "82510 [D loss: 0.999951] [G loss: 1.000070]\n",
      "82520 [D loss: 0.999949] [G loss: 1.000062]\n",
      "82530 [D loss: 0.999999] [G loss: 1.000026]\n",
      "82540 [D loss: 0.999952] [G loss: 1.000071]\n",
      "82550 [D loss: 0.999962] [G loss: 1.000089]\n",
      "82560 [D loss: 0.999984] [G loss: 1.000043]\n",
      "82570 [D loss: 0.999971] [G loss: 1.000053]\n",
      "82580 [D loss: 0.999963] [G loss: 1.000111]\n",
      "82590 [D loss: 0.999988] [G loss: 1.000055]\n",
      "82600 [D loss: 0.999961] [G loss: 1.000079]\n",
      "82610 [D loss: 0.999956] [G loss: 1.000079]\n",
      "82620 [D loss: 0.999950] [G loss: 1.000063]\n",
      "82630 [D loss: 0.999973] [G loss: 1.000088]\n",
      "82640 [D loss: 0.999955] [G loss: 1.000051]\n",
      "82650 [D loss: 0.999952] [G loss: 1.000066]\n",
      "82660 [D loss: 0.999970] [G loss: 1.000071]\n",
      "82670 [D loss: 0.999964] [G loss: 1.000093]\n",
      "82680 [D loss: 1.000033] [G loss: 1.000057]\n",
      "82690 [D loss: 0.999987] [G loss: 1.000047]\n",
      "82700 [D loss: 0.999979] [G loss: 1.000062]\n",
      "82710 [D loss: 0.999922] [G loss: 1.000112]\n",
      "82720 [D loss: 1.000015] [G loss: 0.999993]\n",
      "82730 [D loss: 0.999960] [G loss: 1.000076]\n",
      "82740 [D loss: 0.999954] [G loss: 1.000088]\n",
      "82750 [D loss: 1.000019] [G loss: 1.000109]\n",
      "82760 [D loss: 0.999962] [G loss: 1.000090]\n",
      "82770 [D loss: 0.999950] [G loss: 1.000090]\n",
      "82780 [D loss: 0.999948] [G loss: 1.000039]\n",
      "82790 [D loss: 0.999969] [G loss: 1.000053]\n",
      "82800 [D loss: 0.999969] [G loss: 1.000092]\n",
      "82810 [D loss: 0.999967] [G loss: 1.000099]\n",
      "82820 [D loss: 0.999981] [G loss: 1.000043]\n",
      "82830 [D loss: 0.999957] [G loss: 1.000087]\n",
      "82840 [D loss: 0.999961] [G loss: 1.000061]\n",
      "82850 [D loss: 1.000009] [G loss: 0.999999]\n",
      "82860 [D loss: 0.999958] [G loss: 1.000074]\n",
      "82870 [D loss: 0.999952] [G loss: 1.000064]\n",
      "82880 [D loss: 0.999950] [G loss: 1.000099]\n",
      "82890 [D loss: 0.999999] [G loss: 1.000018]\n",
      "82900 [D loss: 0.999965] [G loss: 1.000074]\n",
      "82910 [D loss: 0.999956] [G loss: 1.000099]\n",
      "82920 [D loss: 0.999980] [G loss: 1.000004]\n",
      "82930 [D loss: 0.999986] [G loss: 1.000062]\n",
      "82940 [D loss: 0.999959] [G loss: 1.000120]\n",
      "82950 [D loss: 0.999933] [G loss: 1.000085]\n",
      "82960 [D loss: 0.999954] [G loss: 1.000091]\n",
      "82970 [D loss: 0.999944] [G loss: 1.000122]\n",
      "82980 [D loss: 0.999984] [G loss: 1.000016]\n",
      "82990 [D loss: 0.999973] [G loss: 1.000072]\n",
      "83000 [D loss: 0.999960] [G loss: 1.000112]\n",
      "83010 [D loss: 0.999986] [G loss: 1.000086]\n",
      "83020 [D loss: 1.000003] [G loss: 1.000006]\n",
      "83030 [D loss: 0.999984] [G loss: 1.000038]\n",
      "83040 [D loss: 0.999973] [G loss: 1.000071]\n",
      "83050 [D loss: 0.999958] [G loss: 1.000073]\n",
      "83060 [D loss: 0.999965] [G loss: 1.000048]\n",
      "83070 [D loss: 0.999981] [G loss: 0.999995]\n",
      "83080 [D loss: 0.999977] [G loss: 1.000062]\n",
      "83090 [D loss: 0.999963] [G loss: 1.000084]\n",
      "83100 [D loss: 0.999942] [G loss: 1.000040]\n",
      "83110 [D loss: 0.999975] [G loss: 1.000024]\n",
      "83120 [D loss: 0.999961] [G loss: 1.000038]\n",
      "83130 [D loss: 0.999996] [G loss: 1.000058]\n",
      "83140 [D loss: 0.999951] [G loss: 1.000085]\n",
      "83150 [D loss: 0.999972] [G loss: 1.000089]\n",
      "83160 [D loss: 0.999977] [G loss: 1.000096]\n",
      "83170 [D loss: 0.999958] [G loss: 1.000070]\n",
      "83180 [D loss: 0.999950] [G loss: 1.000089]\n",
      "83190 [D loss: 0.999983] [G loss: 1.000085]\n",
      "83200 [D loss: 0.999969] [G loss: 1.000097]\n",
      "83210 [D loss: 0.999962] [G loss: 1.000080]\n",
      "83220 [D loss: 0.999972] [G loss: 1.000095]\n",
      "83230 [D loss: 0.999936] [G loss: 1.000061]\n",
      "83240 [D loss: 0.999963] [G loss: 1.000095]\n",
      "83250 [D loss: 0.999931] [G loss: 1.000078]\n",
      "83260 [D loss: 0.999981] [G loss: 1.000027]\n",
      "83270 [D loss: 0.999954] [G loss: 1.000093]\n",
      "83280 [D loss: 0.999961] [G loss: 1.000065]\n",
      "83290 [D loss: 1.000006] [G loss: 1.000057]\n",
      "83300 [D loss: 0.999974] [G loss: 1.000026]\n",
      "83310 [D loss: 0.999953] [G loss: 1.000068]\n",
      "83320 [D loss: 0.999969] [G loss: 1.000054]\n",
      "83330 [D loss: 0.999973] [G loss: 1.000041]\n",
      "83340 [D loss: 0.999955] [G loss: 1.000074]\n",
      "83350 [D loss: 0.999957] [G loss: 1.000086]\n",
      "83360 [D loss: 0.999964] [G loss: 1.000054]\n",
      "83370 [D loss: 0.999954] [G loss: 1.000092]\n",
      "83380 [D loss: 0.999981] [G loss: 1.000076]\n",
      "83390 [D loss: 0.999947] [G loss: 1.000073]\n",
      "83400 [D loss: 0.999950] [G loss: 1.000087]\n",
      "83410 [D loss: 1.000044] [G loss: 1.000041]\n",
      "83420 [D loss: 0.999972] [G loss: 1.000085]\n",
      "83430 [D loss: 0.999941] [G loss: 1.000111]\n",
      "83440 [D loss: 0.999984] [G loss: 1.000026]\n",
      "83450 [D loss: 0.999962] [G loss: 1.000104]\n",
      "83460 [D loss: 0.999938] [G loss: 1.000095]\n",
      "83470 [D loss: 0.999972] [G loss: 0.999999]\n",
      "83480 [D loss: 0.999971] [G loss: 1.000092]\n",
      "83490 [D loss: 0.999950] [G loss: 1.000118]\n",
      "83500 [D loss: 0.999958] [G loss: 1.000032]\n",
      "83510 [D loss: 0.999954] [G loss: 1.000105]\n",
      "83520 [D loss: 0.999942] [G loss: 1.000071]\n",
      "83530 [D loss: 0.999958] [G loss: 1.000060]\n",
      "83540 [D loss: 0.999958] [G loss: 1.000062]\n",
      "83550 [D loss: 0.999967] [G loss: 1.000023]\n",
      "83560 [D loss: 0.999999] [G loss: 1.000008]\n",
      "83570 [D loss: 0.999973] [G loss: 1.000071]\n",
      "83580 [D loss: 0.999948] [G loss: 1.000082]\n",
      "83590 [D loss: 0.999973] [G loss: 1.000117]\n",
      "83600 [D loss: 0.999959] [G loss: 1.000077]\n",
      "83610 [D loss: 0.999989] [G loss: 1.000130]\n",
      "83620 [D loss: 0.999995] [G loss: 1.000039]\n",
      "83630 [D loss: 0.999961] [G loss: 1.000081]\n",
      "83640 [D loss: 0.999962] [G loss: 1.000134]\n",
      "83650 [D loss: 0.999958] [G loss: 1.000040]\n",
      "83660 [D loss: 0.999975] [G loss: 1.000021]\n",
      "83670 [D loss: 0.999957] [G loss: 1.000098]\n",
      "83680 [D loss: 0.999993] [G loss: 1.000084]\n",
      "83690 [D loss: 0.999970] [G loss: 1.000049]\n",
      "83700 [D loss: 0.999974] [G loss: 1.000073]\n",
      "83710 [D loss: 0.999943] [G loss: 1.000135]\n",
      "83720 [D loss: 0.999951] [G loss: 1.000063]\n",
      "83730 [D loss: 0.999958] [G loss: 1.000085]\n",
      "83740 [D loss: 0.999975] [G loss: 1.000132]\n",
      "83750 [D loss: 0.999999] [G loss: 1.000036]\n",
      "83760 [D loss: 0.999985] [G loss: 1.000034]\n",
      "83770 [D loss: 0.999974] [G loss: 1.000077]\n",
      "83780 [D loss: 0.999981] [G loss: 1.000122]\n",
      "83790 [D loss: 1.000011] [G loss: 1.000021]\n",
      "83800 [D loss: 0.999972] [G loss: 1.000111]\n",
      "83810 [D loss: 0.999975] [G loss: 1.000089]\n",
      "83820 [D loss: 0.999965] [G loss: 1.000054]\n",
      "83830 [D loss: 0.999981] [G loss: 1.000103]\n",
      "83840 [D loss: 0.999989] [G loss: 1.000081]\n",
      "83850 [D loss: 0.999974] [G loss: 1.000037]\n",
      "83860 [D loss: 0.999976] [G loss: 1.000050]\n",
      "83870 [D loss: 0.999952] [G loss: 1.000078]\n",
      "83880 [D loss: 0.999997] [G loss: 1.000036]\n",
      "83890 [D loss: 0.999977] [G loss: 1.000033]\n",
      "83900 [D loss: 0.999965] [G loss: 1.000103]\n",
      "83910 [D loss: 1.000003] [G loss: 1.000005]\n",
      "83920 [D loss: 1.000010] [G loss: 1.000038]\n",
      "83930 [D loss: 0.999980] [G loss: 1.000096]\n",
      "83940 [D loss: 0.999964] [G loss: 1.000087]\n",
      "83950 [D loss: 0.999967] [G loss: 1.000009]\n",
      "83960 [D loss: 0.999986] [G loss: 1.000048]\n",
      "83970 [D loss: 0.999947] [G loss: 1.000078]\n",
      "83980 [D loss: 0.999950] [G loss: 1.000085]\n",
      "83990 [D loss: 1.000014] [G loss: 1.000052]\n",
      "84000 [D loss: 0.999984] [G loss: 1.000112]\n",
      "84010 [D loss: 0.999964] [G loss: 1.000061]\n",
      "84020 [D loss: 0.999972] [G loss: 1.000121]\n",
      "84030 [D loss: 0.999968] [G loss: 1.000106]\n",
      "84040 [D loss: 0.999932] [G loss: 1.000045]\n",
      "84050 [D loss: 0.999973] [G loss: 1.000056]\n",
      "84060 [D loss: 0.999957] [G loss: 1.000057]\n",
      "84070 [D loss: 0.999947] [G loss: 1.000079]\n",
      "84080 [D loss: 0.999954] [G loss: 1.000077]\n",
      "84090 [D loss: 0.999983] [G loss: 1.000001]\n",
      "84100 [D loss: 0.999963] [G loss: 1.000091]\n",
      "84110 [D loss: 0.999943] [G loss: 1.000101]\n",
      "84120 [D loss: 0.999978] [G loss: 1.000031]\n",
      "84130 [D loss: 0.999945] [G loss: 1.000052]\n",
      "84140 [D loss: 0.999961] [G loss: 1.000090]\n",
      "84150 [D loss: 0.999973] [G loss: 1.000112]\n",
      "84160 [D loss: 0.999980] [G loss: 1.000061]\n",
      "84170 [D loss: 0.999991] [G loss: 1.000012]\n",
      "84180 [D loss: 0.999980] [G loss: 1.000074]\n",
      "84190 [D loss: 0.999951] [G loss: 1.000070]\n",
      "84200 [D loss: 0.999988] [G loss: 0.999978]\n",
      "84210 [D loss: 0.999959] [G loss: 1.000065]\n",
      "84220 [D loss: 0.999975] [G loss: 1.000079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84230 [D loss: 0.999956] [G loss: 1.000104]\n",
      "84240 [D loss: 0.999970] [G loss: 1.000113]\n",
      "84250 [D loss: 0.999969] [G loss: 1.000035]\n",
      "84260 [D loss: 0.999943] [G loss: 1.000039]\n",
      "84270 [D loss: 0.999966] [G loss: 1.000100]\n",
      "84280 [D loss: 0.999962] [G loss: 1.000090]\n",
      "84290 [D loss: 0.999968] [G loss: 1.000067]\n",
      "84300 [D loss: 0.999994] [G loss: 1.000048]\n",
      "84310 [D loss: 0.999968] [G loss: 1.000069]\n",
      "84320 [D loss: 0.999939] [G loss: 1.000020]\n",
      "84330 [D loss: 0.999998] [G loss: 1.000104]\n",
      "84340 [D loss: 0.999966] [G loss: 1.000063]\n",
      "84350 [D loss: 0.999978] [G loss: 1.000052]\n",
      "84360 [D loss: 0.999933] [G loss: 1.000079]\n",
      "84370 [D loss: 0.999990] [G loss: 0.999983]\n",
      "84380 [D loss: 0.999953] [G loss: 1.000074]\n",
      "84390 [D loss: 0.999964] [G loss: 1.000078]\n",
      "84400 [D loss: 0.999924] [G loss: 1.000150]\n",
      "84410 [D loss: 0.999991] [G loss: 1.000099]\n",
      "84420 [D loss: 0.999972] [G loss: 1.000093]\n",
      "84430 [D loss: 0.999934] [G loss: 1.000131]\n",
      "84440 [D loss: 0.999989] [G loss: 1.000121]\n",
      "84450 [D loss: 0.999974] [G loss: 1.000041]\n",
      "84460 [D loss: 0.999928] [G loss: 1.000096]\n",
      "84470 [D loss: 0.999946] [G loss: 1.000137]\n",
      "84480 [D loss: 1.000066] [G loss: 1.000064]\n",
      "84490 [D loss: 0.999959] [G loss: 1.000055]\n",
      "84500 [D loss: 0.999951] [G loss: 1.000114]\n",
      "84510 [D loss: 0.999940] [G loss: 1.000159]\n",
      "84520 [D loss: 1.000043] [G loss: 1.000035]\n",
      "84530 [D loss: 0.999999] [G loss: 1.000120]\n",
      "84540 [D loss: 0.999964] [G loss: 1.000105]\n",
      "84550 [D loss: 1.000002] [G loss: 1.000094]\n",
      "84560 [D loss: 0.999999] [G loss: 1.000045]\n",
      "84570 [D loss: 0.999941] [G loss: 1.000162]\n",
      "84580 [D loss: 0.999974] [G loss: 1.000143]\n",
      "84590 [D loss: 1.000022] [G loss: 0.999973]\n",
      "84600 [D loss: 0.999982] [G loss: 1.000095]\n",
      "84610 [D loss: 0.999976] [G loss: 1.000071]\n",
      "84620 [D loss: 0.999969] [G loss: 1.000050]\n",
      "84630 [D loss: 0.999965] [G loss: 1.000143]\n",
      "84640 [D loss: 0.999982] [G loss: 0.999996]\n",
      "84650 [D loss: 0.999942] [G loss: 1.000055]\n",
      "84660 [D loss: 0.999949] [G loss: 1.000089]\n",
      "84670 [D loss: 0.999927] [G loss: 1.000093]\n",
      "84680 [D loss: 1.000047] [G loss: 1.000040]\n",
      "84690 [D loss: 0.999992] [G loss: 1.000037]\n",
      "84700 [D loss: 0.999999] [G loss: 1.000144]\n",
      "84710 [D loss: 0.999965] [G loss: 1.000041]\n",
      "84720 [D loss: 0.999951] [G loss: 1.000067]\n",
      "84730 [D loss: 1.000002] [G loss: 1.000066]\n",
      "84740 [D loss: 0.999974] [G loss: 1.000032]\n",
      "84750 [D loss: 0.999958] [G loss: 1.000118]\n",
      "84760 [D loss: 0.999960] [G loss: 1.000093]\n",
      "84770 [D loss: 1.000003] [G loss: 0.999968]\n",
      "84780 [D loss: 0.999959] [G loss: 1.000097]\n",
      "84790 [D loss: 1.000008] [G loss: 1.000048]\n",
      "84800 [D loss: 0.999962] [G loss: 1.000011]\n",
      "84810 [D loss: 0.999948] [G loss: 1.000110]\n",
      "84820 [D loss: 0.999974] [G loss: 1.000186]\n",
      "84830 [D loss: 0.999978] [G loss: 0.999990]\n",
      "84840 [D loss: 0.999969] [G loss: 1.000087]\n",
      "84850 [D loss: 0.999964] [G loss: 1.000073]\n",
      "84860 [D loss: 0.999959] [G loss: 1.000030]\n",
      "84870 [D loss: 0.999959] [G loss: 1.000111]\n",
      "84880 [D loss: 0.999907] [G loss: 1.000116]\n",
      "84890 [D loss: 0.999972] [G loss: 1.000109]\n",
      "84900 [D loss: 0.999937] [G loss: 1.000114]\n",
      "84910 [D loss: 0.999973] [G loss: 1.000071]\n",
      "84920 [D loss: 1.000032] [G loss: 0.999961]\n",
      "84930 [D loss: 0.999981] [G loss: 1.000091]\n",
      "84940 [D loss: 0.999945] [G loss: 1.000081]\n",
      "84950 [D loss: 0.999960] [G loss: 1.000107]\n",
      "84960 [D loss: 0.999990] [G loss: 1.000031]\n",
      "84970 [D loss: 0.999989] [G loss: 1.000063]\n",
      "84980 [D loss: 0.999987] [G loss: 1.000110]\n",
      "84990 [D loss: 0.999987] [G loss: 1.000127]\n",
      "85000 [D loss: 0.999960] [G loss: 1.000064]\n",
      "85010 [D loss: 0.999937] [G loss: 1.000114]\n",
      "85020 [D loss: 1.000005] [G loss: 1.000061]\n",
      "85030 [D loss: 0.999955] [G loss: 1.000044]\n",
      "85040 [D loss: 0.999964] [G loss: 1.000091]\n",
      "85050 [D loss: 0.999935] [G loss: 1.000113]\n",
      "85060 [D loss: 0.999980] [G loss: 0.999998]\n",
      "85070 [D loss: 1.000004] [G loss: 1.000055]\n",
      "85080 [D loss: 0.999950] [G loss: 1.000046]\n",
      "85090 [D loss: 0.999947] [G loss: 1.000098]\n",
      "85100 [D loss: 0.999981] [G loss: 1.000065]\n",
      "85110 [D loss: 0.999964] [G loss: 1.000116]\n",
      "85120 [D loss: 0.999977] [G loss: 1.000110]\n",
      "85130 [D loss: 0.999936] [G loss: 1.000033]\n",
      "85140 [D loss: 0.999961] [G loss: 1.000085]\n",
      "85150 [D loss: 0.999961] [G loss: 0.999988]\n",
      "85160 [D loss: 0.999961] [G loss: 1.000089]\n",
      "85170 [D loss: 0.999945] [G loss: 1.000117]\n",
      "85180 [D loss: 0.999963] [G loss: 1.000017]\n",
      "85190 [D loss: 0.999967] [G loss: 1.000092]\n",
      "85200 [D loss: 0.999977] [G loss: 1.000029]\n",
      "85210 [D loss: 0.999956] [G loss: 1.000059]\n",
      "85220 [D loss: 0.999953] [G loss: 1.000094]\n",
      "85230 [D loss: 0.999969] [G loss: 1.000097]\n",
      "85240 [D loss: 0.999973] [G loss: 1.000059]\n",
      "85250 [D loss: 0.999963] [G loss: 1.000107]\n",
      "85260 [D loss: 0.999939] [G loss: 1.000096]\n",
      "85270 [D loss: 0.999984] [G loss: 1.000064]\n",
      "85280 [D loss: 0.999977] [G loss: 1.000063]\n",
      "85290 [D loss: 0.999950] [G loss: 1.000104]\n",
      "85300 [D loss: 0.999959] [G loss: 1.000050]\n",
      "85310 [D loss: 0.999972] [G loss: 1.000012]\n",
      "85320 [D loss: 0.999995] [G loss: 1.000045]\n",
      "85330 [D loss: 0.999955] [G loss: 1.000110]\n",
      "85340 [D loss: 0.999981] [G loss: 1.000013]\n",
      "85350 [D loss: 0.999975] [G loss: 1.000064]\n",
      "85360 [D loss: 0.999974] [G loss: 1.000061]\n",
      "85370 [D loss: 0.999964] [G loss: 1.000091]\n",
      "85380 [D loss: 0.999958] [G loss: 1.000078]\n",
      "85390 [D loss: 0.999968] [G loss: 1.000030]\n",
      "85400 [D loss: 0.999973] [G loss: 1.000072]\n",
      "85410 [D loss: 0.999966] [G loss: 1.000069]\n",
      "85420 [D loss: 0.999975] [G loss: 1.000031]\n",
      "85430 [D loss: 0.999969] [G loss: 1.000058]\n",
      "85440 [D loss: 0.999965] [G loss: 1.000099]\n",
      "85450 [D loss: 0.999989] [G loss: 1.000079]\n",
      "85460 [D loss: 0.999950] [G loss: 1.000041]\n",
      "85470 [D loss: 0.999970] [G loss: 1.000075]\n",
      "85480 [D loss: 0.999967] [G loss: 1.000083]\n",
      "85490 [D loss: 0.999979] [G loss: 1.000069]\n",
      "85500 [D loss: 0.999977] [G loss: 1.000079]\n",
      "85510 [D loss: 0.999957] [G loss: 1.000082]\n",
      "85520 [D loss: 0.999949] [G loss: 1.000059]\n",
      "85530 [D loss: 0.999979] [G loss: 1.000036]\n",
      "85540 [D loss: 0.999955] [G loss: 1.000011]\n",
      "85550 [D loss: 0.999975] [G loss: 1.000060]\n",
      "85560 [D loss: 0.999975] [G loss: 1.000020]\n",
      "85570 [D loss: 0.999965] [G loss: 1.000071]\n",
      "85580 [D loss: 0.999991] [G loss: 1.000090]\n",
      "85590 [D loss: 0.999959] [G loss: 1.000065]\n",
      "85600 [D loss: 0.999960] [G loss: 1.000062]\n",
      "85610 [D loss: 0.999945] [G loss: 1.000085]\n",
      "85620 [D loss: 0.999976] [G loss: 1.000064]\n",
      "85630 [D loss: 0.999958] [G loss: 1.000060]\n",
      "85640 [D loss: 0.999955] [G loss: 1.000026]\n",
      "85650 [D loss: 0.999979] [G loss: 1.000066]\n",
      "85660 [D loss: 0.999949] [G loss: 1.000059]\n",
      "85670 [D loss: 0.999951] [G loss: 1.000070]\n",
      "85680 [D loss: 0.999979] [G loss: 1.000031]\n",
      "85690 [D loss: 0.999963] [G loss: 1.000065]\n",
      "85700 [D loss: 0.999965] [G loss: 1.000058]\n",
      "85710 [D loss: 0.999940] [G loss: 1.000056]\n",
      "85720 [D loss: 0.999948] [G loss: 1.000053]\n",
      "85730 [D loss: 0.999978] [G loss: 1.000011]\n",
      "85740 [D loss: 0.999980] [G loss: 1.000011]\n",
      "85750 [D loss: 0.999972] [G loss: 1.000061]\n",
      "85760 [D loss: 0.999971] [G loss: 1.000082]\n",
      "85770 [D loss: 0.999975] [G loss: 1.000039]\n",
      "85780 [D loss: 0.999954] [G loss: 1.000057]\n",
      "85790 [D loss: 0.999964] [G loss: 1.000070]\n",
      "85800 [D loss: 0.999956] [G loss: 1.000068]\n",
      "85810 [D loss: 0.999995] [G loss: 1.000051]\n",
      "85820 [D loss: 0.999966] [G loss: 1.000054]\n",
      "85830 [D loss: 0.999966] [G loss: 1.000050]\n",
      "85840 [D loss: 0.999958] [G loss: 1.000094]\n",
      "85850 [D loss: 0.999955] [G loss: 1.000032]\n",
      "85860 [D loss: 0.999977] [G loss: 1.000074]\n",
      "85870 [D loss: 0.999972] [G loss: 1.000017]\n",
      "85880 [D loss: 0.999947] [G loss: 1.000059]\n",
      "85890 [D loss: 0.999964] [G loss: 1.000060]\n",
      "85900 [D loss: 0.999970] [G loss: 1.000052]\n",
      "85910 [D loss: 0.999973] [G loss: 1.000057]\n",
      "85920 [D loss: 0.999982] [G loss: 1.000043]\n",
      "85930 [D loss: 0.999966] [G loss: 1.000027]\n",
      "85940 [D loss: 0.999968] [G loss: 1.000025]\n",
      "85950 [D loss: 0.999961] [G loss: 1.000078]\n",
      "85960 [D loss: 0.999976] [G loss: 1.000028]\n",
      "85970 [D loss: 0.999977] [G loss: 1.000041]\n",
      "85980 [D loss: 0.999975] [G loss: 1.000069]\n",
      "85990 [D loss: 0.999985] [G loss: 1.000060]\n",
      "86000 [D loss: 0.999956] [G loss: 1.000057]\n",
      "86010 [D loss: 0.999951] [G loss: 1.000042]\n",
      "86020 [D loss: 0.999934] [G loss: 1.000071]\n",
      "86030 [D loss: 0.999974] [G loss: 1.000062]\n",
      "86040 [D loss: 0.999959] [G loss: 1.000051]\n",
      "86050 [D loss: 0.999974] [G loss: 1.000049]\n",
      "86060 [D loss: 0.999980] [G loss: 1.000089]\n",
      "86070 [D loss: 0.999969] [G loss: 1.000075]\n",
      "86080 [D loss: 0.999994] [G loss: 1.000055]\n",
      "86090 [D loss: 0.999964] [G loss: 1.000046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86100 [D loss: 0.999960] [G loss: 1.000050]\n",
      "86110 [D loss: 0.999982] [G loss: 1.000083]\n",
      "86120 [D loss: 0.999973] [G loss: 1.000067]\n",
      "86130 [D loss: 0.999954] [G loss: 1.000068]\n",
      "86140 [D loss: 0.999961] [G loss: 1.000047]\n",
      "86150 [D loss: 0.999950] [G loss: 1.000014]\n",
      "86160 [D loss: 0.999959] [G loss: 1.000062]\n",
      "86170 [D loss: 0.999975] [G loss: 1.000054]\n",
      "86180 [D loss: 0.999967] [G loss: 1.000053]\n",
      "86190 [D loss: 0.999979] [G loss: 1.000043]\n",
      "86200 [D loss: 0.999977] [G loss: 1.000011]\n",
      "86210 [D loss: 0.999972] [G loss: 1.000051]\n",
      "86220 [D loss: 0.999966] [G loss: 1.000057]\n",
      "86230 [D loss: 0.999977] [G loss: 1.000093]\n",
      "86240 [D loss: 0.999952] [G loss: 1.000061]\n",
      "86250 [D loss: 0.999938] [G loss: 1.000059]\n",
      "86260 [D loss: 0.999936] [G loss: 1.000047]\n",
      "86270 [D loss: 0.999986] [G loss: 1.000102]\n",
      "86280 [D loss: 0.999955] [G loss: 1.000082]\n",
      "86290 [D loss: 0.999960] [G loss: 1.000095]\n",
      "86300 [D loss: 0.999969] [G loss: 1.000042]\n",
      "86310 [D loss: 0.999972] [G loss: 1.000060]\n",
      "86320 [D loss: 0.999975] [G loss: 1.000024]\n",
      "86330 [D loss: 0.999962] [G loss: 1.000083]\n",
      "86340 [D loss: 0.999990] [G loss: 1.000046]\n",
      "86350 [D loss: 0.999959] [G loss: 1.000051]\n",
      "86360 [D loss: 0.999975] [G loss: 1.000059]\n",
      "86370 [D loss: 0.999965] [G loss: 1.000076]\n",
      "86380 [D loss: 0.999960] [G loss: 1.000035]\n",
      "86390 [D loss: 0.999973] [G loss: 1.000119]\n",
      "86400 [D loss: 0.999957] [G loss: 1.000073]\n",
      "86410 [D loss: 0.999973] [G loss: 1.000064]\n",
      "86420 [D loss: 0.999971] [G loss: 1.000069]\n",
      "86430 [D loss: 0.999972] [G loss: 1.000061]\n",
      "86440 [D loss: 0.999960] [G loss: 1.000058]\n",
      "86450 [D loss: 0.999976] [G loss: 1.000047]\n",
      "86460 [D loss: 0.999949] [G loss: 1.000069]\n",
      "86470 [D loss: 0.999962] [G loss: 1.000064]\n",
      "86480 [D loss: 0.999962] [G loss: 1.000073]\n",
      "86490 [D loss: 0.999950] [G loss: 1.000071]\n",
      "86500 [D loss: 0.999972] [G loss: 1.000070]\n",
      "86510 [D loss: 0.999970] [G loss: 1.000070]\n",
      "86520 [D loss: 0.999956] [G loss: 1.000068]\n",
      "86530 [D loss: 0.999963] [G loss: 1.000070]\n",
      "86540 [D loss: 0.999958] [G loss: 1.000059]\n",
      "86550 [D loss: 0.999960] [G loss: 1.000066]\n",
      "86560 [D loss: 0.999970] [G loss: 1.000047]\n",
      "86570 [D loss: 0.999978] [G loss: 1.000059]\n",
      "86580 [D loss: 0.999985] [G loss: 1.000043]\n",
      "86590 [D loss: 0.999962] [G loss: 1.000060]\n",
      "86600 [D loss: 0.999946] [G loss: 1.000067]\n",
      "86610 [D loss: 0.999982] [G loss: 1.000072]\n",
      "86620 [D loss: 0.999957] [G loss: 1.000069]\n",
      "86630 [D loss: 0.999976] [G loss: 1.000053]\n",
      "86640 [D loss: 0.999964] [G loss: 1.000066]\n",
      "86650 [D loss: 0.999956] [G loss: 1.000063]\n",
      "86660 [D loss: 0.999952] [G loss: 1.000064]\n",
      "86670 [D loss: 0.999969] [G loss: 1.000042]\n",
      "86680 [D loss: 0.999980] [G loss: 1.000069]\n",
      "86690 [D loss: 0.999982] [G loss: 1.000053]\n",
      "86700 [D loss: 0.999951] [G loss: 1.000084]\n",
      "86710 [D loss: 0.999962] [G loss: 1.000064]\n",
      "86720 [D loss: 0.999961] [G loss: 1.000040]\n",
      "86730 [D loss: 0.999973] [G loss: 1.000073]\n",
      "86740 [D loss: 0.999966] [G loss: 1.000068]\n",
      "86750 [D loss: 0.999973] [G loss: 1.000042]\n",
      "86760 [D loss: 0.999972] [G loss: 1.000081]\n",
      "86770 [D loss: 0.999957] [G loss: 1.000057]\n",
      "86780 [D loss: 0.999962] [G loss: 1.000077]\n",
      "86790 [D loss: 0.999950] [G loss: 1.000033]\n",
      "86800 [D loss: 0.999952] [G loss: 1.000058]\n",
      "86810 [D loss: 0.999955] [G loss: 1.000008]\n",
      "86820 [D loss: 0.999966] [G loss: 1.000082]\n",
      "86830 [D loss: 0.999974] [G loss: 1.000068]\n",
      "86840 [D loss: 0.999961] [G loss: 1.000098]\n",
      "86850 [D loss: 0.999963] [G loss: 1.000057]\n",
      "86860 [D loss: 0.999985] [G loss: 1.000053]\n",
      "86870 [D loss: 0.999966] [G loss: 1.000049]\n",
      "86880 [D loss: 0.999960] [G loss: 1.000061]\n",
      "86890 [D loss: 0.999970] [G loss: 1.000066]\n",
      "86900 [D loss: 0.999980] [G loss: 1.000055]\n",
      "86910 [D loss: 0.999969] [G loss: 1.000082]\n",
      "86920 [D loss: 0.999977] [G loss: 1.000046]\n",
      "86930 [D loss: 0.999990] [G loss: 1.000037]\n",
      "86940 [D loss: 0.999982] [G loss: 1.000094]\n",
      "86950 [D loss: 0.999950] [G loss: 1.000065]\n",
      "86960 [D loss: 0.999963] [G loss: 1.000049]\n",
      "86970 [D loss: 0.999972] [G loss: 1.000038]\n",
      "86980 [D loss: 0.999966] [G loss: 1.000104]\n",
      "86990 [D loss: 0.999973] [G loss: 1.000061]\n",
      "87000 [D loss: 0.999974] [G loss: 1.000074]\n",
      "87010 [D loss: 0.999928] [G loss: 1.000080]\n",
      "87020 [D loss: 0.999977] [G loss: 1.000064]\n",
      "87030 [D loss: 0.999983] [G loss: 1.000097]\n",
      "87040 [D loss: 0.999971] [G loss: 1.000077]\n",
      "87050 [D loss: 0.999960] [G loss: 1.000067]\n",
      "87060 [D loss: 0.999973] [G loss: 1.000042]\n",
      "87070 [D loss: 0.999966] [G loss: 1.000049]\n",
      "87080 [D loss: 0.999976] [G loss: 1.000047]\n",
      "87090 [D loss: 1.000008] [G loss: 1.000022]\n",
      "87100 [D loss: 0.999970] [G loss: 1.000024]\n",
      "87110 [D loss: 0.999974] [G loss: 1.000052]\n",
      "87120 [D loss: 0.999961] [G loss: 1.000080]\n",
      "87130 [D loss: 0.999945] [G loss: 1.000039]\n",
      "87140 [D loss: 0.999954] [G loss: 1.000077]\n",
      "87150 [D loss: 0.999950] [G loss: 1.000080]\n",
      "87160 [D loss: 0.999984] [G loss: 1.000037]\n",
      "87170 [D loss: 0.999943] [G loss: 1.000034]\n",
      "87180 [D loss: 0.999944] [G loss: 1.000088]\n",
      "87190 [D loss: 0.999981] [G loss: 1.000096]\n",
      "87200 [D loss: 0.999970] [G loss: 1.000078]\n",
      "87210 [D loss: 0.999964] [G loss: 1.000091]\n",
      "87220 [D loss: 0.999979] [G loss: 1.000146]\n",
      "87230 [D loss: 0.999940] [G loss: 1.000084]\n",
      "87240 [D loss: 0.999949] [G loss: 1.000071]\n",
      "87250 [D loss: 0.999968] [G loss: 1.000078]\n",
      "87260 [D loss: 1.000004] [G loss: 1.000063]\n",
      "87270 [D loss: 0.999952] [G loss: 1.000003]\n",
      "87280 [D loss: 0.999964] [G loss: 1.000069]\n",
      "87290 [D loss: 0.999954] [G loss: 1.000042]\n",
      "87300 [D loss: 0.999967] [G loss: 1.000011]\n",
      "87310 [D loss: 0.999945] [G loss: 1.000085]\n",
      "87320 [D loss: 0.999953] [G loss: 1.000050]\n",
      "87330 [D loss: 0.999965] [G loss: 1.000076]\n",
      "87340 [D loss: 0.999978] [G loss: 1.000052]\n",
      "87350 [D loss: 0.999935] [G loss: 1.000048]\n",
      "87360 [D loss: 0.999974] [G loss: 1.000081]\n",
      "87370 [D loss: 0.999987] [G loss: 0.999992]\n",
      "87380 [D loss: 0.999966] [G loss: 1.000049]\n",
      "87390 [D loss: 0.999967] [G loss: 1.000081]\n",
      "87400 [D loss: 0.999967] [G loss: 1.000050]\n",
      "87410 [D loss: 0.999970] [G loss: 1.000027]\n",
      "87420 [D loss: 0.999950] [G loss: 1.000039]\n",
      "87430 [D loss: 0.999965] [G loss: 1.000081]\n",
      "87440 [D loss: 0.999978] [G loss: 1.000093]\n",
      "87450 [D loss: 0.999963] [G loss: 1.000060]\n",
      "87460 [D loss: 0.999957] [G loss: 1.000037]\n",
      "87470 [D loss: 1.000000] [G loss: 1.000057]\n",
      "87480 [D loss: 0.999961] [G loss: 1.000082]\n",
      "87490 [D loss: 0.999968] [G loss: 1.000067]\n",
      "87500 [D loss: 0.999992] [G loss: 1.000066]\n",
      "87510 [D loss: 0.999958] [G loss: 1.000086]\n",
      "87520 [D loss: 0.999980] [G loss: 1.000048]\n",
      "87530 [D loss: 0.999984] [G loss: 1.000074]\n",
      "87540 [D loss: 0.999957] [G loss: 1.000095]\n",
      "87550 [D loss: 0.999956] [G loss: 1.000069]\n",
      "87560 [D loss: 0.999976] [G loss: 1.000071]\n",
      "87570 [D loss: 0.999960] [G loss: 1.000041]\n",
      "87580 [D loss: 0.999951] [G loss: 1.000035]\n",
      "87590 [D loss: 0.999957] [G loss: 1.000069]\n",
      "87600 [D loss: 0.999973] [G loss: 1.000076]\n",
      "87610 [D loss: 1.000024] [G loss: 1.000037]\n",
      "87620 [D loss: 0.999960] [G loss: 1.000060]\n",
      "87630 [D loss: 0.999969] [G loss: 1.000065]\n",
      "87640 [D loss: 1.000019] [G loss: 1.000064]\n",
      "87650 [D loss: 0.999966] [G loss: 1.000071]\n",
      "87660 [D loss: 0.999954] [G loss: 1.000082]\n",
      "87670 [D loss: 0.999984] [G loss: 1.000051]\n",
      "87680 [D loss: 0.999969] [G loss: 1.000062]\n",
      "87690 [D loss: 0.999983] [G loss: 1.000089]\n",
      "87700 [D loss: 0.999963] [G loss: 1.000055]\n",
      "87710 [D loss: 0.999966] [G loss: 1.000073]\n",
      "87720 [D loss: 0.999982] [G loss: 1.000030]\n",
      "87730 [D loss: 0.999958] [G loss: 1.000064]\n",
      "87740 [D loss: 0.999961] [G loss: 1.000091]\n",
      "87750 [D loss: 1.000021] [G loss: 0.999987]\n",
      "87760 [D loss: 0.999959] [G loss: 1.000092]\n",
      "87770 [D loss: 0.999952] [G loss: 1.000058]\n",
      "87780 [D loss: 0.999958] [G loss: 1.000035]\n",
      "87790 [D loss: 0.999988] [G loss: 1.000086]\n",
      "87800 [D loss: 0.999966] [G loss: 1.000047]\n",
      "87810 [D loss: 0.999974] [G loss: 1.000089]\n",
      "87820 [D loss: 0.999963] [G loss: 1.000056]\n",
      "87830 [D loss: 0.999980] [G loss: 1.000065]\n",
      "87840 [D loss: 0.999965] [G loss: 1.000064]\n",
      "87850 [D loss: 0.999993] [G loss: 1.000057]\n",
      "87860 [D loss: 0.999991] [G loss: 0.999998]\n",
      "87870 [D loss: 0.999956] [G loss: 1.000073]\n",
      "87880 [D loss: 0.999978] [G loss: 1.000047]\n",
      "87890 [D loss: 0.999965] [G loss: 1.000101]\n",
      "87900 [D loss: 0.999981] [G loss: 1.000053]\n",
      "87910 [D loss: 0.999986] [G loss: 1.000102]\n",
      "87920 [D loss: 0.999983] [G loss: 1.000061]\n",
      "87930 [D loss: 0.999980] [G loss: 1.000080]\n",
      "87940 [D loss: 0.999985] [G loss: 1.000066]\n",
      "87950 [D loss: 0.999970] [G loss: 1.000077]\n",
      "87960 [D loss: 0.999941] [G loss: 1.000076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87970 [D loss: 0.999976] [G loss: 1.000043]\n",
      "87980 [D loss: 0.999952] [G loss: 1.000070]\n",
      "87990 [D loss: 0.999985] [G loss: 1.000091]\n",
      "88000 [D loss: 0.999980] [G loss: 1.000035]\n",
      "88010 [D loss: 0.999978] [G loss: 1.000077]\n",
      "88020 [D loss: 0.999999] [G loss: 0.999994]\n",
      "88030 [D loss: 0.999968] [G loss: 1.000059]\n",
      "88040 [D loss: 0.999957] [G loss: 1.000099]\n",
      "88050 [D loss: 0.999972] [G loss: 1.000044]\n",
      "88060 [D loss: 0.999983] [G loss: 1.000078]\n",
      "88070 [D loss: 0.999998] [G loss: 1.000049]\n",
      "88080 [D loss: 0.999964] [G loss: 1.000020]\n",
      "88090 [D loss: 0.999965] [G loss: 1.000105]\n",
      "88100 [D loss: 0.999939] [G loss: 1.000064]\n",
      "88110 [D loss: 0.999965] [G loss: 1.000079]\n",
      "88120 [D loss: 0.999966] [G loss: 1.000081]\n",
      "88130 [D loss: 0.999993] [G loss: 1.000033]\n",
      "88140 [D loss: 0.999951] [G loss: 1.000087]\n",
      "88150 [D loss: 0.999996] [G loss: 1.000069]\n",
      "88160 [D loss: 0.999960] [G loss: 1.000063]\n",
      "88170 [D loss: 0.999974] [G loss: 1.000072]\n",
      "88180 [D loss: 0.999974] [G loss: 1.000073]\n",
      "88190 [D loss: 0.999969] [G loss: 1.000084]\n",
      "88200 [D loss: 0.999984] [G loss: 1.000085]\n",
      "88210 [D loss: 0.999974] [G loss: 1.000036]\n",
      "88220 [D loss: 0.999956] [G loss: 1.000066]\n",
      "88230 [D loss: 0.999954] [G loss: 1.000066]\n",
      "88240 [D loss: 0.999966] [G loss: 1.000074]\n",
      "88250 [D loss: 0.999962] [G loss: 1.000062]\n",
      "88260 [D loss: 0.999969] [G loss: 1.000056]\n",
      "88270 [D loss: 0.999976] [G loss: 1.000058]\n",
      "88280 [D loss: 0.999978] [G loss: 1.000067]\n",
      "88290 [D loss: 0.999955] [G loss: 1.000056]\n",
      "88300 [D loss: 0.999974] [G loss: 1.000054]\n",
      "88310 [D loss: 0.999972] [G loss: 1.000046]\n",
      "88320 [D loss: 0.999968] [G loss: 1.000089]\n",
      "88330 [D loss: 0.999960] [G loss: 1.000078]\n",
      "88340 [D loss: 0.999974] [G loss: 1.000066]\n",
      "88350 [D loss: 1.000012] [G loss: 1.000023]\n",
      "88360 [D loss: 0.999964] [G loss: 1.000028]\n",
      "88370 [D loss: 0.999954] [G loss: 1.000060]\n",
      "88380 [D loss: 0.999975] [G loss: 1.000083]\n",
      "88390 [D loss: 0.999957] [G loss: 1.000057]\n",
      "88400 [D loss: 0.999981] [G loss: 1.000091]\n",
      "88410 [D loss: 0.999964] [G loss: 1.000065]\n",
      "88420 [D loss: 0.999961] [G loss: 1.000073]\n",
      "88430 [D loss: 0.999972] [G loss: 1.000092]\n",
      "88440 [D loss: 0.999978] [G loss: 1.000085]\n",
      "88450 [D loss: 0.999977] [G loss: 1.000035]\n",
      "88460 [D loss: 0.999973] [G loss: 1.000055]\n",
      "88470 [D loss: 0.999963] [G loss: 1.000085]\n",
      "88480 [D loss: 0.999959] [G loss: 1.000048]\n",
      "88490 [D loss: 0.999965] [G loss: 1.000041]\n",
      "88500 [D loss: 0.999980] [G loss: 1.000080]\n",
      "88510 [D loss: 0.999965] [G loss: 1.000087]\n",
      "88520 [D loss: 0.999980] [G loss: 1.000031]\n",
      "88530 [D loss: 0.999972] [G loss: 1.000069]\n",
      "88540 [D loss: 0.999977] [G loss: 1.000056]\n",
      "88550 [D loss: 0.999963] [G loss: 1.000082]\n",
      "88560 [D loss: 0.999956] [G loss: 1.000045]\n",
      "88570 [D loss: 0.999947] [G loss: 1.000058]\n",
      "88580 [D loss: 0.999955] [G loss: 1.000080]\n",
      "88590 [D loss: 0.999962] [G loss: 1.000081]\n",
      "88600 [D loss: 0.999980] [G loss: 1.000080]\n",
      "88610 [D loss: 0.999942] [G loss: 1.000070]\n",
      "88620 [D loss: 0.999971] [G loss: 1.000053]\n",
      "88630 [D loss: 0.999961] [G loss: 1.000083]\n",
      "88640 [D loss: 0.999961] [G loss: 1.000118]\n",
      "88650 [D loss: 0.999955] [G loss: 1.000058]\n",
      "88660 [D loss: 0.999963] [G loss: 1.000091]\n",
      "88670 [D loss: 0.999994] [G loss: 1.000102]\n",
      "88680 [D loss: 0.999996] [G loss: 1.000019]\n",
      "88690 [D loss: 0.999973] [G loss: 1.000065]\n",
      "88700 [D loss: 0.999959] [G loss: 1.000066]\n",
      "88710 [D loss: 0.999974] [G loss: 1.000065]\n",
      "88720 [D loss: 0.999967] [G loss: 1.000071]\n",
      "88730 [D loss: 0.999965] [G loss: 1.000068]\n",
      "88740 [D loss: 0.999972] [G loss: 1.000062]\n",
      "88750 [D loss: 0.999957] [G loss: 1.000071]\n",
      "88760 [D loss: 0.999984] [G loss: 1.000024]\n",
      "88770 [D loss: 0.999967] [G loss: 1.000078]\n",
      "88780 [D loss: 0.999966] [G loss: 1.000055]\n",
      "88790 [D loss: 0.999977] [G loss: 1.000045]\n",
      "88800 [D loss: 0.999970] [G loss: 1.000062]\n",
      "88810 [D loss: 0.999974] [G loss: 1.000087]\n",
      "88820 [D loss: 0.999969] [G loss: 1.000052]\n",
      "88830 [D loss: 0.999971] [G loss: 1.000076]\n",
      "88840 [D loss: 0.999956] [G loss: 1.000063]\n",
      "88850 [D loss: 0.999971] [G loss: 1.000061]\n",
      "88860 [D loss: 0.999976] [G loss: 1.000049]\n",
      "88870 [D loss: 0.999976] [G loss: 1.000082]\n",
      "88880 [D loss: 0.999970] [G loss: 1.000060]\n",
      "88890 [D loss: 0.999960] [G loss: 1.000073]\n",
      "88900 [D loss: 0.999970] [G loss: 1.000056]\n",
      "88910 [D loss: 0.999966] [G loss: 1.000056]\n",
      "88920 [D loss: 0.999975] [G loss: 1.000065]\n",
      "88930 [D loss: 0.999979] [G loss: 1.000052]\n",
      "88940 [D loss: 0.999970] [G loss: 1.000047]\n",
      "88950 [D loss: 0.999970] [G loss: 1.000061]\n",
      "88960 [D loss: 0.999961] [G loss: 1.000060]\n",
      "88970 [D loss: 0.999976] [G loss: 1.000051]\n",
      "88980 [D loss: 0.999971] [G loss: 1.000068]\n",
      "88990 [D loss: 0.999956] [G loss: 1.000077]\n",
      "89000 [D loss: 0.999961] [G loss: 1.000068]\n",
      "89010 [D loss: 0.999960] [G loss: 1.000068]\n",
      "89020 [D loss: 0.999966] [G loss: 1.000060]\n",
      "89030 [D loss: 0.999956] [G loss: 1.000047]\n",
      "89040 [D loss: 0.999968] [G loss: 1.000055]\n",
      "89050 [D loss: 0.999965] [G loss: 1.000072]\n",
      "89060 [D loss: 0.999961] [G loss: 1.000053]\n",
      "89070 [D loss: 0.999980] [G loss: 1.000077]\n",
      "89080 [D loss: 0.999964] [G loss: 1.000066]\n",
      "89090 [D loss: 0.999962] [G loss: 1.000073]\n",
      "89100 [D loss: 0.999961] [G loss: 1.000075]\n",
      "89110 [D loss: 0.999973] [G loss: 1.000066]\n",
      "89120 [D loss: 0.999957] [G loss: 1.000072]\n",
      "89130 [D loss: 0.999959] [G loss: 1.000081]\n",
      "89140 [D loss: 0.999972] [G loss: 1.000072]\n",
      "89150 [D loss: 0.999968] [G loss: 1.000055]\n",
      "89160 [D loss: 0.999973] [G loss: 1.000064]\n",
      "89170 [D loss: 0.999972] [G loss: 1.000046]\n",
      "89180 [D loss: 0.999971] [G loss: 1.000075]\n",
      "89190 [D loss: 0.999979] [G loss: 1.000080]\n",
      "89200 [D loss: 0.999962] [G loss: 1.000053]\n",
      "89210 [D loss: 0.999962] [G loss: 1.000065]\n",
      "89220 [D loss: 0.999971] [G loss: 1.000056]\n",
      "89230 [D loss: 0.999970] [G loss: 1.000067]\n",
      "89240 [D loss: 0.999983] [G loss: 1.000059]\n",
      "89250 [D loss: 0.999989] [G loss: 1.000044]\n",
      "89260 [D loss: 0.999947] [G loss: 1.000067]\n",
      "89270 [D loss: 0.999946] [G loss: 1.000054]\n",
      "89280 [D loss: 0.999989] [G loss: 1.000057]\n",
      "89290 [D loss: 0.999965] [G loss: 1.000076]\n",
      "89300 [D loss: 0.999964] [G loss: 1.000067]\n",
      "89310 [D loss: 0.999980] [G loss: 1.000063]\n",
      "89320 [D loss: 0.999963] [G loss: 1.000067]\n",
      "89330 [D loss: 0.999972] [G loss: 1.000059]\n",
      "89340 [D loss: 0.999978] [G loss: 1.000072]\n",
      "89350 [D loss: 0.999967] [G loss: 1.000055]\n",
      "89360 [D loss: 0.999968] [G loss: 1.000065]\n",
      "89370 [D loss: 0.999960] [G loss: 1.000067]\n",
      "89380 [D loss: 0.999967] [G loss: 1.000073]\n",
      "89390 [D loss: 0.999980] [G loss: 1.000072]\n",
      "89400 [D loss: 0.999957] [G loss: 1.000020]\n",
      "89410 [D loss: 0.999978] [G loss: 1.000034]\n",
      "89420 [D loss: 0.999969] [G loss: 1.000060]\n",
      "89430 [D loss: 0.999984] [G loss: 1.000076]\n",
      "89440 [D loss: 0.999968] [G loss: 1.000058]\n",
      "89450 [D loss: 0.999973] [G loss: 1.000070]\n",
      "89460 [D loss: 0.999978] [G loss: 1.000049]\n",
      "89470 [D loss: 0.999971] [G loss: 1.000086]\n",
      "89480 [D loss: 0.999974] [G loss: 1.000061]\n",
      "89490 [D loss: 0.999972] [G loss: 1.000070]\n",
      "89500 [D loss: 0.999965] [G loss: 1.000062]\n",
      "89510 [D loss: 0.999965] [G loss: 1.000063]\n",
      "89520 [D loss: 0.999970] [G loss: 1.000052]\n",
      "89530 [D loss: 0.999954] [G loss: 1.000055]\n",
      "89540 [D loss: 0.999964] [G loss: 1.000059]\n",
      "89550 [D loss: 0.999972] [G loss: 1.000061]\n",
      "89560 [D loss: 0.999964] [G loss: 1.000079]\n",
      "89570 [D loss: 0.999969] [G loss: 1.000062]\n",
      "89580 [D loss: 0.999987] [G loss: 1.000020]\n",
      "89590 [D loss: 0.999973] [G loss: 1.000043]\n",
      "89600 [D loss: 0.999959] [G loss: 1.000048]\n",
      "89610 [D loss: 0.999958] [G loss: 1.000064]\n",
      "89620 [D loss: 0.999974] [G loss: 1.000064]\n",
      "89630 [D loss: 0.999965] [G loss: 1.000061]\n",
      "89640 [D loss: 0.999975] [G loss: 1.000065]\n",
      "89650 [D loss: 0.999965] [G loss: 1.000067]\n",
      "89660 [D loss: 0.999959] [G loss: 1.000058]\n",
      "89670 [D loss: 0.999975] [G loss: 1.000074]\n",
      "89680 [D loss: 0.999977] [G loss: 1.000058]\n",
      "89690 [D loss: 0.999965] [G loss: 1.000035]\n",
      "89700 [D loss: 0.999968] [G loss: 1.000062]\n",
      "89710 [D loss: 0.999975] [G loss: 1.000064]\n",
      "89720 [D loss: 0.999960] [G loss: 1.000059]\n",
      "89730 [D loss: 0.999964] [G loss: 1.000054]\n",
      "89740 [D loss: 0.999972] [G loss: 1.000074]\n",
      "89750 [D loss: 0.999970] [G loss: 1.000063]\n",
      "89760 [D loss: 0.999964] [G loss: 1.000057]\n",
      "89770 [D loss: 0.999960] [G loss: 1.000054]\n",
      "89780 [D loss: 0.999962] [G loss: 1.000071]\n",
      "89790 [D loss: 0.999966] [G loss: 1.000070]\n",
      "89800 [D loss: 0.999969] [G loss: 1.000056]\n",
      "89810 [D loss: 0.999974] [G loss: 1.000048]\n",
      "89820 [D loss: 0.999972] [G loss: 1.000053]\n",
      "89830 [D loss: 0.999955] [G loss: 1.000046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89840 [D loss: 0.999979] [G loss: 1.000052]\n",
      "89850 [D loss: 0.999962] [G loss: 1.000049]\n",
      "89860 [D loss: 0.999945] [G loss: 1.000073]\n",
      "89870 [D loss: 0.999954] [G loss: 1.000082]\n",
      "89880 [D loss: 0.999989] [G loss: 1.000043]\n",
      "89890 [D loss: 0.999940] [G loss: 1.000073]\n",
      "89900 [D loss: 0.999973] [G loss: 1.000052]\n",
      "89910 [D loss: 0.999990] [G loss: 1.000068]\n",
      "89920 [D loss: 0.999978] [G loss: 1.000069]\n",
      "89930 [D loss: 0.999962] [G loss: 1.000047]\n",
      "89940 [D loss: 0.999970] [G loss: 1.000064]\n",
      "89950 [D loss: 0.999964] [G loss: 1.000049]\n",
      "89960 [D loss: 0.999971] [G loss: 1.000056]\n",
      "89970 [D loss: 0.999964] [G loss: 1.000068]\n",
      "89980 [D loss: 0.999977] [G loss: 1.000050]\n",
      "89990 [D loss: 0.999956] [G loss: 1.000065]\n",
      "90000 [D loss: 0.999972] [G loss: 1.000018]\n",
      "90010 [D loss: 0.999951] [G loss: 1.000062]\n",
      "90020 [D loss: 0.999952] [G loss: 1.000042]\n",
      "90030 [D loss: 0.999986] [G loss: 1.000068]\n",
      "90040 [D loss: 0.999968] [G loss: 1.000082]\n",
      "90050 [D loss: 0.999978] [G loss: 1.000069]\n",
      "90060 [D loss: 0.999951] [G loss: 1.000052]\n",
      "90070 [D loss: 0.999972] [G loss: 1.000085]\n",
      "90080 [D loss: 0.999957] [G loss: 1.000081]\n",
      "90090 [D loss: 0.999961] [G loss: 1.000047]\n",
      "90100 [D loss: 0.999961] [G loss: 1.000089]\n",
      "90110 [D loss: 0.999970] [G loss: 1.000066]\n",
      "90120 [D loss: 0.999962] [G loss: 1.000093]\n",
      "90130 [D loss: 0.999958] [G loss: 1.000068]\n",
      "90140 [D loss: 0.999990] [G loss: 1.000100]\n",
      "90150 [D loss: 0.999957] [G loss: 1.000039]\n",
      "90160 [D loss: 0.999972] [G loss: 1.000067]\n",
      "90170 [D loss: 0.999965] [G loss: 1.000059]\n",
      "90180 [D loss: 0.999972] [G loss: 1.000029]\n",
      "90190 [D loss: 0.999957] [G loss: 1.000049]\n",
      "90200 [D loss: 0.999955] [G loss: 1.000057]\n",
      "90210 [D loss: 0.999966] [G loss: 1.000062]\n",
      "90220 [D loss: 0.999962] [G loss: 1.000074]\n",
      "90230 [D loss: 0.999961] [G loss: 1.000082]\n",
      "90240 [D loss: 0.999993] [G loss: 1.000051]\n",
      "90250 [D loss: 0.999968] [G loss: 1.000040]\n",
      "90260 [D loss: 0.999966] [G loss: 1.000094]\n",
      "90270 [D loss: 0.999968] [G loss: 1.000076]\n",
      "90280 [D loss: 0.999973] [G loss: 1.000080]\n",
      "90290 [D loss: 0.999978] [G loss: 1.000052]\n",
      "90300 [D loss: 0.999970] [G loss: 1.000023]\n",
      "90310 [D loss: 0.999971] [G loss: 1.000082]\n",
      "90320 [D loss: 0.999970] [G loss: 1.000078]\n",
      "90330 [D loss: 0.999931] [G loss: 1.000098]\n",
      "90340 [D loss: 0.999979] [G loss: 1.000083]\n",
      "90350 [D loss: 0.999962] [G loss: 1.000052]\n",
      "90360 [D loss: 0.999966] [G loss: 1.000060]\n",
      "90370 [D loss: 0.999968] [G loss: 1.000074]\n",
      "90380 [D loss: 0.999954] [G loss: 1.000065]\n",
      "90390 [D loss: 0.999969] [G loss: 1.000070]\n",
      "90400 [D loss: 0.999955] [G loss: 1.000044]\n",
      "90410 [D loss: 0.999974] [G loss: 1.000055]\n",
      "90420 [D loss: 0.999953] [G loss: 1.000057]\n",
      "90430 [D loss: 0.999966] [G loss: 1.000080]\n",
      "90440 [D loss: 0.999965] [G loss: 1.000048]\n",
      "90450 [D loss: 0.999961] [G loss: 1.000038]\n",
      "90460 [D loss: 0.999972] [G loss: 1.000058]\n",
      "90470 [D loss: 0.999969] [G loss: 1.000071]\n",
      "90480 [D loss: 0.999967] [G loss: 1.000067]\n",
      "90490 [D loss: 0.999969] [G loss: 1.000085]\n",
      "90500 [D loss: 0.999976] [G loss: 1.000080]\n",
      "90510 [D loss: 0.999970] [G loss: 1.000060]\n",
      "90520 [D loss: 0.999974] [G loss: 1.000060]\n",
      "90530 [D loss: 0.999979] [G loss: 1.000012]\n",
      "90540 [D loss: 0.999973] [G loss: 1.000059]\n",
      "90550 [D loss: 0.999949] [G loss: 1.000069]\n",
      "90560 [D loss: 0.999952] [G loss: 1.000052]\n",
      "90570 [D loss: 0.999968] [G loss: 1.000039]\n",
      "90580 [D loss: 0.999966] [G loss: 1.000060]\n",
      "90590 [D loss: 0.999958] [G loss: 1.000072]\n",
      "90600 [D loss: 0.999963] [G loss: 1.000062]\n",
      "90610 [D loss: 0.999966] [G loss: 1.000079]\n",
      "90620 [D loss: 0.999980] [G loss: 1.000089]\n",
      "90630 [D loss: 0.999967] [G loss: 1.000083]\n",
      "90640 [D loss: 0.999972] [G loss: 1.000058]\n",
      "90650 [D loss: 0.999964] [G loss: 1.000058]\n",
      "90660 [D loss: 0.999969] [G loss: 1.000070]\n",
      "90670 [D loss: 0.999969] [G loss: 1.000068]\n",
      "90680 [D loss: 0.999966] [G loss: 1.000074]\n",
      "90690 [D loss: 0.999959] [G loss: 1.000061]\n",
      "90700 [D loss: 0.999966] [G loss: 1.000057]\n",
      "90710 [D loss: 0.999968] [G loss: 1.000076]\n",
      "90720 [D loss: 0.999978] [G loss: 1.000039]\n",
      "90730 [D loss: 0.999955] [G loss: 1.000063]\n",
      "90740 [D loss: 0.999970] [G loss: 1.000063]\n",
      "90750 [D loss: 0.999970] [G loss: 1.000042]\n",
      "90760 [D loss: 0.999968] [G loss: 1.000067]\n",
      "90770 [D loss: 0.999974] [G loss: 1.000065]\n",
      "90780 [D loss: 0.999973] [G loss: 1.000065]\n",
      "90790 [D loss: 0.999972] [G loss: 1.000045]\n",
      "90800 [D loss: 0.999963] [G loss: 1.000056]\n",
      "90810 [D loss: 0.999971] [G loss: 1.000062]\n",
      "90820 [D loss: 0.999964] [G loss: 1.000058]\n",
      "90830 [D loss: 0.999959] [G loss: 1.000066]\n",
      "90840 [D loss: 0.999972] [G loss: 1.000056]\n",
      "90850 [D loss: 0.999965] [G loss: 1.000061]\n",
      "90860 [D loss: 0.999965] [G loss: 1.000068]\n",
      "90870 [D loss: 0.999966] [G loss: 1.000057]\n",
      "90880 [D loss: 0.999970] [G loss: 1.000046]\n",
      "90890 [D loss: 0.999964] [G loss: 1.000054]\n",
      "90900 [D loss: 0.999964] [G loss: 1.000057]\n",
      "90910 [D loss: 0.999971] [G loss: 1.000054]\n",
      "90920 [D loss: 0.999967] [G loss: 1.000064]\n",
      "90930 [D loss: 0.999963] [G loss: 1.000061]\n",
      "90940 [D loss: 0.999969] [G loss: 1.000061]\n",
      "90950 [D loss: 0.999964] [G loss: 1.000059]\n",
      "90960 [D loss: 0.999965] [G loss: 1.000069]\n",
      "90970 [D loss: 0.999962] [G loss: 1.000069]\n",
      "90980 [D loss: 0.999979] [G loss: 1.000058]\n",
      "90990 [D loss: 0.999965] [G loss: 1.000052]\n",
      "91000 [D loss: 0.999974] [G loss: 1.000059]\n",
      "91010 [D loss: 0.999970] [G loss: 1.000062]\n",
      "91020 [D loss: 0.999966] [G loss: 1.000054]\n",
      "91030 [D loss: 0.999964] [G loss: 1.000075]\n",
      "91040 [D loss: 0.999975] [G loss: 1.000059]\n",
      "91050 [D loss: 0.999966] [G loss: 1.000061]\n",
      "91060 [D loss: 0.999963] [G loss: 1.000074]\n",
      "91070 [D loss: 0.999956] [G loss: 1.000063]\n",
      "91080 [D loss: 0.999969] [G loss: 1.000053]\n",
      "91090 [D loss: 0.999971] [G loss: 1.000057]\n",
      "91100 [D loss: 0.999962] [G loss: 1.000064]\n",
      "91110 [D loss: 0.999965] [G loss: 1.000054]\n",
      "91120 [D loss: 0.999965] [G loss: 1.000062]\n",
      "91130 [D loss: 0.999972] [G loss: 1.000066]\n",
      "91140 [D loss: 0.999971] [G loss: 1.000056]\n",
      "91150 [D loss: 0.999961] [G loss: 1.000062]\n",
      "91160 [D loss: 0.999966] [G loss: 1.000062]\n",
      "91170 [D loss: 0.999964] [G loss: 1.000048]\n",
      "91180 [D loss: 0.999965] [G loss: 1.000073]\n",
      "91190 [D loss: 0.999956] [G loss: 1.000054]\n",
      "91200 [D loss: 0.999971] [G loss: 1.000071]\n",
      "91210 [D loss: 0.999963] [G loss: 1.000052]\n",
      "91220 [D loss: 0.999966] [G loss: 1.000070]\n",
      "91230 [D loss: 0.999962] [G loss: 1.000060]\n",
      "91240 [D loss: 0.999968] [G loss: 1.000055]\n",
      "91250 [D loss: 0.999966] [G loss: 1.000062]\n",
      "91260 [D loss: 0.999966] [G loss: 1.000057]\n",
      "91270 [D loss: 0.999965] [G loss: 1.000065]\n",
      "91280 [D loss: 0.999954] [G loss: 1.000062]\n",
      "91290 [D loss: 0.999967] [G loss: 1.000068]\n",
      "91300 [D loss: 0.999965] [G loss: 1.000082]\n",
      "91310 [D loss: 0.999985] [G loss: 1.000062]\n",
      "91320 [D loss: 0.999969] [G loss: 1.000054]\n",
      "91330 [D loss: 0.999968] [G loss: 1.000062]\n",
      "91340 [D loss: 0.999966] [G loss: 1.000056]\n",
      "91350 [D loss: 0.999980] [G loss: 1.000058]\n",
      "91360 [D loss: 0.999969] [G loss: 1.000074]\n",
      "91370 [D loss: 0.999970] [G loss: 1.000053]\n",
      "91380 [D loss: 0.999965] [G loss: 1.000059]\n",
      "91390 [D loss: 0.999957] [G loss: 1.000063]\n",
      "91400 [D loss: 0.999966] [G loss: 1.000067]\n",
      "91410 [D loss: 0.999969] [G loss: 1.000070]\n",
      "91420 [D loss: 0.999969] [G loss: 1.000071]\n",
      "91430 [D loss: 0.999969] [G loss: 1.000048]\n",
      "91440 [D loss: 0.999956] [G loss: 1.000069]\n",
      "91450 [D loss: 0.999970] [G loss: 1.000054]\n",
      "91460 [D loss: 0.999974] [G loss: 1.000059]\n",
      "91470 [D loss: 0.999972] [G loss: 1.000041]\n",
      "91480 [D loss: 0.999960] [G loss: 1.000068]\n",
      "91490 [D loss: 0.999969] [G loss: 1.000051]\n",
      "91500 [D loss: 0.999964] [G loss: 1.000051]\n",
      "91510 [D loss: 0.999969] [G loss: 1.000068]\n",
      "91520 [D loss: 0.999958] [G loss: 1.000069]\n",
      "91530 [D loss: 0.999978] [G loss: 1.000051]\n",
      "91540 [D loss: 0.999965] [G loss: 1.000084]\n",
      "91550 [D loss: 0.999981] [G loss: 1.000036]\n",
      "91560 [D loss: 0.999961] [G loss: 1.000055]\n",
      "91570 [D loss: 0.999963] [G loss: 1.000060]\n",
      "91580 [D loss: 0.999944] [G loss: 1.000058]\n",
      "91590 [D loss: 0.999967] [G loss: 1.000053]\n",
      "91600 [D loss: 0.999953] [G loss: 1.000058]\n",
      "91610 [D loss: 0.999954] [G loss: 1.000052]\n",
      "91620 [D loss: 0.999968] [G loss: 1.000049]\n",
      "91630 [D loss: 0.999966] [G loss: 1.000055]\n",
      "91640 [D loss: 0.999970] [G loss: 1.000075]\n",
      "91650 [D loss: 0.999969] [G loss: 1.000048]\n",
      "91660 [D loss: 0.999966] [G loss: 1.000063]\n",
      "91670 [D loss: 0.999970] [G loss: 1.000051]\n",
      "91680 [D loss: 0.999967] [G loss: 1.000090]\n",
      "91690 [D loss: 0.999957] [G loss: 1.000061]\n",
      "91700 [D loss: 0.999968] [G loss: 1.000053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91710 [D loss: 0.999977] [G loss: 1.000063]\n",
      "91720 [D loss: 0.999972] [G loss: 1.000054]\n",
      "91730 [D loss: 0.999974] [G loss: 1.000078]\n",
      "91740 [D loss: 0.999967] [G loss: 1.000059]\n",
      "91750 [D loss: 0.999966] [G loss: 1.000061]\n",
      "91760 [D loss: 0.999955] [G loss: 1.000068]\n",
      "91770 [D loss: 0.999970] [G loss: 1.000059]\n",
      "91780 [D loss: 0.999954] [G loss: 1.000059]\n",
      "91790 [D loss: 0.999966] [G loss: 1.000060]\n",
      "91800 [D loss: 0.999968] [G loss: 1.000051]\n",
      "91810 [D loss: 0.999971] [G loss: 1.000068]\n",
      "91820 [D loss: 0.999968] [G loss: 1.000057]\n",
      "91830 [D loss: 0.999973] [G loss: 1.000065]\n",
      "91840 [D loss: 0.999966] [G loss: 1.000049]\n",
      "91850 [D loss: 0.999966] [G loss: 1.000054]\n",
      "91860 [D loss: 0.999965] [G loss: 1.000067]\n",
      "91870 [D loss: 0.999968] [G loss: 1.000058]\n",
      "91880 [D loss: 0.999966] [G loss: 1.000054]\n",
      "91890 [D loss: 0.999973] [G loss: 1.000052]\n",
      "91900 [D loss: 0.999956] [G loss: 1.000058]\n",
      "91910 [D loss: 0.999969] [G loss: 1.000055]\n",
      "91920 [D loss: 0.999971] [G loss: 1.000064]\n",
      "91930 [D loss: 0.999965] [G loss: 1.000075]\n",
      "91940 [D loss: 0.999962] [G loss: 1.000067]\n",
      "91950 [D loss: 0.999970] [G loss: 1.000057]\n",
      "91960 [D loss: 0.999958] [G loss: 1.000071]\n",
      "91970 [D loss: 0.999956] [G loss: 1.000072]\n",
      "91980 [D loss: 0.999970] [G loss: 1.000070]\n",
      "91990 [D loss: 0.999977] [G loss: 1.000064]\n",
      "92000 [D loss: 0.999974] [G loss: 1.000067]\n",
      "92010 [D loss: 0.999969] [G loss: 1.000073]\n",
      "92020 [D loss: 0.999962] [G loss: 1.000051]\n",
      "92030 [D loss: 0.999975] [G loss: 1.000077]\n",
      "92040 [D loss: 0.999968] [G loss: 1.000060]\n",
      "92050 [D loss: 0.999963] [G loss: 1.000060]\n",
      "92060 [D loss: 0.999972] [G loss: 1.000066]\n",
      "92070 [D loss: 0.999962] [G loss: 1.000058]\n",
      "92080 [D loss: 0.999969] [G loss: 1.000066]\n",
      "92090 [D loss: 0.999963] [G loss: 1.000069]\n",
      "92100 [D loss: 0.999965] [G loss: 1.000041]\n",
      "92110 [D loss: 0.999967] [G loss: 1.000060]\n",
      "92120 [D loss: 0.999962] [G loss: 1.000074]\n",
      "92130 [D loss: 0.999965] [G loss: 1.000043]\n",
      "92140 [D loss: 0.999963] [G loss: 1.000061]\n",
      "92150 [D loss: 0.999964] [G loss: 1.000058]\n",
      "92160 [D loss: 0.999976] [G loss: 1.000072]\n",
      "92170 [D loss: 0.999977] [G loss: 1.000044]\n",
      "92180 [D loss: 0.999983] [G loss: 1.000069]\n",
      "92190 [D loss: 0.999981] [G loss: 1.000057]\n",
      "92200 [D loss: 0.999967] [G loss: 1.000046]\n",
      "92210 [D loss: 0.999960] [G loss: 1.000082]\n",
      "92220 [D loss: 0.999962] [G loss: 1.000070]\n",
      "92230 [D loss: 0.999963] [G loss: 1.000070]\n",
      "92240 [D loss: 0.999969] [G loss: 1.000074]\n",
      "92250 [D loss: 0.999959] [G loss: 1.000064]\n",
      "92260 [D loss: 0.999968] [G loss: 1.000078]\n",
      "92270 [D loss: 0.999969] [G loss: 1.000050]\n",
      "92280 [D loss: 0.999962] [G loss: 1.000066]\n",
      "92290 [D loss: 0.999956] [G loss: 1.000057]\n",
      "92300 [D loss: 0.999971] [G loss: 1.000067]\n",
      "92310 [D loss: 0.999955] [G loss: 1.000066]\n",
      "92320 [D loss: 0.999968] [G loss: 1.000048]\n",
      "92330 [D loss: 0.999972] [G loss: 1.000076]\n",
      "92340 [D loss: 0.999972] [G loss: 1.000067]\n",
      "92350 [D loss: 0.999973] [G loss: 1.000068]\n",
      "92360 [D loss: 0.999970] [G loss: 1.000041]\n",
      "92370 [D loss: 0.999983] [G loss: 1.000049]\n",
      "92380 [D loss: 0.999961] [G loss: 1.000057]\n",
      "92390 [D loss: 0.999965] [G loss: 1.000052]\n",
      "92400 [D loss: 0.999961] [G loss: 1.000068]\n",
      "92410 [D loss: 0.999965] [G loss: 1.000074]\n",
      "92420 [D loss: 0.999966] [G loss: 1.000073]\n",
      "92430 [D loss: 0.999966] [G loss: 1.000056]\n",
      "92440 [D loss: 0.999970] [G loss: 1.000065]\n",
      "92450 [D loss: 0.999965] [G loss: 1.000073]\n",
      "92460 [D loss: 0.999968] [G loss: 1.000060]\n",
      "92470 [D loss: 0.999977] [G loss: 1.000062]\n",
      "92480 [D loss: 0.999964] [G loss: 1.000059]\n",
      "92490 [D loss: 0.999965] [G loss: 1.000058]\n",
      "92500 [D loss: 0.999978] [G loss: 1.000037]\n",
      "92510 [D loss: 0.999977] [G loss: 1.000051]\n",
      "92520 [D loss: 0.999967] [G loss: 1.000061]\n",
      "92530 [D loss: 0.999974] [G loss: 1.000058]\n",
      "92540 [D loss: 0.999980] [G loss: 1.000043]\n",
      "92550 [D loss: 0.999965] [G loss: 1.000061]\n",
      "92560 [D loss: 0.999965] [G loss: 1.000061]\n",
      "92570 [D loss: 0.999972] [G loss: 1.000053]\n",
      "92580 [D loss: 0.999970] [G loss: 1.000059]\n",
      "92590 [D loss: 0.999969] [G loss: 1.000054]\n",
      "92600 [D loss: 0.999970] [G loss: 1.000062]\n",
      "92610 [D loss: 0.999960] [G loss: 1.000062]\n",
      "92620 [D loss: 0.999966] [G loss: 1.000076]\n",
      "92630 [D loss: 0.999971] [G loss: 1.000065]\n",
      "92640 [D loss: 0.999975] [G loss: 1.000073]\n",
      "92650 [D loss: 0.999961] [G loss: 1.000048]\n",
      "92660 [D loss: 0.999962] [G loss: 1.000062]\n",
      "92670 [D loss: 0.999971] [G loss: 1.000064]\n",
      "92680 [D loss: 0.999973] [G loss: 1.000074]\n",
      "92690 [D loss: 0.999967] [G loss: 1.000059]\n",
      "92700 [D loss: 0.999964] [G loss: 1.000060]\n",
      "92710 [D loss: 0.999979] [G loss: 1.000071]\n",
      "92720 [D loss: 0.999970] [G loss: 1.000061]\n",
      "92730 [D loss: 0.999960] [G loss: 1.000066]\n",
      "92740 [D loss: 0.999955] [G loss: 1.000059]\n",
      "92750 [D loss: 0.999976] [G loss: 1.000065]\n",
      "92760 [D loss: 0.999983] [G loss: 1.000055]\n",
      "92770 [D loss: 0.999964] [G loss: 1.000062]\n",
      "92780 [D loss: 0.999969] [G loss: 1.000073]\n",
      "92790 [D loss: 0.999984] [G loss: 1.000047]\n",
      "92800 [D loss: 0.999952] [G loss: 1.000064]\n",
      "92810 [D loss: 0.999964] [G loss: 1.000072]\n",
      "92820 [D loss: 0.999960] [G loss: 1.000055]\n",
      "92830 [D loss: 0.999956] [G loss: 1.000066]\n",
      "92840 [D loss: 0.999965] [G loss: 1.000056]\n",
      "92850 [D loss: 0.999964] [G loss: 1.000062]\n",
      "92860 [D loss: 0.999966] [G loss: 1.000049]\n",
      "92870 [D loss: 0.999954] [G loss: 1.000076]\n",
      "92880 [D loss: 0.999963] [G loss: 1.000084]\n",
      "92890 [D loss: 0.999971] [G loss: 1.000057]\n",
      "92900 [D loss: 0.999967] [G loss: 1.000086]\n",
      "92910 [D loss: 0.999960] [G loss: 1.000052]\n",
      "92920 [D loss: 0.999962] [G loss: 1.000052]\n",
      "92930 [D loss: 0.999966] [G loss: 1.000054]\n",
      "92940 [D loss: 0.999973] [G loss: 1.000061]\n",
      "92950 [D loss: 0.999969] [G loss: 1.000060]\n",
      "92960 [D loss: 0.999967] [G loss: 1.000073]\n",
      "92970 [D loss: 0.999967] [G loss: 1.000094]\n",
      "92980 [D loss: 0.999952] [G loss: 1.000053]\n",
      "92990 [D loss: 0.999969] [G loss: 1.000044]\n",
      "93000 [D loss: 0.999967] [G loss: 1.000061]\n",
      "93010 [D loss: 0.999960] [G loss: 1.000061]\n",
      "93020 [D loss: 0.999973] [G loss: 1.000073]\n",
      "93030 [D loss: 0.999984] [G loss: 1.000027]\n",
      "93040 [D loss: 0.999979] [G loss: 1.000070]\n",
      "93050 [D loss: 0.999974] [G loss: 1.000068]\n",
      "93060 [D loss: 0.999962] [G loss: 1.000066]\n",
      "93070 [D loss: 0.999969] [G loss: 1.000088]\n",
      "93080 [D loss: 0.999971] [G loss: 1.000077]\n",
      "93090 [D loss: 0.999978] [G loss: 1.000053]\n",
      "93100 [D loss: 0.999965] [G loss: 1.000064]\n",
      "93110 [D loss: 0.999967] [G loss: 1.000064]\n",
      "93120 [D loss: 0.999965] [G loss: 1.000074]\n",
      "93130 [D loss: 0.999946] [G loss: 1.000052]\n",
      "93140 [D loss: 0.999960] [G loss: 1.000055]\n",
      "93150 [D loss: 0.999966] [G loss: 1.000051]\n",
      "93160 [D loss: 0.999956] [G loss: 1.000055]\n",
      "93170 [D loss: 0.999964] [G loss: 1.000062]\n",
      "93180 [D loss: 0.999965] [G loss: 1.000090]\n",
      "93190 [D loss: 0.999953] [G loss: 1.000058]\n",
      "93200 [D loss: 0.999950] [G loss: 1.000054]\n",
      "93210 [D loss: 0.999970] [G loss: 1.000050]\n",
      "93220 [D loss: 0.999966] [G loss: 1.000073]\n",
      "93230 [D loss: 0.999967] [G loss: 1.000056]\n",
      "93240 [D loss: 0.999982] [G loss: 1.000071]\n",
      "93250 [D loss: 0.999963] [G loss: 1.000068]\n",
      "93260 [D loss: 0.999971] [G loss: 1.000071]\n",
      "93270 [D loss: 0.999954] [G loss: 1.000055]\n",
      "93280 [D loss: 0.999978] [G loss: 1.000035]\n",
      "93290 [D loss: 0.999952] [G loss: 1.000081]\n",
      "93300 [D loss: 0.999951] [G loss: 1.000079]\n",
      "93310 [D loss: 0.999973] [G loss: 1.000051]\n",
      "93320 [D loss: 0.999980] [G loss: 1.000029]\n",
      "93330 [D loss: 0.999966] [G loss: 1.000057]\n",
      "93340 [D loss: 0.999955] [G loss: 1.000082]\n",
      "93350 [D loss: 0.999944] [G loss: 1.000072]\n",
      "93360 [D loss: 0.999959] [G loss: 1.000042]\n",
      "93370 [D loss: 0.999953] [G loss: 1.000054]\n",
      "93380 [D loss: 0.999966] [G loss: 1.000069]\n",
      "93390 [D loss: 0.999953] [G loss: 1.000065]\n",
      "93400 [D loss: 0.999963] [G loss: 1.000074]\n",
      "93410 [D loss: 0.999956] [G loss: 1.000054]\n",
      "93420 [D loss: 0.999955] [G loss: 1.000051]\n",
      "93430 [D loss: 0.999970] [G loss: 1.000086]\n",
      "93440 [D loss: 0.999966] [G loss: 1.000071]\n",
      "93450 [D loss: 0.999963] [G loss: 1.000058]\n",
      "93460 [D loss: 0.999966] [G loss: 1.000066]\n",
      "93470 [D loss: 0.999965] [G loss: 1.000085]\n",
      "93480 [D loss: 0.999966] [G loss: 1.000063]\n",
      "93490 [D loss: 0.999960] [G loss: 1.000069]\n",
      "93500 [D loss: 0.999973] [G loss: 1.000065]\n",
      "93510 [D loss: 0.999971] [G loss: 1.000066]\n",
      "93520 [D loss: 0.999953] [G loss: 1.000068]\n",
      "93530 [D loss: 0.999972] [G loss: 1.000052]\n",
      "93540 [D loss: 0.999969] [G loss: 1.000068]\n",
      "93550 [D loss: 0.999951] [G loss: 1.000057]\n",
      "93560 [D loss: 0.999967] [G loss: 1.000055]\n",
      "93570 [D loss: 0.999974] [G loss: 1.000052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93580 [D loss: 0.999963] [G loss: 1.000072]\n",
      "93590 [D loss: 0.999973] [G loss: 1.000059]\n",
      "93600 [D loss: 0.999965] [G loss: 1.000059]\n",
      "93610 [D loss: 0.999977] [G loss: 1.000064]\n",
      "93620 [D loss: 0.999964] [G loss: 1.000054]\n",
      "93630 [D loss: 0.999969] [G loss: 1.000059]\n",
      "93640 [D loss: 0.999971] [G loss: 1.000054]\n",
      "93650 [D loss: 0.999968] [G loss: 1.000055]\n",
      "93660 [D loss: 0.999970] [G loss: 1.000067]\n",
      "93670 [D loss: 0.999967] [G loss: 1.000083]\n",
      "93680 [D loss: 0.999964] [G loss: 1.000058]\n",
      "93690 [D loss: 0.999949] [G loss: 1.000038]\n",
      "93700 [D loss: 0.999961] [G loss: 1.000045]\n",
      "93710 [D loss: 0.999970] [G loss: 1.000047]\n",
      "93720 [D loss: 0.999972] [G loss: 1.000047]\n",
      "93730 [D loss: 0.999955] [G loss: 1.000049]\n",
      "93740 [D loss: 0.999963] [G loss: 1.000031]\n",
      "93750 [D loss: 0.999953] [G loss: 1.000055]\n",
      "93760 [D loss: 0.999967] [G loss: 1.000053]\n",
      "93770 [D loss: 0.999963] [G loss: 1.000049]\n",
      "93780 [D loss: 0.999977] [G loss: 1.000055]\n",
      "93790 [D loss: 0.999979] [G loss: 1.000064]\n",
      "93800 [D loss: 0.999956] [G loss: 1.000058]\n",
      "93810 [D loss: 0.999969] [G loss: 1.000059]\n",
      "93820 [D loss: 0.999977] [G loss: 1.000069]\n",
      "93830 [D loss: 0.999967] [G loss: 1.000081]\n",
      "93840 [D loss: 0.999962] [G loss: 1.000064]\n",
      "93850 [D loss: 0.999962] [G loss: 1.000066]\n",
      "93860 [D loss: 0.999962] [G loss: 1.000055]\n",
      "93870 [D loss: 0.999971] [G loss: 1.000058]\n",
      "93880 [D loss: 0.999982] [G loss: 1.000085]\n",
      "93890 [D loss: 0.999974] [G loss: 1.000052]\n",
      "93900 [D loss: 0.999962] [G loss: 1.000061]\n",
      "93910 [D loss: 0.999964] [G loss: 1.000059]\n",
      "93920 [D loss: 0.999942] [G loss: 1.000060]\n",
      "93930 [D loss: 0.999965] [G loss: 1.000057]\n",
      "93940 [D loss: 0.999964] [G loss: 1.000029]\n",
      "93950 [D loss: 0.999965] [G loss: 1.000053]\n",
      "93960 [D loss: 0.999967] [G loss: 1.000078]\n",
      "93970 [D loss: 0.999973] [G loss: 1.000050]\n",
      "93980 [D loss: 0.999970] [G loss: 1.000086]\n",
      "93990 [D loss: 0.999947] [G loss: 1.000081]\n",
      "94000 [D loss: 0.999974] [G loss: 1.000069]\n",
      "94010 [D loss: 0.999965] [G loss: 1.000076]\n",
      "94020 [D loss: 0.999958] [G loss: 1.000061]\n",
      "94030 [D loss: 0.999967] [G loss: 1.000074]\n",
      "94040 [D loss: 0.999960] [G loss: 1.000042]\n",
      "94050 [D loss: 0.999978] [G loss: 1.000067]\n",
      "94060 [D loss: 0.999962] [G loss: 1.000061]\n",
      "94070 [D loss: 0.999959] [G loss: 1.000057]\n",
      "94080 [D loss: 0.999958] [G loss: 1.000081]\n",
      "94090 [D loss: 0.999967] [G loss: 1.000049]\n",
      "94100 [D loss: 0.999961] [G loss: 1.000064]\n",
      "94110 [D loss: 0.999962] [G loss: 1.000063]\n",
      "94120 [D loss: 0.999962] [G loss: 1.000042]\n",
      "94130 [D loss: 0.999960] [G loss: 1.000064]\n",
      "94140 [D loss: 0.999973] [G loss: 1.000056]\n",
      "94150 [D loss: 0.999955] [G loss: 1.000053]\n",
      "94160 [D loss: 0.999959] [G loss: 1.000075]\n",
      "94170 [D loss: 0.999980] [G loss: 1.000041]\n",
      "94180 [D loss: 0.999953] [G loss: 1.000059]\n",
      "94190 [D loss: 0.999961] [G loss: 1.000063]\n",
      "94200 [D loss: 0.999966] [G loss: 1.000060]\n",
      "94210 [D loss: 0.999979] [G loss: 1.000064]\n",
      "94220 [D loss: 0.999958] [G loss: 1.000068]\n",
      "94230 [D loss: 0.999961] [G loss: 1.000056]\n",
      "94240 [D loss: 0.999954] [G loss: 1.000048]\n",
      "94250 [D loss: 0.999964] [G loss: 1.000039]\n",
      "94260 [D loss: 0.999958] [G loss: 1.000075]\n",
      "94270 [D loss: 0.999973] [G loss: 1.000079]\n",
      "94280 [D loss: 0.999967] [G loss: 1.000070]\n",
      "94290 [D loss: 0.999963] [G loss: 1.000067]\n",
      "94300 [D loss: 0.999967] [G loss: 1.000073]\n",
      "94310 [D loss: 0.999957] [G loss: 1.000078]\n",
      "94320 [D loss: 0.999964] [G loss: 1.000064]\n",
      "94330 [D loss: 0.999970] [G loss: 1.000069]\n",
      "94340 [D loss: 0.999970] [G loss: 1.000051]\n",
      "94350 [D loss: 0.999968] [G loss: 1.000066]\n",
      "94360 [D loss: 0.999966] [G loss: 1.000059]\n",
      "94370 [D loss: 0.999963] [G loss: 1.000070]\n",
      "94380 [D loss: 0.999971] [G loss: 1.000059]\n",
      "94390 [D loss: 0.999963] [G loss: 1.000058]\n",
      "94400 [D loss: 0.999970] [G loss: 1.000068]\n",
      "94410 [D loss: 0.999973] [G loss: 1.000067]\n",
      "94420 [D loss: 0.999958] [G loss: 1.000050]\n",
      "94430 [D loss: 0.999959] [G loss: 1.000060]\n",
      "94440 [D loss: 0.999961] [G loss: 1.000038]\n",
      "94450 [D loss: 0.999960] [G loss: 1.000061]\n",
      "94460 [D loss: 0.999957] [G loss: 1.000052]\n",
      "94470 [D loss: 0.999952] [G loss: 1.000072]\n",
      "94480 [D loss: 0.999966] [G loss: 1.000075]\n",
      "94490 [D loss: 0.999956] [G loss: 1.000078]\n",
      "94500 [D loss: 0.999948] [G loss: 1.000071]\n",
      "94510 [D loss: 0.999966] [G loss: 1.000052]\n",
      "94520 [D loss: 0.999972] [G loss: 1.000063]\n",
      "94530 [D loss: 0.999987] [G loss: 1.000073]\n",
      "94540 [D loss: 0.999969] [G loss: 1.000062]\n",
      "94550 [D loss: 0.999972] [G loss: 1.000029]\n",
      "94560 [D loss: 0.999943] [G loss: 1.000048]\n",
      "94570 [D loss: 0.999945] [G loss: 1.000073]\n",
      "94580 [D loss: 0.999984] [G loss: 1.000047]\n",
      "94590 [D loss: 0.999959] [G loss: 1.000076]\n",
      "94600 [D loss: 0.999968] [G loss: 1.000041]\n",
      "94610 [D loss: 0.999956] [G loss: 1.000055]\n",
      "94620 [D loss: 0.999972] [G loss: 1.000068]\n",
      "94630 [D loss: 0.999965] [G loss: 1.000068]\n",
      "94640 [D loss: 0.999964] [G loss: 1.000070]\n",
      "94650 [D loss: 0.999982] [G loss: 1.000063]\n",
      "94660 [D loss: 0.999961] [G loss: 1.000043]\n",
      "94670 [D loss: 0.999968] [G loss: 1.000074]\n",
      "94680 [D loss: 0.999960] [G loss: 1.000050]\n",
      "94690 [D loss: 0.999976] [G loss: 1.000069]\n",
      "94700 [D loss: 0.999963] [G loss: 1.000064]\n",
      "94710 [D loss: 0.999966] [G loss: 1.000066]\n",
      "94720 [D loss: 0.999957] [G loss: 1.000060]\n",
      "94730 [D loss: 0.999960] [G loss: 1.000058]\n",
      "94740 [D loss: 0.999979] [G loss: 1.000034]\n",
      "94750 [D loss: 0.999983] [G loss: 1.000060]\n",
      "94760 [D loss: 0.999966] [G loss: 1.000058]\n",
      "94770 [D loss: 0.999967] [G loss: 1.000078]\n",
      "94780 [D loss: 0.999977] [G loss: 1.000055]\n",
      "94790 [D loss: 0.999972] [G loss: 1.000055]\n",
      "94800 [D loss: 0.999970] [G loss: 1.000049]\n",
      "94810 [D loss: 0.999970] [G loss: 1.000043]\n",
      "94820 [D loss: 0.999972] [G loss: 1.000069]\n",
      "94830 [D loss: 0.999966] [G loss: 1.000081]\n",
      "94840 [D loss: 0.999968] [G loss: 1.000052]\n",
      "94850 [D loss: 0.999987] [G loss: 1.000023]\n",
      "94860 [D loss: 0.999970] [G loss: 1.000063]\n",
      "94870 [D loss: 0.999986] [G loss: 1.000072]\n",
      "94880 [D loss: 0.999964] [G loss: 1.000073]\n",
      "94890 [D loss: 0.999972] [G loss: 1.000072]\n",
      "94900 [D loss: 0.999976] [G loss: 1.000060]\n",
      "94910 [D loss: 0.999957] [G loss: 1.000086]\n",
      "94920 [D loss: 0.999966] [G loss: 1.000072]\n",
      "94930 [D loss: 0.999957] [G loss: 1.000049]\n",
      "94940 [D loss: 0.999969] [G loss: 1.000057]\n",
      "94950 [D loss: 0.999957] [G loss: 1.000064]\n",
      "94960 [D loss: 0.999952] [G loss: 1.000054]\n",
      "94970 [D loss: 0.999949] [G loss: 1.000065]\n",
      "94980 [D loss: 0.999973] [G loss: 1.000061]\n",
      "94990 [D loss: 0.999966] [G loss: 1.000052]\n",
      "95000 [D loss: 0.999963] [G loss: 1.000060]\n",
      "95010 [D loss: 0.999964] [G loss: 1.000069]\n",
      "95020 [D loss: 0.999964] [G loss: 1.000061]\n",
      "95030 [D loss: 0.999973] [G loss: 1.000050]\n",
      "95040 [D loss: 0.999959] [G loss: 1.000042]\n",
      "95050 [D loss: 0.999958] [G loss: 1.000049]\n",
      "95060 [D loss: 0.999967] [G loss: 1.000073]\n",
      "95070 [D loss: 0.999967] [G loss: 1.000070]\n",
      "95080 [D loss: 0.999963] [G loss: 1.000055]\n",
      "95090 [D loss: 0.999962] [G loss: 1.000059]\n",
      "95100 [D loss: 0.999967] [G loss: 1.000059]\n",
      "95110 [D loss: 0.999959] [G loss: 1.000067]\n",
      "95120 [D loss: 0.999956] [G loss: 1.000056]\n",
      "95130 [D loss: 0.999971] [G loss: 1.000058]\n",
      "95140 [D loss: 0.999967] [G loss: 1.000056]\n",
      "95150 [D loss: 0.999958] [G loss: 1.000067]\n",
      "95160 [D loss: 0.999969] [G loss: 1.000068]\n",
      "95170 [D loss: 0.999963] [G loss: 1.000059]\n",
      "95180 [D loss: 0.999960] [G loss: 1.000051]\n",
      "95190 [D loss: 0.999964] [G loss: 1.000064]\n",
      "95200 [D loss: 0.999966] [G loss: 1.000064]\n",
      "95210 [D loss: 0.999971] [G loss: 1.000060]\n",
      "95220 [D loss: 0.999974] [G loss: 1.000055]\n",
      "95230 [D loss: 0.999961] [G loss: 1.000062]\n",
      "95240 [D loss: 0.999966] [G loss: 1.000066]\n",
      "95250 [D loss: 0.999965] [G loss: 1.000069]\n",
      "95260 [D loss: 0.999966] [G loss: 1.000067]\n",
      "95270 [D loss: 0.999971] [G loss: 1.000056]\n",
      "95280 [D loss: 0.999964] [G loss: 1.000073]\n",
      "95290 [D loss: 0.999962] [G loss: 1.000058]\n",
      "95300 [D loss: 0.999962] [G loss: 1.000055]\n",
      "95310 [D loss: 0.999970] [G loss: 1.000063]\n",
      "95320 [D loss: 0.999964] [G loss: 1.000066]\n",
      "95330 [D loss: 0.999967] [G loss: 1.000065]\n",
      "95340 [D loss: 0.999970] [G loss: 1.000066]\n",
      "95350 [D loss: 0.999972] [G loss: 1.000046]\n",
      "95360 [D loss: 0.999973] [G loss: 1.000052]\n",
      "95370 [D loss: 0.999966] [G loss: 1.000065]\n",
      "95380 [D loss: 0.999970] [G loss: 1.000072]\n",
      "95390 [D loss: 0.999971] [G loss: 1.000065]\n",
      "95400 [D loss: 0.999966] [G loss: 1.000063]\n",
      "95410 [D loss: 0.999965] [G loss: 1.000050]\n",
      "95420 [D loss: 0.999964] [G loss: 1.000064]\n",
      "95430 [D loss: 0.999962] [G loss: 1.000057]\n",
      "95440 [D loss: 0.999976] [G loss: 1.000066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95450 [D loss: 0.999966] [G loss: 1.000052]\n",
      "95460 [D loss: 0.999972] [G loss: 1.000068]\n",
      "95470 [D loss: 0.999961] [G loss: 1.000058]\n",
      "95480 [D loss: 0.999968] [G loss: 1.000051]\n",
      "95490 [D loss: 0.999973] [G loss: 1.000050]\n",
      "95500 [D loss: 0.999977] [G loss: 1.000062]\n",
      "95510 [D loss: 0.999966] [G loss: 1.000058]\n",
      "95520 [D loss: 0.999966] [G loss: 1.000055]\n",
      "95530 [D loss: 0.999960] [G loss: 1.000065]\n",
      "95540 [D loss: 0.999971] [G loss: 1.000044]\n",
      "95550 [D loss: 0.999964] [G loss: 1.000064]\n",
      "95560 [D loss: 0.999964] [G loss: 1.000053]\n",
      "95570 [D loss: 0.999970] [G loss: 1.000076]\n",
      "95580 [D loss: 0.999971] [G loss: 1.000063]\n",
      "95590 [D loss: 0.999967] [G loss: 1.000051]\n",
      "95600 [D loss: 0.999970] [G loss: 1.000063]\n",
      "95610 [D loss: 0.999964] [G loss: 1.000062]\n",
      "95620 [D loss: 0.999966] [G loss: 1.000062]\n",
      "95630 [D loss: 0.999969] [G loss: 1.000060]\n",
      "95640 [D loss: 0.999964] [G loss: 1.000056]\n",
      "95650 [D loss: 0.999969] [G loss: 1.000057]\n",
      "95660 [D loss: 0.999964] [G loss: 1.000070]\n",
      "95670 [D loss: 0.999968] [G loss: 1.000059]\n",
      "95680 [D loss: 0.999965] [G loss: 1.000059]\n",
      "95690 [D loss: 0.999966] [G loss: 1.000060]\n",
      "95700 [D loss: 0.999963] [G loss: 1.000059]\n",
      "95710 [D loss: 0.999959] [G loss: 1.000063]\n",
      "95720 [D loss: 0.999963] [G loss: 1.000052]\n",
      "95730 [D loss: 0.999975] [G loss: 1.000057]\n",
      "95740 [D loss: 0.999972] [G loss: 1.000062]\n",
      "95750 [D loss: 0.999964] [G loss: 1.000063]\n",
      "95760 [D loss: 0.999977] [G loss: 1.000052]\n",
      "95770 [D loss: 0.999961] [G loss: 1.000059]\n",
      "95780 [D loss: 0.999966] [G loss: 1.000064]\n",
      "95790 [D loss: 0.999960] [G loss: 1.000070]\n",
      "95800 [D loss: 0.999967] [G loss: 1.000057]\n",
      "95810 [D loss: 0.999970] [G loss: 1.000056]\n",
      "95820 [D loss: 0.999971] [G loss: 1.000062]\n",
      "95830 [D loss: 0.999964] [G loss: 1.000073]\n",
      "95840 [D loss: 0.999967] [G loss: 1.000075]\n",
      "95850 [D loss: 0.999967] [G loss: 1.000051]\n",
      "95860 [D loss: 0.999963] [G loss: 1.000064]\n",
      "95870 [D loss: 0.999973] [G loss: 1.000065]\n",
      "95880 [D loss: 0.999970] [G loss: 1.000054]\n",
      "95890 [D loss: 0.999967] [G loss: 1.000047]\n",
      "95900 [D loss: 0.999971] [G loss: 1.000057]\n",
      "95910 [D loss: 0.999960] [G loss: 1.000073]\n",
      "95920 [D loss: 0.999971] [G loss: 1.000070]\n",
      "95930 [D loss: 0.999965] [G loss: 1.000051]\n",
      "95940 [D loss: 0.999974] [G loss: 1.000053]\n",
      "95950 [D loss: 0.999969] [G loss: 1.000069]\n",
      "95960 [D loss: 0.999959] [G loss: 1.000058]\n",
      "95970 [D loss: 0.999966] [G loss: 1.000069]\n",
      "95980 [D loss: 0.999960] [G loss: 1.000068]\n",
      "95990 [D loss: 0.999977] [G loss: 1.000063]\n",
      "96000 [D loss: 0.999957] [G loss: 1.000054]\n",
      "96010 [D loss: 0.999966] [G loss: 1.000050]\n",
      "96020 [D loss: 0.999972] [G loss: 1.000072]\n",
      "96030 [D loss: 0.999969] [G loss: 1.000058]\n",
      "96040 [D loss: 0.999972] [G loss: 1.000057]\n",
      "96050 [D loss: 0.999971] [G loss: 1.000054]\n",
      "96060 [D loss: 0.999963] [G loss: 1.000063]\n",
      "96070 [D loss: 0.999961] [G loss: 1.000055]\n",
      "96080 [D loss: 0.999971] [G loss: 1.000048]\n",
      "96090 [D loss: 0.999967] [G loss: 1.000071]\n",
      "96100 [D loss: 0.999965] [G loss: 1.000067]\n",
      "96110 [D loss: 0.999961] [G loss: 1.000067]\n",
      "96120 [D loss: 0.999967] [G loss: 1.000064]\n",
      "96130 [D loss: 0.999972] [G loss: 1.000057]\n",
      "96140 [D loss: 0.999965] [G loss: 1.000066]\n",
      "96150 [D loss: 0.999967] [G loss: 1.000065]\n",
      "96160 [D loss: 0.999963] [G loss: 1.000054]\n",
      "96170 [D loss: 0.999965] [G loss: 1.000057]\n",
      "96180 [D loss: 0.999969] [G loss: 1.000054]\n",
      "96190 [D loss: 0.999955] [G loss: 1.000068]\n",
      "96200 [D loss: 0.999973] [G loss: 1.000072]\n",
      "96210 [D loss: 0.999973] [G loss: 1.000061]\n",
      "96220 [D loss: 0.999968] [G loss: 1.000060]\n",
      "96230 [D loss: 0.999968] [G loss: 1.000071]\n",
      "96240 [D loss: 0.999972] [G loss: 1.000062]\n",
      "96250 [D loss: 0.999965] [G loss: 1.000074]\n",
      "96260 [D loss: 0.999965] [G loss: 1.000061]\n",
      "96270 [D loss: 0.999969] [G loss: 1.000047]\n",
      "96280 [D loss: 0.999961] [G loss: 1.000060]\n",
      "96290 [D loss: 0.999965] [G loss: 1.000064]\n",
      "96300 [D loss: 0.999970] [G loss: 1.000061]\n",
      "96310 [D loss: 0.999975] [G loss: 1.000062]\n",
      "96320 [D loss: 0.999967] [G loss: 1.000061]\n",
      "96330 [D loss: 0.999967] [G loss: 1.000071]\n",
      "96340 [D loss: 0.999963] [G loss: 1.000060]\n",
      "96350 [D loss: 0.999967] [G loss: 1.000054]\n",
      "96360 [D loss: 0.999969] [G loss: 1.000073]\n",
      "96370 [D loss: 0.999966] [G loss: 1.000076]\n",
      "96380 [D loss: 0.999971] [G loss: 1.000058]\n",
      "96390 [D loss: 0.999975] [G loss: 1.000058]\n",
      "96400 [D loss: 0.999961] [G loss: 1.000056]\n",
      "96410 [D loss: 0.999974] [G loss: 1.000071]\n",
      "96420 [D loss: 0.999976] [G loss: 1.000055]\n",
      "96430 [D loss: 0.999963] [G loss: 1.000059]\n",
      "96440 [D loss: 0.999970] [G loss: 1.000054]\n",
      "96450 [D loss: 0.999971] [G loss: 1.000080]\n",
      "96460 [D loss: 0.999966] [G loss: 1.000050]\n",
      "96470 [D loss: 0.999967] [G loss: 1.000060]\n",
      "96480 [D loss: 0.999970] [G loss: 1.000076]\n",
      "96490 [D loss: 0.999971] [G loss: 1.000057]\n",
      "96500 [D loss: 0.999962] [G loss: 1.000064]\n",
      "96510 [D loss: 0.999963] [G loss: 1.000050]\n",
      "96520 [D loss: 0.999969] [G loss: 1.000051]\n",
      "96530 [D loss: 0.999967] [G loss: 1.000062]\n",
      "96540 [D loss: 0.999960] [G loss: 1.000059]\n",
      "96550 [D loss: 0.999968] [G loss: 1.000058]\n",
      "96560 [D loss: 0.999968] [G loss: 1.000062]\n",
      "96570 [D loss: 0.999966] [G loss: 1.000064]\n",
      "96580 [D loss: 0.999972] [G loss: 1.000057]\n",
      "96590 [D loss: 0.999964] [G loss: 1.000059]\n",
      "96600 [D loss: 0.999967] [G loss: 1.000051]\n",
      "96610 [D loss: 0.999973] [G loss: 1.000048]\n",
      "96620 [D loss: 0.999964] [G loss: 1.000060]\n",
      "96630 [D loss: 0.999971] [G loss: 1.000070]\n",
      "96640 [D loss: 0.999969] [G loss: 1.000056]\n",
      "96650 [D loss: 0.999967] [G loss: 1.000062]\n",
      "96660 [D loss: 0.999969] [G loss: 1.000058]\n",
      "96670 [D loss: 0.999975] [G loss: 1.000066]\n",
      "96680 [D loss: 0.999967] [G loss: 1.000057]\n",
      "96690 [D loss: 0.999969] [G loss: 1.000060]\n",
      "96700 [D loss: 0.999970] [G loss: 1.000059]\n",
      "96710 [D loss: 0.999961] [G loss: 1.000067]\n",
      "96720 [D loss: 0.999983] [G loss: 1.000050]\n",
      "96730 [D loss: 0.999966] [G loss: 1.000064]\n",
      "96740 [D loss: 0.999971] [G loss: 1.000056]\n",
      "96750 [D loss: 0.999972] [G loss: 1.000067]\n",
      "96760 [D loss: 0.999973] [G loss: 1.000057]\n",
      "96770 [D loss: 0.999964] [G loss: 1.000064]\n",
      "96780 [D loss: 0.999974] [G loss: 1.000067]\n",
      "96790 [D loss: 0.999967] [G loss: 1.000070]\n",
      "96800 [D loss: 0.999962] [G loss: 1.000059]\n",
      "96810 [D loss: 0.999951] [G loss: 1.000056]\n",
      "96820 [D loss: 0.999957] [G loss: 1.000063]\n",
      "96830 [D loss: 0.999966] [G loss: 1.000066]\n",
      "96840 [D loss: 0.999966] [G loss: 1.000068]\n",
      "96850 [D loss: 0.999971] [G loss: 1.000056]\n",
      "96860 [D loss: 0.999988] [G loss: 1.000053]\n",
      "96870 [D loss: 0.999969] [G loss: 1.000052]\n",
      "96880 [D loss: 0.999973] [G loss: 1.000063]\n",
      "96890 [D loss: 0.999967] [G loss: 1.000066]\n",
      "96900 [D loss: 0.999969] [G loss: 1.000058]\n",
      "96910 [D loss: 0.999972] [G loss: 1.000065]\n",
      "96920 [D loss: 0.999963] [G loss: 1.000056]\n",
      "96930 [D loss: 0.999955] [G loss: 1.000051]\n",
      "96940 [D loss: 0.999961] [G loss: 1.000055]\n",
      "96950 [D loss: 0.999969] [G loss: 1.000061]\n",
      "96960 [D loss: 0.999962] [G loss: 1.000069]\n",
      "96970 [D loss: 0.999974] [G loss: 1.000061]\n",
      "96980 [D loss: 0.999967] [G loss: 1.000069]\n",
      "96990 [D loss: 0.999971] [G loss: 1.000057]\n",
      "97000 [D loss: 0.999968] [G loss: 1.000071]\n",
      "97010 [D loss: 0.999966] [G loss: 1.000062]\n",
      "97020 [D loss: 0.999961] [G loss: 1.000079]\n",
      "97030 [D loss: 0.999972] [G loss: 1.000047]\n",
      "97040 [D loss: 0.999969] [G loss: 1.000055]\n",
      "97050 [D loss: 0.999964] [G loss: 1.000057]\n",
      "97060 [D loss: 0.999967] [G loss: 1.000047]\n",
      "97070 [D loss: 0.999965] [G loss: 1.000058]\n",
      "97080 [D loss: 0.999962] [G loss: 1.000064]\n",
      "97090 [D loss: 0.999959] [G loss: 1.000059]\n",
      "97100 [D loss: 0.999964] [G loss: 1.000056]\n",
      "97110 [D loss: 0.999968] [G loss: 1.000059]\n",
      "97120 [D loss: 0.999962] [G loss: 1.000053]\n",
      "97130 [D loss: 0.999964] [G loss: 1.000063]\n",
      "97140 [D loss: 0.999960] [G loss: 1.000065]\n",
      "97150 [D loss: 0.999962] [G loss: 1.000059]\n",
      "97160 [D loss: 0.999964] [G loss: 1.000056]\n",
      "97170 [D loss: 0.999971] [G loss: 1.000056]\n",
      "97180 [D loss: 0.999969] [G loss: 1.000066]\n",
      "97190 [D loss: 0.999962] [G loss: 1.000076]\n",
      "97200 [D loss: 0.999966] [G loss: 1.000066]\n",
      "97210 [D loss: 0.999964] [G loss: 1.000056]\n",
      "97220 [D loss: 0.999969] [G loss: 1.000082]\n",
      "97230 [D loss: 0.999977] [G loss: 1.000057]\n",
      "97240 [D loss: 0.999971] [G loss: 1.000061]\n",
      "97250 [D loss: 0.999971] [G loss: 1.000060]\n",
      "97260 [D loss: 0.999966] [G loss: 1.000065]\n",
      "97270 [D loss: 0.999959] [G loss: 1.000065]\n",
      "97280 [D loss: 0.999966] [G loss: 1.000067]\n",
      "97290 [D loss: 0.999961] [G loss: 1.000053]\n",
      "97300 [D loss: 0.999969] [G loss: 1.000069]\n",
      "97310 [D loss: 0.999971] [G loss: 1.000051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97320 [D loss: 0.999969] [G loss: 1.000054]\n",
      "97330 [D loss: 0.999969] [G loss: 1.000073]\n",
      "97340 [D loss: 0.999954] [G loss: 1.000068]\n",
      "97350 [D loss: 0.999961] [G loss: 1.000041]\n",
      "97360 [D loss: 0.999971] [G loss: 1.000050]\n",
      "97370 [D loss: 0.999959] [G loss: 1.000074]\n",
      "97380 [D loss: 0.999968] [G loss: 1.000073]\n",
      "97390 [D loss: 0.999975] [G loss: 1.000035]\n",
      "97400 [D loss: 0.999965] [G loss: 1.000063]\n",
      "97410 [D loss: 0.999966] [G loss: 1.000059]\n",
      "97420 [D loss: 0.999973] [G loss: 1.000065]\n",
      "97430 [D loss: 0.999955] [G loss: 1.000057]\n",
      "97440 [D loss: 0.999967] [G loss: 1.000062]\n",
      "97450 [D loss: 0.999970] [G loss: 1.000064]\n",
      "97460 [D loss: 0.999970] [G loss: 1.000061]\n",
      "97470 [D loss: 0.999964] [G loss: 1.000074]\n",
      "97480 [D loss: 0.999970] [G loss: 1.000063]\n",
      "97490 [D loss: 0.999970] [G loss: 1.000062]\n",
      "97500 [D loss: 0.999974] [G loss: 1.000059]\n",
      "97510 [D loss: 0.999968] [G loss: 1.000054]\n",
      "97520 [D loss: 0.999965] [G loss: 1.000053]\n",
      "97530 [D loss: 0.999977] [G loss: 1.000036]\n",
      "97540 [D loss: 0.999951] [G loss: 1.000068]\n",
      "97550 [D loss: 0.999975] [G loss: 1.000056]\n",
      "97560 [D loss: 0.999965] [G loss: 1.000050]\n",
      "97570 [D loss: 0.999960] [G loss: 1.000067]\n",
      "97580 [D loss: 0.999960] [G loss: 1.000075]\n",
      "97590 [D loss: 0.999968] [G loss: 1.000077]\n",
      "97600 [D loss: 0.999967] [G loss: 1.000062]\n",
      "97610 [D loss: 0.999956] [G loss: 1.000073]\n",
      "97620 [D loss: 0.999963] [G loss: 1.000061]\n",
      "97630 [D loss: 0.999969] [G loss: 1.000043]\n",
      "97640 [D loss: 0.999961] [G loss: 1.000070]\n",
      "97650 [D loss: 0.999960] [G loss: 1.000054]\n",
      "97660 [D loss: 0.999970] [G loss: 1.000066]\n",
      "97670 [D loss: 0.999968] [G loss: 1.000071]\n",
      "97680 [D loss: 0.999970] [G loss: 1.000062]\n",
      "97690 [D loss: 0.999972] [G loss: 1.000072]\n",
      "97700 [D loss: 0.999956] [G loss: 1.000058]\n",
      "97710 [D loss: 0.999961] [G loss: 1.000059]\n",
      "97720 [D loss: 0.999962] [G loss: 1.000043]\n",
      "97730 [D loss: 0.999960] [G loss: 1.000052]\n",
      "97740 [D loss: 0.999962] [G loss: 1.000067]\n",
      "97750 [D loss: 0.999977] [G loss: 1.000039]\n",
      "97760 [D loss: 0.999970] [G loss: 1.000065]\n",
      "97770 [D loss: 0.999978] [G loss: 1.000062]\n",
      "97780 [D loss: 0.999979] [G loss: 1.000058]\n",
      "97790 [D loss: 0.999957] [G loss: 1.000055]\n",
      "97800 [D loss: 0.999974] [G loss: 1.000058]\n",
      "97810 [D loss: 0.999955] [G loss: 1.000062]\n",
      "97820 [D loss: 0.999973] [G loss: 1.000057]\n",
      "97830 [D loss: 0.999969] [G loss: 1.000077]\n",
      "97840 [D loss: 0.999976] [G loss: 1.000043]\n",
      "97850 [D loss: 0.999961] [G loss: 1.000082]\n",
      "97860 [D loss: 0.999972] [G loss: 1.000049]\n",
      "97870 [D loss: 0.999953] [G loss: 1.000075]\n",
      "97880 [D loss: 0.999956] [G loss: 1.000064]\n",
      "97890 [D loss: 0.999972] [G loss: 1.000066]\n",
      "97900 [D loss: 0.999963] [G loss: 1.000033]\n",
      "97910 [D loss: 0.999988] [G loss: 1.000029]\n",
      "97920 [D loss: 0.999988] [G loss: 1.000065]\n",
      "97930 [D loss: 0.999961] [G loss: 1.000030]\n",
      "97940 [D loss: 0.999952] [G loss: 1.000061]\n",
      "97950 [D loss: 0.999969] [G loss: 1.000096]\n",
      "97960 [D loss: 0.999967] [G loss: 1.000060]\n",
      "97970 [D loss: 0.999961] [G loss: 1.000054]\n",
      "97980 [D loss: 0.999965] [G loss: 1.000043]\n",
      "97990 [D loss: 0.999973] [G loss: 1.000052]\n",
      "98000 [D loss: 0.999962] [G loss: 1.000078]\n",
      "98010 [D loss: 0.999972] [G loss: 1.000079]\n",
      "98020 [D loss: 0.999956] [G loss: 1.000085]\n",
      "98030 [D loss: 0.999972] [G loss: 1.000042]\n",
      "98040 [D loss: 0.999965] [G loss: 1.000067]\n",
      "98050 [D loss: 0.999972] [G loss: 1.000020]\n",
      "98060 [D loss: 0.999975] [G loss: 1.000054]\n",
      "98070 [D loss: 0.999969] [G loss: 1.000053]\n",
      "98080 [D loss: 0.999959] [G loss: 1.000068]\n",
      "98090 [D loss: 0.999964] [G loss: 1.000061]\n",
      "98100 [D loss: 0.999970] [G loss: 1.000067]\n",
      "98110 [D loss: 0.999960] [G loss: 1.000054]\n",
      "98120 [D loss: 0.999982] [G loss: 1.000051]\n",
      "98130 [D loss: 0.999967] [G loss: 1.000045]\n",
      "98140 [D loss: 0.999979] [G loss: 1.000031]\n",
      "98150 [D loss: 0.999981] [G loss: 1.000073]\n",
      "98160 [D loss: 0.999957] [G loss: 1.000090]\n",
      "98170 [D loss: 0.999975] [G loss: 1.000042]\n",
      "98180 [D loss: 0.999942] [G loss: 1.000096]\n",
      "98190 [D loss: 0.999972] [G loss: 1.000047]\n",
      "98200 [D loss: 0.999988] [G loss: 1.000045]\n",
      "98210 [D loss: 0.999981] [G loss: 1.000033]\n",
      "98220 [D loss: 0.999966] [G loss: 1.000051]\n",
      "98230 [D loss: 0.999968] [G loss: 1.000083]\n",
      "98240 [D loss: 0.999942] [G loss: 1.000041]\n",
      "98250 [D loss: 0.999957] [G loss: 1.000073]\n",
      "98260 [D loss: 0.999953] [G loss: 1.000071]\n",
      "98270 [D loss: 0.999989] [G loss: 1.000056]\n",
      "98280 [D loss: 0.999974] [G loss: 1.000069]\n",
      "98290 [D loss: 0.999977] [G loss: 1.000059]\n",
      "98300 [D loss: 0.999962] [G loss: 1.000076]\n",
      "98310 [D loss: 0.999947] [G loss: 1.000079]\n",
      "98320 [D loss: 0.999951] [G loss: 1.000037]\n",
      "98330 [D loss: 0.999958] [G loss: 1.000068]\n",
      "98340 [D loss: 0.999965] [G loss: 1.000071]\n",
      "98350 [D loss: 0.999962] [G loss: 1.000075]\n",
      "98360 [D loss: 0.999976] [G loss: 1.000075]\n",
      "98370 [D loss: 0.999995] [G loss: 1.000036]\n",
      "98380 [D loss: 0.999970] [G loss: 1.000051]\n",
      "98390 [D loss: 0.999960] [G loss: 1.000077]\n",
      "98400 [D loss: 0.999971] [G loss: 1.000069]\n",
      "98410 [D loss: 0.999962] [G loss: 1.000073]\n",
      "98420 [D loss: 0.999970] [G loss: 1.000045]\n",
      "98430 [D loss: 0.999960] [G loss: 1.000054]\n",
      "98440 [D loss: 0.999964] [G loss: 1.000068]\n",
      "98450 [D loss: 0.999970] [G loss: 1.000048]\n",
      "98460 [D loss: 0.999971] [G loss: 1.000062]\n",
      "98470 [D loss: 0.999977] [G loss: 1.000042]\n",
      "98480 [D loss: 0.999976] [G loss: 1.000056]\n",
      "98490 [D loss: 0.999982] [G loss: 1.000068]\n",
      "98500 [D loss: 0.999964] [G loss: 1.000058]\n",
      "98510 [D loss: 0.999970] [G loss: 1.000084]\n",
      "98520 [D loss: 0.999966] [G loss: 1.000059]\n",
      "98530 [D loss: 0.999956] [G loss: 1.000057]\n",
      "98540 [D loss: 0.999958] [G loss: 1.000063]\n",
      "98550 [D loss: 0.999969] [G loss: 1.000089]\n",
      "98560 [D loss: 0.999969] [G loss: 1.000086]\n",
      "98570 [D loss: 0.999953] [G loss: 1.000071]\n",
      "98580 [D loss: 0.999962] [G loss: 1.000073]\n",
      "98590 [D loss: 0.999975] [G loss: 1.000033]\n",
      "98600 [D loss: 0.999972] [G loss: 1.000037]\n",
      "98610 [D loss: 0.999956] [G loss: 1.000083]\n",
      "98620 [D loss: 0.999981] [G loss: 1.000057]\n",
      "98630 [D loss: 0.999968] [G loss: 1.000063]\n",
      "98640 [D loss: 0.999958] [G loss: 1.000068]\n",
      "98650 [D loss: 0.999963] [G loss: 1.000042]\n",
      "98660 [D loss: 0.999956] [G loss: 1.000033]\n",
      "98670 [D loss: 0.999956] [G loss: 1.000072]\n",
      "98680 [D loss: 0.999951] [G loss: 1.000055]\n",
      "98690 [D loss: 0.999970] [G loss: 1.000045]\n",
      "98700 [D loss: 0.999970] [G loss: 1.000068]\n",
      "98710 [D loss: 0.999952] [G loss: 1.000086]\n",
      "98720 [D loss: 0.999976] [G loss: 1.000055]\n",
      "98730 [D loss: 0.999972] [G loss: 1.000069]\n",
      "98740 [D loss: 1.000001] [G loss: 1.000071]\n",
      "98750 [D loss: 0.999970] [G loss: 1.000049]\n",
      "98760 [D loss: 0.999960] [G loss: 1.000058]\n",
      "98770 [D loss: 0.999947] [G loss: 1.000065]\n",
      "98780 [D loss: 0.999972] [G loss: 1.000068]\n",
      "98790 [D loss: 0.999964] [G loss: 1.000058]\n",
      "98800 [D loss: 0.999974] [G loss: 1.000059]\n",
      "98810 [D loss: 0.999967] [G loss: 1.000067]\n",
      "98820 [D loss: 0.999960] [G loss: 1.000066]\n",
      "98830 [D loss: 0.999961] [G loss: 1.000056]\n",
      "98840 [D loss: 0.999950] [G loss: 1.000068]\n",
      "98850 [D loss: 0.999969] [G loss: 1.000069]\n",
      "98860 [D loss: 0.999955] [G loss: 1.000092]\n",
      "98870 [D loss: 0.999968] [G loss: 1.000060]\n",
      "98880 [D loss: 0.999955] [G loss: 1.000070]\n",
      "98890 [D loss: 0.999960] [G loss: 1.000071]\n",
      "98900 [D loss: 0.999973] [G loss: 1.000057]\n",
      "98910 [D loss: 0.999960] [G loss: 1.000057]\n",
      "98920 [D loss: 0.999956] [G loss: 1.000076]\n",
      "98930 [D loss: 0.999969] [G loss: 1.000062]\n",
      "98940 [D loss: 0.999962] [G loss: 1.000066]\n",
      "98950 [D loss: 0.999966] [G loss: 1.000054]\n",
      "98960 [D loss: 0.999944] [G loss: 1.000081]\n",
      "98970 [D loss: 0.999971] [G loss: 1.000056]\n",
      "98980 [D loss: 0.999975] [G loss: 1.000040]\n",
      "98990 [D loss: 0.999964] [G loss: 1.000077]\n",
      "99000 [D loss: 0.999961] [G loss: 1.000055]\n",
      "99010 [D loss: 0.999948] [G loss: 1.000045]\n",
      "99020 [D loss: 0.999973] [G loss: 1.000058]\n",
      "99030 [D loss: 0.999952] [G loss: 1.000057]\n",
      "99040 [D loss: 0.999969] [G loss: 1.000070]\n",
      "99050 [D loss: 0.999967] [G loss: 1.000067]\n",
      "99060 [D loss: 0.999977] [G loss: 1.000066]\n",
      "99070 [D loss: 0.999958] [G loss: 1.000055]\n",
      "99080 [D loss: 0.999961] [G loss: 1.000057]\n",
      "99090 [D loss: 0.999959] [G loss: 1.000072]\n",
      "99100 [D loss: 0.999970] [G loss: 1.000060]\n",
      "99110 [D loss: 0.999969] [G loss: 1.000057]\n",
      "99120 [D loss: 0.999962] [G loss: 1.000058]\n",
      "99130 [D loss: 0.999973] [G loss: 1.000061]\n",
      "99140 [D loss: 0.999976] [G loss: 1.000059]\n",
      "99150 [D loss: 0.999975] [G loss: 1.000068]\n",
      "99160 [D loss: 0.999980] [G loss: 1.000057]\n",
      "99170 [D loss: 0.999982] [G loss: 1.000068]\n",
      "99180 [D loss: 0.999974] [G loss: 1.000063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99190 [D loss: 0.999965] [G loss: 1.000062]\n",
      "99200 [D loss: 0.999985] [G loss: 1.000034]\n",
      "99210 [D loss: 0.999971] [G loss: 1.000057]\n",
      "99220 [D loss: 0.999965] [G loss: 1.000061]\n",
      "99230 [D loss: 0.999962] [G loss: 1.000078]\n",
      "99240 [D loss: 0.999979] [G loss: 1.000060]\n",
      "99250 [D loss: 0.999970] [G loss: 1.000064]\n",
      "99260 [D loss: 0.999961] [G loss: 1.000067]\n",
      "99270 [D loss: 0.999966] [G loss: 1.000060]\n",
      "99280 [D loss: 0.999960] [G loss: 1.000070]\n",
      "99290 [D loss: 0.999972] [G loss: 1.000061]\n",
      "99300 [D loss: 0.999953] [G loss: 1.000059]\n",
      "99310 [D loss: 0.999958] [G loss: 1.000080]\n",
      "99320 [D loss: 0.999978] [G loss: 1.000111]\n",
      "99330 [D loss: 0.999976] [G loss: 1.000061]\n",
      "99340 [D loss: 0.999969] [G loss: 1.000067]\n",
      "99350 [D loss: 0.999971] [G loss: 1.000032]\n",
      "99360 [D loss: 0.999972] [G loss: 1.000060]\n",
      "99370 [D loss: 0.999973] [G loss: 1.000064]\n",
      "99380 [D loss: 0.999956] [G loss: 1.000059]\n",
      "99390 [D loss: 0.999968] [G loss: 1.000074]\n",
      "99400 [D loss: 0.999963] [G loss: 1.000059]\n",
      "99410 [D loss: 0.999976] [G loss: 1.000046]\n",
      "99420 [D loss: 0.999969] [G loss: 1.000081]\n",
      "99430 [D loss: 0.999988] [G loss: 1.000076]\n",
      "99440 [D loss: 0.999958] [G loss: 1.000055]\n",
      "99450 [D loss: 0.999949] [G loss: 1.000082]\n",
      "99460 [D loss: 0.999998] [G loss: 1.000063]\n",
      "99470 [D loss: 0.999962] [G loss: 1.000043]\n",
      "99480 [D loss: 0.999995] [G loss: 1.000050]\n",
      "99490 [D loss: 0.999958] [G loss: 1.000051]\n",
      "99500 [D loss: 0.999958] [G loss: 1.000077]\n",
      "99510 [D loss: 0.999968] [G loss: 1.000042]\n",
      "99520 [D loss: 0.999964] [G loss: 1.000068]\n",
      "99530 [D loss: 0.999966] [G loss: 1.000056]\n",
      "99540 [D loss: 0.999985] [G loss: 1.000036]\n",
      "99550 [D loss: 0.999970] [G loss: 1.000061]\n",
      "99560 [D loss: 0.999965] [G loss: 1.000049]\n",
      "99570 [D loss: 0.999962] [G loss: 1.000060]\n",
      "99580 [D loss: 0.999957] [G loss: 1.000059]\n",
      "99590 [D loss: 0.999968] [G loss: 1.000068]\n",
      "99600 [D loss: 0.999961] [G loss: 1.000068]\n",
      "99610 [D loss: 0.999964] [G loss: 1.000055]\n",
      "99620 [D loss: 0.999956] [G loss: 1.000051]\n",
      "99630 [D loss: 0.999969] [G loss: 1.000071]\n",
      "99640 [D loss: 0.999972] [G loss: 1.000058]\n",
      "99650 [D loss: 0.999965] [G loss: 1.000076]\n",
      "99660 [D loss: 0.999983] [G loss: 1.000049]\n",
      "99670 [D loss: 0.999967] [G loss: 1.000066]\n",
      "99680 [D loss: 0.999965] [G loss: 1.000076]\n",
      "99690 [D loss: 0.999964] [G loss: 1.000079]\n",
      "99700 [D loss: 0.999960] [G loss: 1.000059]\n",
      "99710 [D loss: 0.999974] [G loss: 1.000069]\n",
      "99720 [D loss: 0.999969] [G loss: 1.000067]\n",
      "99730 [D loss: 0.999967] [G loss: 1.000051]\n",
      "99740 [D loss: 0.999968] [G loss: 1.000065]\n",
      "99750 [D loss: 0.999973] [G loss: 1.000070]\n",
      "99760 [D loss: 0.999969] [G loss: 1.000066]\n",
      "99770 [D loss: 0.999977] [G loss: 1.000042]\n",
      "99780 [D loss: 0.999959] [G loss: 1.000069]\n",
      "99790 [D loss: 0.999961] [G loss: 1.000065]\n",
      "99800 [D loss: 0.999954] [G loss: 1.000058]\n",
      "99810 [D loss: 0.999959] [G loss: 1.000094]\n",
      "99820 [D loss: 0.999974] [G loss: 1.000070]\n",
      "99830 [D loss: 0.999970] [G loss: 1.000044]\n",
      "99840 [D loss: 0.999961] [G loss: 1.000052]\n",
      "99850 [D loss: 0.999968] [G loss: 1.000065]\n",
      "99860 [D loss: 0.999956] [G loss: 1.000054]\n",
      "99870 [D loss: 0.999963] [G loss: 1.000062]\n",
      "99880 [D loss: 0.999966] [G loss: 1.000063]\n",
      "99890 [D loss: 0.999968] [G loss: 1.000057]\n",
      "99900 [D loss: 0.999962] [G loss: 1.000069]\n",
      "99910 [D loss: 0.999957] [G loss: 1.000059]\n",
      "99920 [D loss: 0.999966] [G loss: 1.000065]\n",
      "99930 [D loss: 0.999963] [G loss: 1.000074]\n",
      "99940 [D loss: 0.999983] [G loss: 1.000044]\n",
      "99950 [D loss: 0.999960] [G loss: 1.000054]\n",
      "99960 [D loss: 0.999967] [G loss: 1.000073]\n",
      "99970 [D loss: 0.999964] [G loss: 1.000051]\n",
      "99980 [D loss: 0.999966] [G loss: 1.000054]\n",
      "99990 [D loss: 0.999982] [G loss: 1.000047]\n"
     ]
    }
   ],
   "source": [
    "real_label = -np.ones((batch_size, 1))\n",
    "fake_label = np.ones_like(real_label)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    if ep < 30000:\n",
    "        continue\n",
    "    for _ in range(critic_repeat_times):\n",
    "        real_imgs = next(train_generator)\n",
    "        while real_imgs.shape[0] != batch_size:\n",
    "            real_imgs = next(train_generator)\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, G_IN_DIM))\n",
    "\n",
    "        fake_imgs = generator.predict(z)\n",
    "\n",
    "        d_loss_real = critic.train_on_batch(real_imgs, real_label)\n",
    "        d_loss_fake = critic.train_on_batch(fake_imgs, fake_label)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Clip critic weights\n",
    "        for l in critic.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "\n",
    "    g_loss = dg.train_on_batch(z, real_label)\n",
    "\n",
    "    # Plot the progress\n",
    "    if ep % 10 == 0:\n",
    "        print(\"%d [D loss: %f] [G loss: %f]\" %\n",
    "              (ep, 1 - d_loss[0], 1 - g_loss[0]))\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if ep % 100 == 0:\n",
    "        update_tb_summary(ep, 1 - d_loss[0], 1 - g_loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T08:12:00.180334Z",
     "start_time": "2018-11-20T08:12:00.162571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64, 64, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_imgs = next(train_generator)\n",
    "real_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
